{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "id": "ArFnLMfvvPAG",
    "outputId": "95d7b013-857d-47ba-f439-e366c70ae1b0"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "fsqWgtdkweov"
   },
   "outputs": [],
   "source": [
    "import math\n",
    "import random\n",
    "from torchvision import datasets ,transforms\n",
    "from torchvision.datasets import ImageFolder\n",
    "from torch.utils.data import DataLoader,Dataset\n",
    "from torch import nn \n",
    "from torch.nn.functional import relu ,softmax \n",
    "import copy \n",
    "from torch.utils.data import Subset\n",
    "import pandas as pd \n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import time\n",
    "import torch\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import torch.nn.functional as F\n",
    "#device=torch.device(\"cuda:0\"if torch.cuda.is_available () else \"cpu\")\n",
    "device=\"cpu\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 256
    },
    "colab_type": "code",
    "id": "Bzm5sf5zwiV_",
    "outputId": "927911e5-363d-4f8b-abb5-a23101a2361a"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>att_1</th>\n",
       "      <th>att_2</th>\n",
       "      <th>att_3</th>\n",
       "      <th>att_4</th>\n",
       "      <th>att_5</th>\n",
       "      <th>att_6</th>\n",
       "      <th>att_7</th>\n",
       "      <th>att_8</th>\n",
       "      <th>att_9</th>\n",
       "      <th>att_10</th>\n",
       "      <th>...</th>\n",
       "      <th>att_172</th>\n",
       "      <th>att_173</th>\n",
       "      <th>att_174</th>\n",
       "      <th>att_175</th>\n",
       "      <th>att_176</th>\n",
       "      <th>att_177</th>\n",
       "      <th>att_178</th>\n",
       "      <th>att_179</th>\n",
       "      <th>att_180</th>\n",
       "      <th>class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>12.184764</td>\n",
       "      <td>18.534243</td>\n",
       "      <td>28.945810</td>\n",
       "      <td>22.241393</td>\n",
       "      <td>7.353465</td>\n",
       "      <td>17.095399</td>\n",
       "      <td>26.996429</td>\n",
       "      <td>41.427220</td>\n",
       "      <td>24.394928</td>\n",
       "      <td>3.888953</td>\n",
       "      <td>...</td>\n",
       "      <td>0.010558</td>\n",
       "      <td>0.014658</td>\n",
       "      <td>0.016326</td>\n",
       "      <td>0.009556</td>\n",
       "      <td>0.007948</td>\n",
       "      <td>0.011146</td>\n",
       "      <td>0.016568</td>\n",
       "      <td>0.017170</td>\n",
       "      <td>0.008511</td>\n",
       "      <td>A</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>25.639720</td>\n",
       "      <td>46.945876</td>\n",
       "      <td>63.464762</td>\n",
       "      <td>44.700204</td>\n",
       "      <td>23.165613</td>\n",
       "      <td>18.646121</td>\n",
       "      <td>28.515218</td>\n",
       "      <td>41.926007</td>\n",
       "      <td>29.490375</td>\n",
       "      <td>8.800943</td>\n",
       "      <td>...</td>\n",
       "      <td>0.014752</td>\n",
       "      <td>0.022414</td>\n",
       "      <td>0.024294</td>\n",
       "      <td>0.014430</td>\n",
       "      <td>0.010937</td>\n",
       "      <td>0.016972</td>\n",
       "      <td>0.025050</td>\n",
       "      <td>0.023918</td>\n",
       "      <td>0.014216</td>\n",
       "      <td>A</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>14.381637</td>\n",
       "      <td>24.706305</td>\n",
       "      <td>35.822925</td>\n",
       "      <td>29.553382</td>\n",
       "      <td>22.935236</td>\n",
       "      <td>13.837493</td>\n",
       "      <td>22.689212</td>\n",
       "      <td>36.892506</td>\n",
       "      <td>33.376486</td>\n",
       "      <td>9.703304</td>\n",
       "      <td>...</td>\n",
       "      <td>0.013782</td>\n",
       "      <td>0.019098</td>\n",
       "      <td>0.024171</td>\n",
       "      <td>0.020194</td>\n",
       "      <td>0.009770</td>\n",
       "      <td>0.014811</td>\n",
       "      <td>0.021661</td>\n",
       "      <td>0.021909</td>\n",
       "      <td>0.015823</td>\n",
       "      <td>A</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>20.123721</td>\n",
       "      <td>39.021980</td>\n",
       "      <td>59.427789</td>\n",
       "      <td>33.768032</td>\n",
       "      <td>16.884216</td>\n",
       "      <td>14.235512</td>\n",
       "      <td>24.346061</td>\n",
       "      <td>33.150834</td>\n",
       "      <td>17.558946</td>\n",
       "      <td>5.433499</td>\n",
       "      <td>...</td>\n",
       "      <td>0.014727</td>\n",
       "      <td>0.022686</td>\n",
       "      <td>0.022876</td>\n",
       "      <td>0.013031</td>\n",
       "      <td>0.010639</td>\n",
       "      <td>0.016706</td>\n",
       "      <td>0.026014</td>\n",
       "      <td>0.023591</td>\n",
       "      <td>0.012991</td>\n",
       "      <td>A</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>20.855774</td>\n",
       "      <td>31.241891</td>\n",
       "      <td>46.056155</td>\n",
       "      <td>36.617243</td>\n",
       "      <td>20.152911</td>\n",
       "      <td>15.612698</td>\n",
       "      <td>27.206975</td>\n",
       "      <td>42.754484</td>\n",
       "      <td>29.983009</td>\n",
       "      <td>7.457517</td>\n",
       "      <td>...</td>\n",
       "      <td>0.014497</td>\n",
       "      <td>0.019523</td>\n",
       "      <td>0.023203</td>\n",
       "      <td>0.016096</td>\n",
       "      <td>0.011517</td>\n",
       "      <td>0.016587</td>\n",
       "      <td>0.024086</td>\n",
       "      <td>0.022221</td>\n",
       "      <td>0.015074</td>\n",
       "      <td>A</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 181 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       att_1      att_2      att_3      att_4      att_5      att_6  \\\n",
       "0  12.184764  18.534243  28.945810  22.241393   7.353465  17.095399   \n",
       "1  25.639720  46.945876  63.464762  44.700204  23.165613  18.646121   \n",
       "2  14.381637  24.706305  35.822925  29.553382  22.935236  13.837493   \n",
       "3  20.123721  39.021980  59.427789  33.768032  16.884216  14.235512   \n",
       "4  20.855774  31.241891  46.056155  36.617243  20.152911  15.612698   \n",
       "\n",
       "       att_7      att_8      att_9    att_10  ...   att_172   att_173  \\\n",
       "0  26.996429  41.427220  24.394928  3.888953  ...  0.010558  0.014658   \n",
       "1  28.515218  41.926007  29.490375  8.800943  ...  0.014752  0.022414   \n",
       "2  22.689212  36.892506  33.376486  9.703304  ...  0.013782  0.019098   \n",
       "3  24.346061  33.150834  17.558946  5.433499  ...  0.014727  0.022686   \n",
       "4  27.206975  42.754484  29.983009  7.457517  ...  0.014497  0.019523   \n",
       "\n",
       "    att_174   att_175   att_176   att_177   att_178   att_179   att_180  class  \n",
       "0  0.016326  0.009556  0.007948  0.011146  0.016568  0.017170  0.008511      A  \n",
       "1  0.024294  0.014430  0.010937  0.016972  0.025050  0.023918  0.014216      A  \n",
       "2  0.024171  0.020194  0.009770  0.014811  0.021661  0.021909  0.015823      A  \n",
       "3  0.022876  0.013031  0.010639  0.016706  0.026014  0.023591  0.012991      A  \n",
       "4  0.023203  0.016096  0.011517  0.016587  0.024086  0.022221  0.015074      A  \n",
       "\n",
       "[5 rows x 181 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "path ='Riktim_Testing/Word-level-Features/MLG_180_Features.csv'\n",
    "df = pd.read_csv(path) #header=None\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "id": "q9lhL3iVwyc6",
    "outputId": "1e35db16-a4c7-4523-d023-9e9d6feb633d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(12000, 181)\n"
     ]
    }
   ],
   "source": [
    "print(df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 127
    },
    "colab_type": "code",
    "id": "S4K9LYdmw4iW",
    "outputId": "6a2c8931-b341-4de4-b864-c9d9771e1595"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(12000, 181)\n",
      "<class 'numpy.ndarray'>\n",
      "(12000, 180)\n",
      "\n",
      "['A' 'B' 'C' 'D' 'E' 'F' 'G' 'H' 'I' 'J' 'K' 'L']\n",
      "[0 0 0 0 0 0 0 0 0 0]\n"
     ]
    }
   ],
   "source": [
    "print(df.shape)\n",
    "#X=df.loc[:,df.columns != 'Classifier'].values\n",
    "X=df.loc[:,df.columns != df.columns[-1]].values\n",
    "print(type(X))\n",
    "print(X.shape)\n",
    "#y_label=df['Classifier'].values \n",
    "y_label =df.iloc[:,-1].values\n",
    "#print(type(y_label))\n",
    "#print(X[0])\n",
    "print()\n",
    "from sklearn import preprocessing\n",
    "le = preprocessing.LabelEncoder()\n",
    "le.fit(y_label)\n",
    "print(le.classes_)\n",
    "y=le.transform(y_label)\n",
    "print(y[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 54
    },
    "colab_type": "code",
    "id": "ASaktl2pxAXk",
    "outputId": "95a10aee-01f1-4229-ffb4-fbd5825700d3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11} {0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11}\n",
      "(12000, 180) (12000,)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler #for normalization\n",
    "sc = StandardScaler()\n",
    "X = sc.fit_transform(X)\n",
    "# from sklearn.preprocessing import MinMaxScaler\n",
    "# scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "# X = scaler.fit_transform(X)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42, shuffle=True)\n",
    "# print(y_train)\n",
    "# print(y_test)\n",
    "#print(len(X_train), len(y_train), len(X_test), len(y_test))\n",
    "# # X=X_train\n",
    "# # y=y_train\n",
    "print(set(y_train), set(y_test))\n",
    "print(X.shape, y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "IU4XOA5fy_4d"
   },
   "outputs": [],
   "source": [
    "X_train = torch.tensor(X_train, dtype=torch.float)\n",
    "y_train = torch.tensor(y_train, dtype=torch.long)\n",
    "X_test = torch.tensor(X_test, dtype=torch.float)\n",
    "y_test = torch.tensor(y_test, dtype=torch.long)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "AeGznOD8xEPs"
   },
   "outputs": [],
   "source": [
    "#n_inputs = X_train.shape[1]\n",
    "#n_hidden = 180\n",
    "#n_classes = len(le.classes_)\n",
    "#N = X_train.shape[0] # Number of samples\n",
    "\n",
    "#print(N)\n",
    "#hyperparameters\n",
    "input_size = X_train.shape[1]    # The image size = 28 x 28 = 784\n",
    "hidden_size = X_train.shape[1]    # The number of nodes at the hidden layer\n",
    "num_classes = len(le.classes_)       # The number of output classes. In this case, from 0 to 9\n",
    "num_epochs = 100      # The number of times entire dataset is trained\n",
    "batch_size = X_train.shape[0]    # The size of input data took for one iteration\n",
    "lr = 0.001  # The speed of convergence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "fatO2VKDxILM"
   },
   "outputs": [],
   "source": [
    "# #build model\n",
    "# class Net(nn.Module):\n",
    "\n",
    "#     def __init__(self):\n",
    "#         super(Net, self).__init__()\n",
    "#         self.fc1 = nn.Linear(n_inputs, n_hidden)\n",
    "#         self.drop = nn.Dropout (p=0.5)\n",
    "#         self.fc2 = nn.Linear(n_hidden,n_classes )\n",
    "\n",
    "#     def forward(self, x):\n",
    "#         x = F.relu(self.fc1(x))\n",
    "#         #x=self.drop(x)\n",
    "#         x = self.fc2(x)\n",
    "#         return x\n",
    "        \n",
    "# net = Net()\n",
    "class Model(nn.Module):\n",
    "    def __init__(self,input_size, hidden_size, num_classes):\n",
    "        super(Model, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_size , hidden_size)\n",
    "        self.fc2 = nn.Linear(hidden_size, num_classes)\n",
    "        self.drop = nn.Dropout (p=0.4)\n",
    "\n",
    "    def masking (self,act1,mask,p):\n",
    "        if (self.training ==True ):\n",
    "            return ((act1*mask)/p)\n",
    "        else :\n",
    "            return (act1)\n",
    "        \n",
    "\n",
    "    def Forward(self, x,mask,p):\n",
    "        #print(x.shape)\n",
    "        #x = x.view(-1, 32 * 32 * 3)\n",
    "        #print(x.shape)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        #x = self.drop(x)\n",
    "        x = self.masking (x,mask,p)\n",
    "        x = self.fc2(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "xnf3Kd39xosT"
   },
   "outputs": [],
   "source": [
    "def train_model(model, criterion, optimizer,mask,p, num_epochs=25):\n",
    "  # the mask has to be specified \n",
    "      since = time.time()\n",
    "\n",
    "      best_model_wts = copy.deepcopy(model.state_dict())\n",
    "      best_acc = 0.0\n",
    "      losses=[]\n",
    "      accuracies=[]\n",
    "\n",
    "      for epoch in range(num_epochs):\n",
    "          print('Epoch {}/{}'.format(epoch, num_epochs - 1))\n",
    "          print('-' * 10)\n",
    "\n",
    "          # Each epoch has a training and validation phase\n",
    "          for phase in ['train', 'val']:\n",
    "              if phase == 'train':\n",
    "                  model.train()  # Set model to training mode\n",
    "                  optimizer.zero_grad()\n",
    "                  with torch.set_grad_enabled(phase == 'train'):\n",
    "                    outputs= model.Forward(X_train,mask,p) #inputs\n",
    "                    train_loss = criterion(outputs, y_train)# backward + optimize only if in training phase,labels\n",
    "                    train_loss.backward()\n",
    "                    optimizer.step()\n",
    "                  train_acc = 100 * torch.sum(y_train==torch.max(outputs.data, 1)[1]).double() / len(y_train)\n",
    "                  print('********************{}*************'.format(phase))\n",
    "                  print ('Epoch [%d/%d] Loss: %.4f   Acc: %.4f'%(epoch+1, num_epochs, train_loss.item(), train_acc.item()))\n",
    "                  losses.append(train_loss)\n",
    "                  accuracies.append(train_acc)\n",
    "              else:\n",
    "                  model.eval()   # Set model to evaluate mode\n",
    "                  outputs= model.Forward(X_test,mask,p) #inputs\n",
    "                  _, predicted = torch.max(outputs, 1)\n",
    "                  test_loss = criterion(outputs, y_test)\n",
    "                  test_acc = 100 * torch.sum(y_test==predicted).double() / len(y_test)\n",
    "                  print('********************{}*************'.format(phase))\n",
    "                  print ('Epoch [%d/%d] Loss: %.4f   Acc: %.4f'%(epoch+1, num_epochs, test_loss.item(), test_acc.item()))\n",
    "                  if test_acc > best_acc:\n",
    "                    best_acc = test_acc\n",
    "                    best_model_wts = copy.deepcopy(model.state_dict())\n",
    "\n",
    "              \n",
    "\n",
    "\n",
    "\n",
    "      time_elapsed = time.time() - since\n",
    "      print('Training complete in {:.0f}m {:.0f}s'.format(\n",
    "          time_elapsed // 60, time_elapsed % 60))\n",
    "      print('Best val Acc: {:4f}'.format(best_acc))\n",
    "\n",
    "      # load best model weights\n",
    "      model.load_state_dict(best_model_wts)\n",
    "      return model,losses,accuracies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "_rFn0uEUxvQt"
   },
   "outputs": [],
   "source": [
    "class Population:   \n",
    "    def __init__(self,m,num,maskLength):\n",
    "        # constructor for initialising the population list\n",
    "        #list of DNA objects\n",
    "        self.population=[]\n",
    "        #muation rate for mutation\n",
    "        self.mutation_rate=m\n",
    "        #maximum number of entities in the population\n",
    "        self.popmax=num\n",
    "        self.maskLength=maskLength\n",
    "        for i in range (num):\n",
    "            #creating a dna object\n",
    "            #an initial random population created \n",
    "            dna =DNA(self.maskLength)\n",
    "            self.population.append (dna)\n",
    "      \n",
    "        self.matingPool=[]\n",
    "\n",
    "    def calcFitness (self,model):\n",
    "        # going through all the entities of population \n",
    "        #finding fitness of all population entities \n",
    "        for i in range(0,self.popmax):\n",
    "            self.population[i].fitness (model)\n",
    "\n",
    "    def naturalSelection(self):\n",
    "        self.matingPool=[]\n",
    "        maxFitness=0\n",
    "        for i in range (self.popmax):\n",
    "            # moving throught the entire population \n",
    "            if (self.population[i].fit>maxFitness):\n",
    "                maxFitness=self.population[i].fit\n",
    "       \n",
    "        # max Fitness has the maximum loss score of the entire population  \n",
    "        for i in range (self.popmax ):\n",
    "        # iterating through the all inviduals of the population\n",
    "            n=self.Mymap(self.population[i].fit,0,maxFitness,0,1)\n",
    "        \n",
    "            n=math.floor(n*100)\n",
    "            \n",
    "            for j in range (n):\n",
    "                #creating mating pool\n",
    "                self.matingPool.append (self.population[i])\n",
    "\n",
    "    def Mymap(self,num,prevlow,prevhigh,nextlow,nexthigh):\n",
    "        \n",
    "        prevrange =float((num-prevlow)/(prevhigh-prevlow))\n",
    "        return nextlow+(nexthigh-nextlow)*prevrange\n",
    "\n",
    "    def   generate (self):\n",
    "        for i in range (self.popmax ):\n",
    "            index_1=math.floor(random.randint  (0,len(self.matingPool)-1))\n",
    "            index_2=math.floor (random.randint (0,len(self.matingPool)-1))\n",
    "            parent1=self.matingPool[index_1]\n",
    "            parent2=self.matingPool[index_2]\n",
    "            child=parent1.crossover(parent2)\n",
    "            child.mutate(self.mutation_rate)\n",
    "            self.population[i]=child \n",
    "\n",
    "    def fittest(self):\n",
    "        #returns the fiitest individual mask of the population \n",
    "        #also returns the keeping probability of the fittest mask \n",
    "        fittest=self.population[0]\n",
    "        for i  in range (self.popmax):\n",
    "            if (fittest.fit<self.population[i].fit):\n",
    "                fittest=self.population[i]\n",
    "        return fittest,fittest.keep_prob()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "dcIsijbQxxbM"
   },
   "outputs": [],
   "source": [
    "class DNA:  \n",
    "    def __init__(self,maskLength):\n",
    "        #constructor for the creation of the mask as a gene object \n",
    "        self.maskLength=maskLength\n",
    "        #creation of mask \n",
    "        self.gene=torch.bernoulli(torch.empty(1,maskLength).uniform_(0,1))\n",
    "        self.fit=0\n",
    "\n",
    "    def keep_prob (self):\n",
    "        num_one =0\n",
    "        for i in range (self.maskLength):\n",
    "            if (self.gene[0,i]==1):\n",
    "                num_one=num_one+1\n",
    "        return float(num_one/self.maskLength)\n",
    "\n",
    "    def fitness(self,model):\n",
    "        # finding the fitness of a particular mask\n",
    "        #accuracy of all training set is the fitness in one epoch\n",
    "        #putting model in train mode \n",
    "        running_loss=0\n",
    "        running_corrects=0\n",
    "        model.train()\n",
    "\n",
    "        # for inputs,labels in dataloader ['train']:   ->>>>>>>>>>>>>>changed by riktim\n",
    "        #     inputs=inputs.to(device)\n",
    "        #     labels=labels.to(device)\n",
    "        #     outputs=model.Forward(inputs,self.gene,self.keep_prob())\n",
    "        #     _,preds=torch.max(outputs,1)\n",
    "        #     loss=criterion (outputs,labels)\n",
    "        #     running_loss += loss.item() * inputs.size(0)\n",
    "        #     running_corrects += torch.sum(preds == labels.data)\n",
    "        # epoch_loss = running_loss / dataset_sizes['train']\n",
    "        # epoch_acc = running_corrects.double() / dataset_sizes['train']      \n",
    "        # self.fit=epoch_acc\n",
    "        # return epoch_acc\n",
    "        \n",
    "        outputs=model.Forward(X_train,self.gene,self.keep_prob())\n",
    "        _,preds=torch.max(outputs,1)\n",
    "        loss=criterion (outputs,y_train)\n",
    "        acc = 100*torch.sum(y_train==torch.max(outputs.data, 1)[1]).double() / len(y_train)\n",
    "        self.fit=acc\n",
    "        return acc\n",
    "        \n",
    "\n",
    "    def crossover (self,parent2):\n",
    "        #one parent is the passed in the argument \n",
    "        #another parent is the one from which this function is called \n",
    "        #another parent is self.gene\n",
    "        child =DNA(self.maskLength)\n",
    "        midpoint =random .randint (0,self.maskLength-1)\n",
    "        for i in range (0,self.maskLength):\n",
    "            if (i>midpoint):\n",
    "                child.gene [0,i]=self.gene[0,i]\n",
    "            else :\n",
    "                child.gene [0,i]=parent2.gene[0,i]\n",
    "        \n",
    "        return child \n",
    "\n",
    "    def mutate(self,mutation_rate):\n",
    "        #randomly activate some of the nodes  \n",
    "        #mutate some of the genes \n",
    "        for i in range (self.maskLength):\n",
    "            if (random.randint (0,99)<=mutation_rate*100):\n",
    "                self.gene[0,i]=1 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "X3LuAn4Oxz6q"
   },
   "outputs": [],
   "source": [
    "mutation_rate =0\n",
    "max_population=30\n",
    "maskLength=X_train.shape[1]\n",
    "#seeded so that each time same initial weights generated \n",
    "#torch.manual_seed(6)\n",
    "model=Model(input_size, hidden_size, num_classes)#creating the object of the class\n",
    "model.to(device)\n",
    "criterion=nn.CrossEntropyLoss()\n",
    "#optimizer=torch.optim.SGD(model.parameters(),lr=0.001,momentum=0.9)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "apf1SKKNx9Q5",
    "outputId": "b002f9b0-bcc6-429c-afd8-789fb968c68d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch generations ( 0 /100) :accuracy(fittest mask) tensor(13.9524, dtype=torch.float64) keep_prob 0.4166666666666667\n",
      "Epoch 0/29\n",
      "----------\n",
      "********************train*************\n",
      "Epoch [1/30] Loss: 2.5205   Acc: 13.9524\n",
      "********************val*************\n",
      "Epoch [1/30] Loss: 2.4841   Acc: 10.6389\n",
      "Epoch 1/29\n",
      "----------\n",
      "********************train*************\n",
      "Epoch [2/30] Loss: 2.3832   Acc: 19.7024\n",
      "********************val*************\n",
      "Epoch [2/30] Loss: 2.4269   Acc: 14.5833\n",
      "Epoch 2/29\n",
      "----------\n",
      "********************train*************\n",
      "Epoch [3/30] Loss: 2.2571   Acc: 28.3214\n",
      "********************val*************\n",
      "Epoch [3/30] Loss: 2.3718   Acc: 17.5000\n",
      "Epoch 3/29\n",
      "----------\n",
      "********************train*************\n",
      "Epoch [4/30] Loss: 2.1419   Acc: 34.6310\n",
      "********************val*************\n",
      "Epoch [4/30] Loss: 2.3187   Acc: 24.4167\n",
      "Epoch 4/29\n",
      "----------\n",
      "********************train*************\n",
      "Epoch [5/30] Loss: 2.0378   Acc: 39.6905\n",
      "********************val*************\n",
      "Epoch [5/30] Loss: 2.2677   Acc: 30.3056\n",
      "Epoch 5/29\n",
      "----------\n",
      "********************train*************\n",
      "Epoch [6/30] Loss: 1.9434   Acc: 42.5833\n",
      "********************val*************\n",
      "Epoch [6/30] Loss: 2.2184   Acc: 34.7222\n",
      "Epoch 6/29\n",
      "----------\n",
      "********************train*************\n",
      "Epoch [7/30] Loss: 1.8572   Acc: 45.2024\n",
      "********************val*************\n",
      "Epoch [7/30] Loss: 2.1706   Acc: 38.2222\n",
      "Epoch 7/29\n",
      "----------\n",
      "********************train*************\n",
      "Epoch [8/30] Loss: 1.7778   Acc: 48.0119\n",
      "********************val*************\n",
      "Epoch [8/30] Loss: 2.1240   Acc: 41.8889\n",
      "Epoch 8/29\n",
      "----------\n",
      "********************train*************\n",
      "Epoch [9/30] Loss: 1.7043   Acc: 50.5476\n",
      "********************val*************\n",
      "Epoch [9/30] Loss: 2.0786   Acc: 45.6111\n",
      "Epoch 9/29\n",
      "----------\n",
      "********************train*************\n",
      "Epoch [10/30] Loss: 1.6359   Acc: 52.9881\n",
      "********************val*************\n",
      "Epoch [10/30] Loss: 2.0345   Acc: 48.2778\n",
      "Epoch 10/29\n",
      "----------\n",
      "********************train*************\n",
      "Epoch [11/30] Loss: 1.5721   Acc: 55.1310\n",
      "********************val*************\n",
      "Epoch [11/30] Loss: 1.9917   Acc: 51.4167\n",
      "Epoch 11/29\n",
      "----------\n",
      "********************train*************\n",
      "Epoch [12/30] Loss: 1.5121   Acc: 57.1905\n",
      "********************val*************\n",
      "Epoch [12/30] Loss: 1.9504   Acc: 53.5000\n",
      "Epoch 12/29\n",
      "----------\n",
      "********************train*************\n",
      "Epoch [13/30] Loss: 1.4555   Acc: 59.2500\n",
      "********************val*************\n",
      "Epoch [13/30] Loss: 1.9105   Acc: 55.7778\n",
      "Epoch 13/29\n",
      "----------\n",
      "********************train*************\n",
      "Epoch [14/30] Loss: 1.4017   Acc: 61.2381\n",
      "********************val*************\n",
      "Epoch [14/30] Loss: 1.8721   Acc: 58.0278\n",
      "Epoch 14/29\n",
      "----------\n",
      "********************train*************\n",
      "Epoch [15/30] Loss: 1.3503   Acc: 62.5833\n",
      "********************val*************\n",
      "Epoch [15/30] Loss: 1.8352   Acc: 59.6389\n",
      "Epoch 15/29\n",
      "----------\n",
      "********************train*************\n",
      "Epoch [16/30] Loss: 1.3011   Acc: 64.0238\n",
      "********************val*************\n",
      "Epoch [16/30] Loss: 1.7998   Acc: 61.3889\n",
      "Epoch 16/29\n",
      "----------\n",
      "********************train*************\n",
      "Epoch [17/30] Loss: 1.2541   Acc: 65.2619\n",
      "********************val*************\n",
      "Epoch [17/30] Loss: 1.7657   Acc: 62.7222\n",
      "Epoch 17/29\n",
      "----------\n",
      "********************train*************\n",
      "Epoch [18/30] Loss: 1.2093   Acc: 66.7976\n",
      "********************val*************\n",
      "Epoch [18/30] Loss: 1.7329   Acc: 64.6944\n",
      "Epoch 18/29\n",
      "----------\n",
      "********************train*************\n",
      "Epoch [19/30] Loss: 1.1667   Acc: 67.9524\n",
      "********************val*************\n",
      "Epoch [19/30] Loss: 1.7013   Acc: 66.1111\n",
      "Epoch 19/29\n",
      "----------\n",
      "********************train*************\n",
      "Epoch [20/30] Loss: 1.1263   Acc: 69.2738\n",
      "********************val*************\n",
      "Epoch [20/30] Loss: 1.6707   Acc: 67.0833\n",
      "Epoch 20/29\n",
      "----------\n",
      "********************train*************\n",
      "Epoch [21/30] Loss: 1.0881   Acc: 70.2738\n",
      "********************val*************\n",
      "Epoch [21/30] Loss: 1.6410   Acc: 68.0278\n",
      "Epoch 21/29\n",
      "----------\n",
      "********************train*************\n",
      "Epoch [22/30] Loss: 1.0519   Acc: 71.3571\n",
      "********************val*************\n",
      "Epoch [22/30] Loss: 1.6119   Acc: 69.2500\n",
      "Epoch 22/29\n",
      "----------\n",
      "********************train*************\n",
      "Epoch [23/30] Loss: 1.0177   Acc: 72.4405\n",
      "********************val*************\n",
      "Epoch [23/30] Loss: 1.5834   Acc: 69.9722\n",
      "Epoch 23/29\n",
      "----------\n",
      "********************train*************\n",
      "Epoch [24/30] Loss: 0.9853   Acc: 73.4524\n",
      "********************val*************\n",
      "Epoch [24/30] Loss: 1.5554   Acc: 70.9167\n",
      "Epoch 24/29\n",
      "----------\n",
      "********************train*************\n",
      "Epoch [25/30] Loss: 0.9545   Acc: 74.3452\n",
      "********************val*************\n",
      "Epoch [25/30] Loss: 1.5278   Acc: 71.4722\n",
      "Epoch 25/29\n",
      "----------\n",
      "********************train*************\n",
      "Epoch [26/30] Loss: 0.9252   Acc: 75.0833\n",
      "********************val*************\n",
      "Epoch [26/30] Loss: 1.5007   Acc: 72.5000\n",
      "Epoch 26/29\n",
      "----------\n",
      "********************train*************\n",
      "Epoch [27/30] Loss: 0.8973   Acc: 75.8929\n",
      "********************val*************\n",
      "Epoch [27/30] Loss: 1.4741   Acc: 73.2222\n",
      "Epoch 27/29\n",
      "----------\n",
      "********************train*************\n",
      "Epoch [28/30] Loss: 0.8708   Acc: 76.5119\n",
      "********************val*************\n",
      "Epoch [28/30] Loss: 1.4481   Acc: 73.8889\n",
      "Epoch 28/29\n",
      "----------\n",
      "********************train*************\n",
      "Epoch [29/30] Loss: 0.8456   Acc: 77.0000\n",
      "********************val*************\n",
      "Epoch [29/30] Loss: 1.4227   Acc: 74.3611\n",
      "Epoch 29/29\n",
      "----------\n",
      "********************train*************\n",
      "Epoch [30/30] Loss: 0.8216   Acc: 77.6548\n",
      "********************val*************\n",
      "Epoch [30/30] Loss: 1.3982   Acc: 75.1944\n",
      "Training complete in 0m 2s\n",
      "Best val Acc: 75.194444\n",
      "Epoch generations ( 1 /100) :accuracy(fittest mask) tensor(71.5476, dtype=torch.float64) keep_prob 0.49444444444444446\n",
      "Epoch generations ( 2 /100) :accuracy(fittest mask) tensor(71.6190, dtype=torch.float64) keep_prob 0.5333333333333333\n",
      "Epoch generations ( 3 /100) :accuracy(fittest mask) tensor(69.6905, dtype=torch.float64) keep_prob 0.5722222222222222\n",
      "Epoch generations ( 4 /100) :accuracy(fittest mask) tensor(71.7619, dtype=torch.float64) keep_prob 0.5611111111111111\n",
      "Epoch generations ( 5 /100) :accuracy(fittest mask) tensor(71.7143, dtype=torch.float64) keep_prob 0.5222222222222223\n",
      "Epoch generations ( 6 /100) :accuracy(fittest mask) tensor(70.7262, dtype=torch.float64) keep_prob 0.5277777777777778\n",
      "Epoch generations ( 7 /100) :accuracy(fittest mask) tensor(70.9762, dtype=torch.float64) keep_prob 0.5611111111111111\n",
      "Epoch generations ( 8 /100) :accuracy(fittest mask) tensor(71.5357, dtype=torch.float64) keep_prob 0.5666666666666667\n",
      "Epoch generations ( 9 /100) :accuracy(fittest mask) tensor(71.8214, dtype=torch.float64) keep_prob 0.5222222222222223\n",
      "Epoch generations ( 10 /100) :accuracy(fittest mask) tensor(72.7262, dtype=torch.float64) keep_prob 0.5388888888888889\n",
      "Epoch 0/29\n",
      "----------\n",
      "********************train*************\n",
      "Epoch [1/30] Loss: 1.3774   Acc: 72.7262\n",
      "********************val*************\n",
      "Epoch [1/30] Loss: 1.3589   Acc: 75.9167\n",
      "Epoch 1/29\n",
      "----------\n",
      "********************train*************\n",
      "Epoch [2/30] Loss: 1.3237   Acc: 73.7024\n",
      "********************val*************\n",
      "Epoch [2/30] Loss: 1.3158   Acc: 76.3333\n",
      "Epoch 2/29\n",
      "----------\n",
      "********************train*************\n",
      "Epoch [3/30] Loss: 1.2628   Acc: 74.4405\n",
      "********************val*************\n",
      "Epoch [3/30] Loss: 1.2721   Acc: 76.6944\n",
      "Epoch 3/29\n",
      "----------\n",
      "********************train*************\n",
      "Epoch [4/30] Loss: 1.2011   Acc: 74.7262\n",
      "********************val*************\n",
      "Epoch [4/30] Loss: 1.2296   Acc: 77.0000\n",
      "Epoch 4/29\n",
      "----------\n",
      "********************train*************\n",
      "Epoch [5/30] Loss: 1.1421   Acc: 74.9762\n",
      "********************val*************\n",
      "Epoch [5/30] Loss: 1.1891   Acc: 77.0833\n",
      "Epoch 5/29\n",
      "----------\n",
      "********************train*************\n",
      "Epoch [6/30] Loss: 1.0876   Acc: 75.1429\n",
      "********************val*************\n",
      "Epoch [6/30] Loss: 1.1512   Acc: 77.3333\n",
      "Epoch 6/29\n",
      "----------\n",
      "********************train*************\n",
      "Epoch [7/30] Loss: 1.0380   Acc: 75.1905\n",
      "********************val*************\n",
      "Epoch [7/30] Loss: 1.1159   Acc: 77.2778\n",
      "Epoch 7/29\n",
      "----------\n",
      "********************train*************\n",
      "Epoch [8/30] Loss: 0.9929   Acc: 75.5595\n",
      "********************val*************\n",
      "Epoch [8/30] Loss: 1.0830   Acc: 77.0833\n",
      "Epoch 8/29\n",
      "----------\n",
      "********************train*************\n",
      "Epoch [9/30] Loss: 0.9513   Acc: 76.1548\n",
      "********************val*************\n",
      "Epoch [9/30] Loss: 1.0524   Acc: 77.4722\n",
      "Epoch 9/29\n",
      "----------\n",
      "********************train*************\n",
      "Epoch [10/30] Loss: 0.9126   Acc: 76.6905\n",
      "********************val*************\n",
      "Epoch [10/30] Loss: 1.0237   Acc: 78.0000\n",
      "Epoch 10/29\n",
      "----------\n",
      "********************train*************\n",
      "Epoch [11/30] Loss: 0.8761   Acc: 77.3214\n",
      "********************val*************\n",
      "Epoch [11/30] Loss: 0.9968   Acc: 78.3333\n",
      "Epoch 11/29\n",
      "----------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "********************train*************\n",
      "Epoch [12/30] Loss: 0.8414   Acc: 78.0000\n",
      "********************val*************\n",
      "Epoch [12/30] Loss: 0.9714   Acc: 78.7222\n",
      "Epoch 12/29\n",
      "----------\n",
      "********************train*************\n",
      "Epoch [13/30] Loss: 0.8085   Acc: 78.5119\n",
      "********************val*************\n",
      "Epoch [13/30] Loss: 0.9474   Acc: 79.4167\n",
      "Epoch 13/29\n",
      "----------\n",
      "********************train*************\n",
      "Epoch [14/30] Loss: 0.7775   Acc: 79.3095\n",
      "********************val*************\n",
      "Epoch [14/30] Loss: 0.9246   Acc: 80.0000\n",
      "Epoch 14/29\n",
      "----------\n",
      "********************train*************\n",
      "Epoch [15/30] Loss: 0.7485   Acc: 80.2024\n",
      "********************val*************\n",
      "Epoch [15/30] Loss: 0.9028   Acc: 80.3889\n",
      "Epoch 15/29\n",
      "----------\n",
      "********************train*************\n",
      "Epoch [16/30] Loss: 0.7211   Acc: 80.7262\n",
      "********************val*************\n",
      "Epoch [16/30] Loss: 0.8820   Acc: 80.5556\n",
      "Epoch 16/29\n",
      "----------\n",
      "********************train*************\n",
      "Epoch [17/30] Loss: 0.6955   Acc: 81.2500\n",
      "********************val*************\n",
      "Epoch [17/30] Loss: 0.8620   Acc: 81.0000\n",
      "Epoch 17/29\n",
      "----------\n",
      "********************train*************\n",
      "Epoch [18/30] Loss: 0.6716   Acc: 81.9048\n",
      "********************val*************\n",
      "Epoch [18/30] Loss: 0.8428   Acc: 81.4444\n",
      "Epoch 18/29\n",
      "----------\n",
      "********************train*************\n",
      "Epoch [19/30] Loss: 0.6495   Acc: 82.5476\n",
      "********************val*************\n",
      "Epoch [19/30] Loss: 0.8243   Acc: 81.9167\n",
      "Epoch 19/29\n",
      "----------\n",
      "********************train*************\n",
      "Epoch [20/30] Loss: 0.6291   Acc: 83.0357\n",
      "********************val*************\n",
      "Epoch [20/30] Loss: 0.8065   Acc: 82.2500\n",
      "Epoch 20/29\n",
      "----------\n",
      "********************train*************\n",
      "Epoch [21/30] Loss: 0.6101   Acc: 83.5476\n",
      "********************val*************\n",
      "Epoch [21/30] Loss: 0.7890   Acc: 82.4722\n",
      "Epoch 21/29\n",
      "----------\n",
      "********************train*************\n",
      "Epoch [22/30] Loss: 0.5921   Acc: 84.0833\n",
      "********************val*************\n",
      "Epoch [22/30] Loss: 0.7719   Acc: 83.0556\n",
      "Epoch 22/29\n",
      "----------\n",
      "********************train*************\n",
      "Epoch [23/30] Loss: 0.5747   Acc: 84.5238\n",
      "********************val*************\n",
      "Epoch [23/30] Loss: 0.7553   Acc: 83.3611\n",
      "Epoch 23/29\n",
      "----------\n",
      "********************train*************\n",
      "Epoch [24/30] Loss: 0.5578   Acc: 84.8929\n",
      "********************val*************\n",
      "Epoch [24/30] Loss: 0.7392   Acc: 83.3611\n",
      "Epoch 24/29\n",
      "----------\n",
      "********************train*************\n",
      "Epoch [25/30] Loss: 0.5413   Acc: 85.2024\n",
      "********************val*************\n",
      "Epoch [25/30] Loss: 0.7237   Acc: 83.6944\n",
      "Epoch 25/29\n",
      "----------\n",
      "********************train*************\n",
      "Epoch [26/30] Loss: 0.5256   Acc: 85.6190\n",
      "********************val*************\n",
      "Epoch [26/30] Loss: 0.7091   Acc: 83.7222\n",
      "Epoch 26/29\n",
      "----------\n",
      "********************train*************\n",
      "Epoch [27/30] Loss: 0.5108   Acc: 86.0595\n",
      "********************val*************\n",
      "Epoch [27/30] Loss: 0.6953   Acc: 84.0556\n",
      "Epoch 27/29\n",
      "----------\n",
      "********************train*************\n",
      "Epoch [28/30] Loss: 0.4969   Acc: 86.2976\n",
      "********************val*************\n",
      "Epoch [28/30] Loss: 0.6824   Acc: 84.5278\n",
      "Epoch 28/29\n",
      "----------\n",
      "********************train*************\n",
      "Epoch [29/30] Loss: 0.4838   Acc: 86.6310\n",
      "********************val*************\n",
      "Epoch [29/30] Loss: 0.6701   Acc: 84.9167\n",
      "Epoch 29/29\n",
      "----------\n",
      "********************train*************\n",
      "Epoch [30/30] Loss: 0.4715   Acc: 87.0119\n",
      "********************val*************\n",
      "Epoch [30/30] Loss: 0.6583   Acc: 85.2778\n",
      "Training complete in 0m 2s\n",
      "Best val Acc: 85.277778\n",
      "Epoch generations ( 11 /100) :accuracy(fittest mask) tensor(85.9167, dtype=torch.float64) keep_prob 0.5666666666666667\n",
      "Epoch generations ( 12 /100) :accuracy(fittest mask) tensor(85.9286, dtype=torch.float64) keep_prob 0.5722222222222222\n",
      "Epoch generations ( 13 /100) :accuracy(fittest mask) tensor(86.2619, dtype=torch.float64) keep_prob 0.5611111111111111\n",
      "Epoch generations ( 14 /100) :accuracy(fittest mask) tensor(86.5833, dtype=torch.float64) keep_prob 0.5277777777777778\n",
      "Epoch generations ( 15 /100) :accuracy(fittest mask) tensor(85.8095, dtype=torch.float64) keep_prob 0.5444444444444444\n",
      "Epoch generations ( 16 /100) :accuracy(fittest mask) tensor(85.0595, dtype=torch.float64) keep_prob 0.5166666666666667\n",
      "Epoch generations ( 17 /100) :accuracy(fittest mask) tensor(84.8333, dtype=torch.float64) keep_prob 0.5111111111111111\n",
      "Epoch generations ( 18 /100) :accuracy(fittest mask) tensor(83.9881, dtype=torch.float64) keep_prob 0.5777777777777777\n",
      "Epoch generations ( 19 /100) :accuracy(fittest mask) tensor(83.0714, dtype=torch.float64) keep_prob 0.5555555555555556\n",
      "Epoch generations ( 20 /100) :accuracy(fittest mask) tensor(83.7500, dtype=torch.float64) keep_prob 0.5055555555555555\n",
      "Epoch 0/29\n",
      "----------\n",
      "********************train*************\n",
      "Epoch [1/30] Loss: 0.6373   Acc: 83.7500\n",
      "********************val*************\n",
      "Epoch [1/30] Loss: 0.6458   Acc: 85.4444\n",
      "Epoch 1/29\n",
      "----------\n",
      "********************train*************\n",
      "Epoch [2/30] Loss: 0.6096   Acc: 84.6786\n",
      "********************val*************\n",
      "Epoch [2/30] Loss: 0.6343   Acc: 85.6667\n",
      "Epoch 2/29\n",
      "----------\n",
      "********************train*************\n",
      "Epoch [3/30] Loss: 0.5791   Acc: 85.7738\n",
      "********************val*************\n",
      "Epoch [3/30] Loss: 0.6245   Acc: 85.3889\n",
      "Epoch 3/29\n",
      "----------\n",
      "********************train*************\n",
      "Epoch [4/30] Loss: 0.5515   Acc: 86.1190\n",
      "********************val*************\n",
      "Epoch [4/30] Loss: 0.6164   Acc: 85.3889\n",
      "Epoch 4/29\n",
      "----------\n",
      "********************train*************\n",
      "Epoch [5/30] Loss: 0.5292   Acc: 86.7262\n",
      "********************val*************\n",
      "Epoch [5/30] Loss: 0.6092   Acc: 84.9444\n",
      "Epoch 5/29\n",
      "----------\n",
      "********************train*************\n",
      "Epoch [6/30] Loss: 0.5116   Acc: 86.9881\n",
      "********************val*************\n",
      "Epoch [6/30] Loss: 0.6018   Acc: 84.6944\n",
      "Epoch 6/29\n",
      "----------\n",
      "********************train*************\n",
      "Epoch [7/30] Loss: 0.4967   Acc: 87.2143\n",
      "********************val*************\n",
      "Epoch [7/30] Loss: 0.5937   Acc: 84.6944\n",
      "Epoch 7/29\n",
      "----------\n",
      "********************train*************\n",
      "Epoch [8/30] Loss: 0.4829   Acc: 87.2857\n",
      "********************val*************\n",
      "Epoch [8/30] Loss: 0.5846   Acc: 84.8333\n",
      "Epoch 8/29\n",
      "----------\n",
      "********************train*************\n",
      "Epoch [9/30] Loss: 0.4693   Acc: 87.6667\n",
      "********************val*************\n",
      "Epoch [9/30] Loss: 0.5748   Acc: 85.1111\n",
      "Epoch 9/29\n",
      "----------\n",
      "********************train*************\n",
      "Epoch [10/30] Loss: 0.4556   Acc: 88.0000\n",
      "********************val*************\n",
      "Epoch [10/30] Loss: 0.5648   Acc: 85.5278\n",
      "Epoch 10/29\n",
      "----------\n",
      "********************train*************\n",
      "Epoch [11/30] Loss: 0.4422   Acc: 88.3095\n",
      "********************val*************\n",
      "Epoch [11/30] Loss: 0.5548   Acc: 85.7500\n",
      "Epoch 11/29\n",
      "----------\n",
      "********************train*************\n",
      "Epoch [12/30] Loss: 0.4293   Acc: 88.6429\n",
      "********************val*************\n",
      "Epoch [12/30] Loss: 0.5450   Acc: 86.0833\n",
      "Epoch 12/29\n",
      "----------\n",
      "********************train*************\n",
      "Epoch [13/30] Loss: 0.4170   Acc: 88.9167\n",
      "********************val*************\n",
      "Epoch [13/30] Loss: 0.5356   Acc: 86.2500\n",
      "Epoch 13/29\n",
      "----------\n",
      "********************train*************\n",
      "Epoch [14/30] Loss: 0.4055   Acc: 89.0476\n",
      "********************val*************\n",
      "Epoch [14/30] Loss: 0.5264   Acc: 86.6111\n",
      "Epoch 14/29\n",
      "----------\n",
      "********************train*************\n",
      "Epoch [15/30] Loss: 0.3948   Acc: 89.4167\n",
      "********************val*************\n",
      "Epoch [15/30] Loss: 0.5177   Acc: 86.8611\n",
      "Epoch 15/29\n",
      "----------\n",
      "********************train*************\n",
      "Epoch [16/30] Loss: 0.3849   Acc: 89.5952\n",
      "********************val*************\n",
      "Epoch [16/30] Loss: 0.5093   Acc: 87.1389\n",
      "Epoch 16/29\n",
      "----------\n",
      "********************train*************\n",
      "Epoch [17/30] Loss: 0.3757   Acc: 89.9286\n",
      "********************val*************\n",
      "Epoch [17/30] Loss: 0.5013   Acc: 87.2500\n",
      "Epoch 17/29\n",
      "----------\n",
      "********************train*************\n",
      "Epoch [18/30] Loss: 0.3668   Acc: 90.0833\n",
      "********************val*************\n",
      "Epoch [18/30] Loss: 0.4938   Acc: 87.4722\n",
      "Epoch 18/29\n",
      "----------\n",
      "********************train*************\n",
      "Epoch [19/30] Loss: 0.3583   Acc: 90.3571\n",
      "********************val*************\n",
      "Epoch [19/30] Loss: 0.4867   Acc: 87.6111\n",
      "Epoch 19/29\n",
      "----------\n",
      "********************train*************\n",
      "Epoch [20/30] Loss: 0.3499   Acc: 90.6667\n",
      "********************val*************\n",
      "Epoch [20/30] Loss: 0.4800   Acc: 87.6944\n",
      "Epoch 20/29\n",
      "----------\n",
      "********************train*************\n",
      "Epoch [21/30] Loss: 0.3418   Acc: 90.7738\n",
      "********************val*************\n",
      "Epoch [21/30] Loss: 0.4736   Acc: 87.9444\n",
      "Epoch 21/29\n",
      "----------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "********************train*************\n",
      "Epoch [22/30] Loss: 0.3339   Acc: 91.0238\n",
      "********************val*************\n",
      "Epoch [22/30] Loss: 0.4675   Acc: 88.0556\n",
      "Epoch 22/29\n",
      "----------\n",
      "********************train*************\n",
      "Epoch [23/30] Loss: 0.3264   Acc: 91.1786\n",
      "********************val*************\n",
      "Epoch [23/30] Loss: 0.4615   Acc: 88.1389\n",
      "Epoch 23/29\n",
      "----------\n",
      "********************train*************\n",
      "Epoch [24/30] Loss: 0.3192   Acc: 91.5119\n",
      "********************val*************\n",
      "Epoch [24/30] Loss: 0.4555   Acc: 88.3333\n",
      "Epoch 24/29\n",
      "----------\n",
      "********************train*************\n",
      "Epoch [25/30] Loss: 0.3123   Acc: 91.6548\n",
      "********************val*************\n",
      "Epoch [25/30] Loss: 0.4495   Acc: 88.3611\n",
      "Epoch 25/29\n",
      "----------\n",
      "********************train*************\n",
      "Epoch [26/30] Loss: 0.3058   Acc: 91.7619\n",
      "********************val*************\n",
      "Epoch [26/30] Loss: 0.4435   Acc: 88.4722\n",
      "Epoch 26/29\n",
      "----------\n",
      "********************train*************\n",
      "Epoch [27/30] Loss: 0.2995   Acc: 91.9286\n",
      "********************val*************\n",
      "Epoch [27/30] Loss: 0.4376   Acc: 88.6389\n",
      "Epoch 27/29\n",
      "----------\n",
      "********************train*************\n",
      "Epoch [28/30] Loss: 0.2934   Acc: 92.0119\n",
      "********************val*************\n",
      "Epoch [28/30] Loss: 0.4318   Acc: 88.8056\n",
      "Epoch 28/29\n",
      "----------\n",
      "********************train*************\n",
      "Epoch [29/30] Loss: 0.2877   Acc: 92.0476\n",
      "********************val*************\n",
      "Epoch [29/30] Loss: 0.4262   Acc: 89.1111\n",
      "Epoch 29/29\n",
      "----------\n",
      "********************train*************\n",
      "Epoch [30/30] Loss: 0.2821   Acc: 92.2143\n",
      "********************val*************\n",
      "Epoch [30/30] Loss: 0.4208   Acc: 89.2778\n",
      "Training complete in 0m 2s\n",
      "Best val Acc: 89.277778\n",
      "Epoch generations ( 21 /100) :accuracy(fittest mask) tensor(91.4524, dtype=torch.float64) keep_prob 0.5555555555555556\n",
      "Epoch generations ( 22 /100) :accuracy(fittest mask) tensor(91.8571, dtype=torch.float64) keep_prob 0.5444444444444444\n",
      "Epoch generations ( 23 /100) :accuracy(fittest mask) tensor(90.2024, dtype=torch.float64) keep_prob 0.5666666666666667\n",
      "Epoch generations ( 24 /100) :accuracy(fittest mask) tensor(89.2976, dtype=torch.float64) keep_prob 0.5444444444444444\n",
      "Epoch generations ( 25 /100) :accuracy(fittest mask) tensor(89.0357, dtype=torch.float64) keep_prob 0.5944444444444444\n",
      "Epoch generations ( 26 /100) :accuracy(fittest mask) tensor(88.8929, dtype=torch.float64) keep_prob 0.6111111111111112\n",
      "Epoch generations ( 27 /100) :accuracy(fittest mask) tensor(89.3929, dtype=torch.float64) keep_prob 0.6111111111111112\n",
      "Epoch generations ( 28 /100) :accuracy(fittest mask) tensor(88.9881, dtype=torch.float64) keep_prob 0.6166666666666667\n",
      "Epoch generations ( 29 /100) :accuracy(fittest mask) tensor(88.8214, dtype=torch.float64) keep_prob 0.5611111111111111\n",
      "Epoch generations ( 30 /100) :accuracy(fittest mask) tensor(88.2024, dtype=torch.float64) keep_prob 0.6\n",
      "Epoch 0/29\n",
      "----------\n",
      "********************train*************\n",
      "Epoch [1/30] Loss: 0.3974   Acc: 88.2024\n",
      "********************val*************\n",
      "Epoch [1/30] Loss: 0.4156   Acc: 89.4444\n",
      "Epoch 1/29\n",
      "----------\n",
      "********************train*************\n",
      "Epoch [2/30] Loss: 0.3789   Acc: 89.2738\n",
      "********************val*************\n",
      "Epoch [2/30] Loss: 0.4129   Acc: 89.6667\n",
      "Epoch 2/29\n",
      "----------\n",
      "********************train*************\n",
      "Epoch [3/30] Loss: 0.3577   Acc: 90.4762\n",
      "********************val*************\n",
      "Epoch [3/30] Loss: 0.4139   Acc: 89.7500\n",
      "Epoch 3/29\n",
      "----------\n",
      "********************train*************\n",
      "Epoch [4/30] Loss: 0.3409   Acc: 91.7500\n",
      "********************val*************\n",
      "Epoch [4/30] Loss: 0.4176   Acc: 89.4167\n",
      "Epoch 4/29\n",
      "----------\n",
      "********************train*************\n",
      "Epoch [5/30] Loss: 0.3307   Acc: 92.0476\n",
      "********************val*************\n",
      "Epoch [5/30] Loss: 0.4199   Acc: 89.0833\n",
      "Epoch 5/29\n",
      "----------\n",
      "********************train*************\n",
      "Epoch [6/30] Loss: 0.3242   Acc: 92.1548\n",
      "********************val*************\n",
      "Epoch [6/30] Loss: 0.4181   Acc: 88.8333\n",
      "Epoch 6/29\n",
      "----------\n",
      "********************train*************\n",
      "Epoch [7/30] Loss: 0.3174   Acc: 92.2976\n",
      "********************val*************\n",
      "Epoch [7/30] Loss: 0.4121   Acc: 88.9167\n",
      "Epoch 7/29\n",
      "----------\n",
      "********************train*************\n",
      "Epoch [8/30] Loss: 0.3095   Acc: 92.3214\n",
      "********************val*************\n",
      "Epoch [8/30] Loss: 0.4039   Acc: 88.8611\n",
      "Epoch 8/29\n",
      "----------\n",
      "********************train*************\n",
      "Epoch [9/30] Loss: 0.3019   Acc: 92.2738\n",
      "********************val*************\n",
      "Epoch [9/30] Loss: 0.3957   Acc: 89.2778\n",
      "Epoch 9/29\n",
      "----------\n",
      "********************train*************\n",
      "Epoch [10/30] Loss: 0.2956   Acc: 92.2857\n",
      "********************val*************\n",
      "Epoch [10/30] Loss: 0.3887   Acc: 89.5556\n",
      "Epoch 10/29\n",
      "----------\n",
      "********************train*************\n",
      "Epoch [11/30] Loss: 0.2903   Acc: 92.3690\n",
      "********************val*************\n",
      "Epoch [11/30] Loss: 0.3833   Acc: 89.8056\n",
      "Epoch 11/29\n",
      "----------\n",
      "********************train*************\n",
      "Epoch [12/30] Loss: 0.2850   Acc: 92.3452\n",
      "********************val*************\n",
      "Epoch [12/30] Loss: 0.3795   Acc: 90.0000\n",
      "Epoch 12/29\n",
      "----------\n",
      "********************train*************\n",
      "Epoch [13/30] Loss: 0.2795   Acc: 92.4881\n",
      "********************val*************\n",
      "Epoch [13/30] Loss: 0.3770   Acc: 90.3333\n",
      "Epoch 13/29\n",
      "----------\n",
      "********************train*************\n",
      "Epoch [14/30] Loss: 0.2743   Acc: 92.7024\n",
      "********************val*************\n",
      "Epoch [14/30] Loss: 0.3754   Acc: 90.1389\n",
      "Epoch 14/29\n",
      "----------\n",
      "********************train*************\n",
      "Epoch [15/30] Loss: 0.2698   Acc: 92.9405\n",
      "********************val*************\n",
      "Epoch [15/30] Loss: 0.3739   Acc: 89.9722\n",
      "Epoch 15/29\n",
      "----------\n",
      "********************train*************\n",
      "Epoch [16/30] Loss: 0.2657   Acc: 93.0476\n",
      "********************val*************\n",
      "Epoch [16/30] Loss: 0.3715   Acc: 90.1944\n",
      "Epoch 16/29\n",
      "----------\n",
      "********************train*************\n",
      "Epoch [17/30] Loss: 0.2614   Acc: 93.2024\n",
      "********************val*************\n",
      "Epoch [17/30] Loss: 0.3678   Acc: 90.1667\n",
      "Epoch 17/29\n",
      "----------\n",
      "********************train*************\n",
      "Epoch [18/30] Loss: 0.2568   Acc: 93.2857\n",
      "********************val*************\n",
      "Epoch [18/30] Loss: 0.3627   Acc: 90.2778\n",
      "Epoch 18/29\n",
      "----------\n",
      "********************train*************\n",
      "Epoch [19/30] Loss: 0.2518   Acc: 93.2738\n",
      "********************val*************\n",
      "Epoch [19/30] Loss: 0.3569   Acc: 90.1944\n",
      "Epoch 19/29\n",
      "----------\n",
      "********************train*************\n",
      "Epoch [20/30] Loss: 0.2470   Acc: 93.4524\n",
      "********************val*************\n",
      "Epoch [20/30] Loss: 0.3510   Acc: 90.5000\n",
      "Epoch 20/29\n",
      "----------\n",
      "********************train*************\n",
      "Epoch [21/30] Loss: 0.2427   Acc: 93.5357\n",
      "********************val*************\n",
      "Epoch [21/30] Loss: 0.3457   Acc: 91.1111\n",
      "Epoch 21/29\n",
      "----------\n",
      "********************train*************\n",
      "Epoch [22/30] Loss: 0.2388   Acc: 93.6548\n",
      "********************val*************\n",
      "Epoch [22/30] Loss: 0.3411   Acc: 91.2500\n",
      "Epoch 22/29\n",
      "----------\n",
      "********************train*************\n",
      "Epoch [23/30] Loss: 0.2350   Acc: 93.7976\n",
      "********************val*************\n",
      "Epoch [23/30] Loss: 0.3373   Acc: 91.2500\n",
      "Epoch 23/29\n",
      "----------\n",
      "********************train*************\n",
      "Epoch [24/30] Loss: 0.2311   Acc: 93.8929\n",
      "********************val*************\n",
      "Epoch [24/30] Loss: 0.3342   Acc: 91.3611\n",
      "Epoch 24/29\n",
      "----------\n",
      "********************train*************\n",
      "Epoch [25/30] Loss: 0.2273   Acc: 93.9167\n",
      "********************val*************\n",
      "Epoch [25/30] Loss: 0.3316   Acc: 91.3611\n",
      "Epoch 25/29\n",
      "----------\n",
      "********************train*************\n",
      "Epoch [26/30] Loss: 0.2237   Acc: 94.1071\n",
      "********************val*************\n",
      "Epoch [26/30] Loss: 0.3290   Acc: 91.3611\n",
      "Epoch 26/29\n",
      "----------\n",
      "********************train*************\n",
      "Epoch [27/30] Loss: 0.2204   Acc: 94.2024\n",
      "********************val*************\n",
      "Epoch [27/30] Loss: 0.3261   Acc: 91.3889\n",
      "Epoch 27/29\n",
      "----------\n",
      "********************train*************\n",
      "Epoch [28/30] Loss: 0.2172   Acc: 94.2738\n",
      "********************val*************\n",
      "Epoch [28/30] Loss: 0.3226   Acc: 91.3889\n",
      "Epoch 28/29\n",
      "----------\n",
      "********************train*************\n",
      "Epoch [29/30] Loss: 0.2140   Acc: 94.3690\n",
      "********************val*************\n",
      "Epoch [29/30] Loss: 0.3186   Acc: 91.5278\n",
      "Epoch 29/29\n",
      "----------\n",
      "********************train*************\n",
      "Epoch [30/30] Loss: 0.2109   Acc: 94.4762\n",
      "********************val*************\n",
      "Epoch [30/30] Loss: 0.3143   Acc: 91.6389\n",
      "Training complete in 0m 3s\n",
      "Best val Acc: 91.638889\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch generations ( 31 /100) :accuracy(fittest mask) tensor(94.4167, dtype=torch.float64) keep_prob 0.6111111111111112\n",
      "Epoch generations ( 32 /100) :accuracy(fittest mask) tensor(93.8690, dtype=torch.float64) keep_prob 0.5611111111111111\n",
      "Epoch generations ( 33 /100) :accuracy(fittest mask) tensor(93.6190, dtype=torch.float64) keep_prob 0.5888888888888889\n",
      "Epoch generations ( 34 /100) :accuracy(fittest mask) tensor(93.7262, dtype=torch.float64) keep_prob 0.5833333333333334\n",
      "Epoch generations ( 35 /100) :accuracy(fittest mask) tensor(93.4762, dtype=torch.float64) keep_prob 0.6166666666666667\n",
      "Epoch generations ( 36 /100) :accuracy(fittest mask) tensor(93.5476, dtype=torch.float64) keep_prob 0.6444444444444445\n",
      "Epoch generations ( 37 /100) :accuracy(fittest mask) tensor(93.5476, dtype=torch.float64) keep_prob 0.6444444444444445\n",
      "Epoch generations ( 38 /100) :accuracy(fittest mask) tensor(93.2143, dtype=torch.float64) keep_prob 0.6444444444444445\n",
      "Epoch generations ( 39 /100) :accuracy(fittest mask) tensor(93.9762, dtype=torch.float64) keep_prob 0.6166666666666667\n",
      "Epoch generations ( 40 /100) :accuracy(fittest mask) tensor(93.9643, dtype=torch.float64) keep_prob 0.6277777777777778\n",
      "Epoch 0/29\n",
      "----------\n",
      "********************train*************\n",
      "Epoch [1/30] Loss: 0.2345   Acc: 93.9643\n",
      "********************val*************\n",
      "Epoch [1/30] Loss: 0.3084   Acc: 91.8611\n",
      "Epoch 1/29\n",
      "----------\n",
      "********************train*************\n",
      "Epoch [2/30] Loss: 0.2287   Acc: 94.1310\n",
      "********************val*************\n",
      "Epoch [2/30] Loss: 0.3028   Acc: 92.0556\n",
      "Epoch 2/29\n",
      "----------\n",
      "********************train*************\n",
      "Epoch [3/30] Loss: 0.2223   Acc: 94.2262\n",
      "********************val*************\n",
      "Epoch [3/30] Loss: 0.2988   Acc: 92.1111\n",
      "Epoch 3/29\n",
      "----------\n",
      "********************train*************\n",
      "Epoch [4/30] Loss: 0.2166   Acc: 94.2619\n",
      "********************val*************\n",
      "Epoch [4/30] Loss: 0.2967   Acc: 92.0278\n",
      "Epoch 4/29\n",
      "----------\n",
      "********************train*************\n",
      "Epoch [5/30] Loss: 0.2117   Acc: 94.4881\n",
      "********************val*************\n",
      "Epoch [5/30] Loss: 0.2964   Acc: 92.0278\n",
      "Epoch 5/29\n",
      "----------\n",
      "********************train*************\n",
      "Epoch [6/30] Loss: 0.2075   Acc: 94.5833\n",
      "********************val*************\n",
      "Epoch [6/30] Loss: 0.2973   Acc: 91.9167\n",
      "Epoch 6/29\n",
      "----------\n",
      "********************train*************\n",
      "Epoch [7/30] Loss: 0.2043   Acc: 94.5595\n",
      "********************val*************\n",
      "Epoch [7/30] Loss: 0.2988   Acc: 91.8611\n",
      "Epoch 7/29\n",
      "----------\n",
      "********************train*************\n",
      "Epoch [8/30] Loss: 0.2019   Acc: 94.6548\n",
      "********************val*************\n",
      "Epoch [8/30] Loss: 0.2999   Acc: 91.8611\n",
      "Epoch 8/29\n",
      "----------\n",
      "********************train*************\n",
      "Epoch [9/30] Loss: 0.2001   Acc: 94.6429\n",
      "********************val*************\n",
      "Epoch [9/30] Loss: 0.2998   Acc: 91.7778\n",
      "Epoch 9/29\n",
      "----------\n",
      "********************train*************\n",
      "Epoch [10/30] Loss: 0.1982   Acc: 94.7857\n",
      "********************val*************\n",
      "Epoch [10/30] Loss: 0.2980   Acc: 91.8333\n",
      "Epoch 10/29\n",
      "----------\n",
      "********************train*************\n",
      "Epoch [11/30] Loss: 0.1957   Acc: 94.9167\n",
      "********************val*************\n",
      "Epoch [11/30] Loss: 0.2945   Acc: 91.9167\n",
      "Epoch 11/29\n",
      "----------\n",
      "********************train*************\n",
      "Epoch [12/30] Loss: 0.1927   Acc: 94.9762\n",
      "********************val*************\n",
      "Epoch [12/30] Loss: 0.2899   Acc: 92.0556\n",
      "Epoch 12/29\n",
      "----------\n",
      "********************train*************\n",
      "Epoch [13/30] Loss: 0.1895   Acc: 94.9048\n",
      "********************val*************\n",
      "Epoch [13/30] Loss: 0.2849   Acc: 92.3056\n",
      "Epoch 13/29\n",
      "----------\n",
      "********************train*************\n",
      "Epoch [14/30] Loss: 0.1864   Acc: 95.0833\n",
      "********************val*************\n",
      "Epoch [14/30] Loss: 0.2800   Acc: 92.4444\n",
      "Epoch 14/29\n",
      "----------\n",
      "********************train*************\n",
      "Epoch [15/30] Loss: 0.1834   Acc: 95.2143\n",
      "********************val*************\n",
      "Epoch [15/30] Loss: 0.2756   Acc: 92.6389\n",
      "Epoch 15/29\n",
      "----------\n",
      "********************train*************\n",
      "Epoch [16/30] Loss: 0.1808   Acc: 95.2857\n",
      "********************val*************\n",
      "Epoch [16/30] Loss: 0.2720   Acc: 92.7222\n",
      "Epoch 16/29\n",
      "----------\n",
      "********************train*************\n",
      "Epoch [17/30] Loss: 0.1784   Acc: 95.5238\n",
      "********************val*************\n",
      "Epoch [17/30] Loss: 0.2691   Acc: 92.6944\n",
      "Epoch 17/29\n",
      "----------\n",
      "********************train*************\n",
      "Epoch [18/30] Loss: 0.1763   Acc: 95.5833\n",
      "********************val*************\n",
      "Epoch [18/30] Loss: 0.2667   Acc: 92.8056\n",
      "Epoch 18/29\n",
      "----------\n",
      "********************train*************\n",
      "Epoch [19/30] Loss: 0.1744   Acc: 95.6190\n",
      "********************val*************\n",
      "Epoch [19/30] Loss: 0.2645   Acc: 92.9444\n",
      "Epoch 19/29\n",
      "----------\n",
      "********************train*************\n",
      "Epoch [20/30] Loss: 0.1726   Acc: 95.6429\n",
      "********************val*************\n",
      "Epoch [20/30] Loss: 0.2624   Acc: 93.0278\n",
      "Epoch 20/29\n",
      "----------\n",
      "********************train*************\n",
      "Epoch [21/30] Loss: 0.1706   Acc: 95.6429\n",
      "********************val*************\n",
      "Epoch [21/30] Loss: 0.2602   Acc: 93.0278\n",
      "Epoch 21/29\n",
      "----------\n",
      "********************train*************\n",
      "Epoch [22/30] Loss: 0.1684   Acc: 95.6786\n",
      "********************val*************\n",
      "Epoch [22/30] Loss: 0.2581   Acc: 93.1111\n",
      "Epoch 22/29\n",
      "----------\n",
      "********************train*************\n",
      "Epoch [23/30] Loss: 0.1662   Acc: 95.6786\n",
      "********************val*************\n",
      "Epoch [23/30] Loss: 0.2561   Acc: 93.0000\n",
      "Epoch 23/29\n",
      "----------\n",
      "********************train*************\n",
      "Epoch [24/30] Loss: 0.1640   Acc: 95.8214\n",
      "********************val*************\n",
      "Epoch [24/30] Loss: 0.2543   Acc: 93.0000\n",
      "Epoch 24/29\n",
      "----------\n",
      "********************train*************\n",
      "Epoch [25/30] Loss: 0.1619   Acc: 95.8452\n",
      "********************val*************\n",
      "Epoch [25/30] Loss: 0.2528   Acc: 93.1111\n",
      "Epoch 25/29\n",
      "----------\n",
      "********************train*************\n",
      "Epoch [26/30] Loss: 0.1599   Acc: 95.8810\n",
      "********************val*************\n",
      "Epoch [26/30] Loss: 0.2515   Acc: 93.0278\n",
      "Epoch 26/29\n",
      "----------\n",
      "********************train*************\n",
      "Epoch [27/30] Loss: 0.1579   Acc: 95.9286\n",
      "********************val*************\n",
      "Epoch [27/30] Loss: 0.2504   Acc: 93.0278\n",
      "Epoch 27/29\n",
      "----------\n",
      "********************train*************\n",
      "Epoch [28/30] Loss: 0.1560   Acc: 95.9762\n",
      "********************val*************\n",
      "Epoch [28/30] Loss: 0.2493   Acc: 92.9722\n",
      "Epoch 28/29\n",
      "----------\n",
      "********************train*************\n",
      "Epoch [29/30] Loss: 0.1543   Acc: 95.9881\n",
      "********************val*************\n",
      "Epoch [29/30] Loss: 0.2480   Acc: 92.9444\n",
      "Epoch 29/29\n",
      "----------\n",
      "********************train*************\n",
      "Epoch [30/30] Loss: 0.1526   Acc: 96.0595\n",
      "********************val*************\n",
      "Epoch [30/30] Loss: 0.2463   Acc: 93.0278\n",
      "Training complete in 0m 2s\n",
      "Best val Acc: 93.111111\n",
      "Epoch generations ( 41 /100) :accuracy(fittest mask) tensor(94.9524, dtype=torch.float64) keep_prob 0.6333333333333333\n",
      "Epoch generations ( 42 /100) :accuracy(fittest mask) tensor(94.6548, dtype=torch.float64) keep_prob 0.6555555555555556\n",
      "Epoch generations ( 43 /100) :accuracy(fittest mask) tensor(94.5714, dtype=torch.float64) keep_prob 0.6666666666666666\n",
      "Epoch generations ( 44 /100) :accuracy(fittest mask) tensor(94.6905, dtype=torch.float64) keep_prob 0.6722222222222223\n",
      "Epoch generations ( 45 /100) :accuracy(fittest mask) tensor(94.7500, dtype=torch.float64) keep_prob 0.6777777777777778\n",
      "Epoch generations ( 46 /100) :accuracy(fittest mask) tensor(94.6190, dtype=torch.float64) keep_prob 0.6777777777777778\n",
      "Epoch generations ( 47 /100) :accuracy(fittest mask) tensor(94.3452, dtype=torch.float64) keep_prob 0.6777777777777778\n",
      "Epoch generations ( 48 /100) :accuracy(fittest mask) tensor(94.1310, dtype=torch.float64) keep_prob 0.6722222222222223\n",
      "Epoch generations ( 49 /100) :accuracy(fittest mask) tensor(94.5833, dtype=torch.float64) keep_prob 0.7055555555555556\n",
      "Epoch generations ( 50 /100) :accuracy(fittest mask) tensor(94.7143, dtype=torch.float64) keep_prob 0.7055555555555556\n",
      "Epoch 0/29\n",
      "----------\n",
      "********************train*************\n",
      "Epoch [1/30] Loss: 0.2012   Acc: 94.7143\n",
      "********************val*************\n",
      "Epoch [1/30] Loss: 0.2537   Acc: 93.2778\n",
      "Epoch 1/29\n",
      "----------\n",
      "********************train*************\n",
      "Epoch [2/30] Loss: 0.1969   Acc: 94.9524\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "********************val*************\n",
      "Epoch [2/30] Loss: 0.2488   Acc: 93.3889\n",
      "Epoch 2/29\n",
      "----------\n",
      "********************train*************\n",
      "Epoch [3/30] Loss: 0.1921   Acc: 95.2500\n",
      "********************val*************\n",
      "Epoch [3/30] Loss: 0.2445   Acc: 93.6389\n",
      "Epoch 3/29\n",
      "----------\n",
      "********************train*************\n",
      "Epoch [4/30] Loss: 0.1883   Acc: 95.2857\n",
      "********************val*************\n",
      "Epoch [4/30] Loss: 0.2413   Acc: 93.8333\n",
      "Epoch 4/29\n",
      "----------\n",
      "********************train*************\n",
      "Epoch [5/30] Loss: 0.1853   Acc: 95.4643\n",
      "********************val*************\n",
      "Epoch [5/30] Loss: 0.2389   Acc: 93.8889\n",
      "Epoch 5/29\n",
      "----------\n",
      "********************train*************\n",
      "Epoch [6/30] Loss: 0.1825   Acc: 95.5595\n",
      "********************val*************\n",
      "Epoch [6/30] Loss: 0.2372   Acc: 94.0556\n",
      "Epoch 6/29\n",
      "----------\n",
      "********************train*************\n",
      "Epoch [7/30] Loss: 0.1798   Acc: 95.8929\n",
      "********************val*************\n",
      "Epoch [7/30] Loss: 0.2362   Acc: 94.0000\n",
      "Epoch 7/29\n",
      "----------\n",
      "********************train*************\n",
      "Epoch [8/30] Loss: 0.1777   Acc: 95.9881\n",
      "********************val*************\n",
      "Epoch [8/30] Loss: 0.2355   Acc: 93.9722\n",
      "Epoch 8/29\n",
      "----------\n",
      "********************train*************\n",
      "Epoch [9/30] Loss: 0.1760   Acc: 95.9167\n",
      "********************val*************\n",
      "Epoch [9/30] Loss: 0.2342   Acc: 93.9722\n",
      "Epoch 9/29\n",
      "----------\n",
      "********************train*************\n",
      "Epoch [10/30] Loss: 0.1741   Acc: 95.8690\n",
      "********************val*************\n",
      "Epoch [10/30] Loss: 0.2322   Acc: 94.0556\n",
      "Epoch 10/29\n",
      "----------\n",
      "********************train*************\n",
      "Epoch [11/30] Loss: 0.1717   Acc: 95.8095\n",
      "********************val*************\n",
      "Epoch [11/30] Loss: 0.2298   Acc: 94.0278\n",
      "Epoch 11/29\n",
      "----------\n",
      "********************train*************\n",
      "Epoch [12/30] Loss: 0.1690   Acc: 95.9286\n",
      "********************val*************\n",
      "Epoch [12/30] Loss: 0.2274   Acc: 94.0000\n",
      "Epoch 12/29\n",
      "----------\n",
      "********************train*************\n",
      "Epoch [13/30] Loss: 0.1666   Acc: 96.0238\n",
      "********************val*************\n",
      "Epoch [13/30] Loss: 0.2252   Acc: 93.9444\n",
      "Epoch 13/29\n",
      "----------\n",
      "********************train*************\n",
      "Epoch [14/30] Loss: 0.1643   Acc: 96.0714\n",
      "********************val*************\n",
      "Epoch [14/30] Loss: 0.2232   Acc: 93.9444\n",
      "Epoch 14/29\n",
      "----------\n",
      "********************train*************\n",
      "Epoch [15/30] Loss: 0.1621   Acc: 96.1310\n",
      "********************val*************\n",
      "Epoch [15/30] Loss: 0.2214   Acc: 93.9167\n",
      "Epoch 15/29\n",
      "----------\n",
      "********************train*************\n",
      "Epoch [16/30] Loss: 0.1599   Acc: 96.0833\n",
      "********************val*************\n",
      "Epoch [16/30] Loss: 0.2197   Acc: 94.0000\n",
      "Epoch 16/29\n",
      "----------\n",
      "********************train*************\n",
      "Epoch [17/30] Loss: 0.1577   Acc: 96.0833\n",
      "********************val*************\n",
      "Epoch [17/30] Loss: 0.2181   Acc: 94.2222\n",
      "Epoch 17/29\n",
      "----------\n",
      "********************train*************\n",
      "Epoch [18/30] Loss: 0.1557   Acc: 96.1786\n",
      "********************val*************\n",
      "Epoch [18/30] Loss: 0.2162   Acc: 94.2222\n",
      "Epoch 18/29\n",
      "----------\n",
      "********************train*************\n",
      "Epoch [19/30] Loss: 0.1538   Acc: 96.2024\n",
      "********************val*************\n",
      "Epoch [19/30] Loss: 0.2139   Acc: 94.3333\n",
      "Epoch 19/29\n",
      "----------\n",
      "********************train*************\n",
      "Epoch [20/30] Loss: 0.1518   Acc: 96.1905\n",
      "********************val*************\n",
      "Epoch [20/30] Loss: 0.2113   Acc: 94.4444\n",
      "Epoch 20/29\n",
      "----------\n",
      "********************train*************\n",
      "Epoch [21/30] Loss: 0.1497   Acc: 96.2500\n",
      "********************val*************\n",
      "Epoch [21/30] Loss: 0.2087   Acc: 94.4444\n",
      "Epoch 21/29\n",
      "----------\n",
      "********************train*************\n",
      "Epoch [22/30] Loss: 0.1476   Acc: 96.3810\n",
      "********************val*************\n",
      "Epoch [22/30] Loss: 0.2063   Acc: 94.4722\n",
      "Epoch 22/29\n",
      "----------\n",
      "********************train*************\n",
      "Epoch [23/30] Loss: 0.1456   Acc: 96.3571\n",
      "********************val*************\n",
      "Epoch [23/30] Loss: 0.2043   Acc: 94.5556\n",
      "Epoch 23/29\n",
      "----------\n",
      "********************train*************\n",
      "Epoch [24/30] Loss: 0.1436   Acc: 96.4286\n",
      "********************val*************\n",
      "Epoch [24/30] Loss: 0.2026   Acc: 94.6111\n",
      "Epoch 24/29\n",
      "----------\n",
      "********************train*************\n",
      "Epoch [25/30] Loss: 0.1417   Acc: 96.5714\n",
      "********************val*************\n",
      "Epoch [25/30] Loss: 0.2011   Acc: 94.6111\n",
      "Epoch 25/29\n",
      "----------\n",
      "********************train*************\n",
      "Epoch [26/30] Loss: 0.1398   Acc: 96.5833\n",
      "********************val*************\n",
      "Epoch [26/30] Loss: 0.1997   Acc: 94.7500\n",
      "Epoch 26/29\n",
      "----------\n",
      "********************train*************\n",
      "Epoch [27/30] Loss: 0.1380   Acc: 96.7143\n",
      "********************val*************\n",
      "Epoch [27/30] Loss: 0.1980   Acc: 94.8056\n",
      "Epoch 27/29\n",
      "----------\n",
      "********************train*************\n",
      "Epoch [28/30] Loss: 0.1362   Acc: 96.7381\n",
      "********************val*************\n",
      "Epoch [28/30] Loss: 0.1961   Acc: 94.8056\n",
      "Epoch 28/29\n",
      "----------\n",
      "********************train*************\n",
      "Epoch [29/30] Loss: 0.1343   Acc: 96.7857\n",
      "********************val*************\n",
      "Epoch [29/30] Loss: 0.1941   Acc: 94.8889\n",
      "Epoch 29/29\n",
      "----------\n",
      "********************train*************\n",
      "Epoch [30/30] Loss: 0.1325   Acc: 96.8690\n",
      "********************val*************\n",
      "Epoch [30/30] Loss: 0.1920   Acc: 94.9167\n",
      "Training complete in 0m 3s\n",
      "Best val Acc: 94.916667\n",
      "Epoch generations ( 51 /100) :accuracy(fittest mask) tensor(96.2381, dtype=torch.float64) keep_prob 0.6722222222222223\n",
      "Epoch generations ( 52 /100) :accuracy(fittest mask) tensor(96.2381, dtype=torch.float64) keep_prob 0.6611111111111111\n",
      "Epoch generations ( 53 /100) :accuracy(fittest mask) tensor(96.4405, dtype=torch.float64) keep_prob 0.6666666666666666\n",
      "Epoch generations ( 54 /100) :accuracy(fittest mask) tensor(96.4048, dtype=torch.float64) keep_prob 0.6833333333333333\n",
      "Epoch generations ( 55 /100) :accuracy(fittest mask) tensor(96.3214, dtype=torch.float64) keep_prob 0.6888888888888889\n",
      "Epoch generations ( 56 /100) :accuracy(fittest mask) tensor(96.2857, dtype=torch.float64) keep_prob 0.6777777777777778\n",
      "Epoch generations ( 57 /100) :accuracy(fittest mask) tensor(96.2143, dtype=torch.float64) keep_prob 0.7055555555555556\n",
      "Epoch generations ( 58 /100) :accuracy(fittest mask) tensor(96.3333, dtype=torch.float64) keep_prob 0.7\n",
      "Epoch generations ( 59 /100) :accuracy(fittest mask) tensor(96.2738, dtype=torch.float64) keep_prob 0.6722222222222223\n",
      "Epoch generations ( 60 /100) :accuracy(fittest mask) tensor(95.9762, dtype=torch.float64) keep_prob 0.7111111111111111\n",
      "Epoch 0/29\n",
      "----------\n",
      "********************train*************\n",
      "Epoch [1/30] Loss: 0.1544   Acc: 95.9762\n",
      "********************val*************\n",
      "Epoch [1/30] Loss: 0.1919   Acc: 94.7222\n",
      "Epoch 1/29\n",
      "----------\n",
      "********************train*************\n",
      "Epoch [2/30] Loss: 0.1491   Acc: 96.2619\n",
      "********************val*************\n",
      "Epoch [2/30] Loss: 0.1944   Acc: 94.6111\n",
      "Epoch 2/29\n",
      "----------\n",
      "********************train*************\n",
      "Epoch [3/30] Loss: 0.1435   Acc: 96.3452\n",
      "********************val*************\n",
      "Epoch [3/30] Loss: 0.1996   Acc: 94.3889\n",
      "Epoch 3/29\n",
      "----------\n",
      "********************train*************\n",
      "Epoch [4/30] Loss: 0.1402   Acc: 96.5952\n",
      "********************val*************\n",
      "Epoch [4/30] Loss: 0.2039   Acc: 94.1667\n",
      "Epoch 4/29\n",
      "----------\n",
      "********************train*************\n",
      "Epoch [5/30] Loss: 0.1385   Acc: 96.6548\n",
      "********************val*************\n",
      "Epoch [5/30] Loss: 0.2037   Acc: 94.2778\n",
      "Epoch 5/29\n",
      "----------\n",
      "********************train*************\n",
      "Epoch [6/30] Loss: 0.1359   Acc: 96.7976\n",
      "********************val*************\n",
      "Epoch [6/30] Loss: 0.1996   Acc: 94.5556\n",
      "Epoch 6/29\n",
      "----------\n",
      "********************train*************\n",
      "Epoch [7/30] Loss: 0.1326   Acc: 96.8452\n",
      "********************val*************\n",
      "Epoch [7/30] Loss: 0.1947   Acc: 94.8889\n",
      "Epoch 7/29\n",
      "----------\n",
      "********************train*************\n",
      "Epoch [8/30] Loss: 0.1302   Acc: 96.9167\n",
      "********************val*************\n",
      "Epoch [8/30] Loss: 0.1913   Acc: 95.1111\n",
      "Epoch 8/29\n",
      "----------\n",
      "********************train*************\n",
      "Epoch [9/30] Loss: 0.1289   Acc: 96.9048\n",
      "********************val*************\n",
      "Epoch [9/30] Loss: 0.1895   Acc: 95.1389\n",
      "Epoch 9/29\n",
      "----------\n",
      "********************train*************\n",
      "Epoch [10/30] Loss: 0.1277   Acc: 97.0000\n",
      "********************val*************\n",
      "Epoch [10/30] Loss: 0.1890   Acc: 95.1667\n",
      "Epoch 10/29\n",
      "----------\n",
      "********************train*************\n",
      "Epoch [11/30] Loss: 0.1259   Acc: 97.0952\n",
      "********************val*************\n",
      "Epoch [11/30] Loss: 0.1895   Acc: 95.1667\n",
      "Epoch 11/29\n",
      "----------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "********************train*************\n",
      "Epoch [12/30] Loss: 0.1237   Acc: 97.2024\n",
      "********************val*************\n",
      "Epoch [12/30] Loss: 0.1910   Acc: 95.0278\n",
      "Epoch 12/29\n",
      "----------\n",
      "********************train*************\n",
      "Epoch [13/30] Loss: 0.1219   Acc: 97.2143\n",
      "********************val*************\n",
      "Epoch [13/30] Loss: 0.1925   Acc: 94.9444\n",
      "Epoch 13/29\n",
      "----------\n",
      "********************train*************\n",
      "Epoch [14/30] Loss: 0.1205   Acc: 97.2619\n",
      "********************val*************\n",
      "Epoch [14/30] Loss: 0.1929   Acc: 94.9167\n",
      "Epoch 14/29\n",
      "----------\n",
      "********************train*************\n",
      "Epoch [15/30] Loss: 0.1192   Acc: 97.3452\n",
      "********************val*************\n",
      "Epoch [15/30] Loss: 0.1911   Acc: 94.9167\n",
      "Epoch 15/29\n",
      "----------\n",
      "********************train*************\n",
      "Epoch [16/30] Loss: 0.1175   Acc: 97.3690\n",
      "********************val*************\n",
      "Epoch [16/30] Loss: 0.1874   Acc: 94.9444\n",
      "Epoch 16/29\n",
      "----------\n",
      "********************train*************\n",
      "Epoch [17/30] Loss: 0.1156   Acc: 97.4167\n",
      "********************val*************\n",
      "Epoch [17/30] Loss: 0.1829   Acc: 95.2778\n",
      "Epoch 17/29\n",
      "----------\n",
      "********************train*************\n",
      "Epoch [18/30] Loss: 0.1139   Acc: 97.3810\n",
      "********************val*************\n",
      "Epoch [18/30] Loss: 0.1788   Acc: 95.3333\n",
      "Epoch 18/29\n",
      "----------\n",
      "********************train*************\n",
      "Epoch [19/30] Loss: 0.1124   Acc: 97.3452\n",
      "********************val*************\n",
      "Epoch [19/30] Loss: 0.1755   Acc: 95.4722\n",
      "Epoch 19/29\n",
      "----------\n",
      "********************train*************\n",
      "Epoch [20/30] Loss: 0.1110   Acc: 97.3929\n",
      "********************val*************\n",
      "Epoch [20/30] Loss: 0.1734   Acc: 95.6389\n",
      "Epoch 20/29\n",
      "----------\n",
      "********************train*************\n",
      "Epoch [21/30] Loss: 0.1096   Acc: 97.3095\n",
      "********************val*************\n",
      "Epoch [21/30] Loss: 0.1723   Acc: 95.6389\n",
      "Epoch 21/29\n",
      "----------\n",
      "********************train*************\n",
      "Epoch [22/30] Loss: 0.1082   Acc: 97.3095\n",
      "********************val*************\n",
      "Epoch [22/30] Loss: 0.1720   Acc: 95.6944\n",
      "Epoch 22/29\n",
      "----------\n",
      "********************train*************\n",
      "Epoch [23/30] Loss: 0.1069   Acc: 97.3929\n",
      "********************val*************\n",
      "Epoch [23/30] Loss: 0.1719   Acc: 95.7500\n",
      "Epoch 23/29\n",
      "----------\n",
      "********************train*************\n",
      "Epoch [24/30] Loss: 0.1057   Acc: 97.4405\n",
      "********************val*************\n",
      "Epoch [24/30] Loss: 0.1716   Acc: 95.6667\n",
      "Epoch 24/29\n",
      "----------\n",
      "********************train*************\n",
      "Epoch [25/30] Loss: 0.1044   Acc: 97.5119\n",
      "********************val*************\n",
      "Epoch [25/30] Loss: 0.1706   Acc: 95.5833\n",
      "Epoch 25/29\n",
      "----------\n",
      "********************train*************\n",
      "Epoch [26/30] Loss: 0.1030   Acc: 97.5714\n",
      "********************val*************\n",
      "Epoch [26/30] Loss: 0.1691   Acc: 95.6389\n",
      "Epoch 26/29\n",
      "----------\n",
      "********************train*************\n",
      "Epoch [27/30] Loss: 0.1017   Acc: 97.6548\n",
      "********************val*************\n",
      "Epoch [27/30] Loss: 0.1673   Acc: 95.6667\n",
      "Epoch 27/29\n",
      "----------\n",
      "********************train*************\n",
      "Epoch [28/30] Loss: 0.1005   Acc: 97.6429\n",
      "********************val*************\n",
      "Epoch [28/30] Loss: 0.1655   Acc: 95.7222\n",
      "Epoch 28/29\n",
      "----------\n",
      "********************train*************\n",
      "Epoch [29/30] Loss: 0.0993   Acc: 97.6667\n",
      "********************val*************\n",
      "Epoch [29/30] Loss: 0.1641   Acc: 95.8333\n",
      "Epoch 29/29\n",
      "----------\n",
      "********************train*************\n",
      "Epoch [30/30] Loss: 0.0981   Acc: 97.7619\n",
      "********************val*************\n",
      "Epoch [30/30] Loss: 0.1631   Acc: 95.8611\n",
      "Training complete in 0m 2s\n",
      "Best val Acc: 95.861111\n",
      "Epoch generations ( 61 /100) :accuracy(fittest mask) tensor(97.1667, dtype=torch.float64) keep_prob 0.65\n",
      "Epoch generations ( 62 /100) :accuracy(fittest mask) tensor(97.1071, dtype=torch.float64) keep_prob 0.6555555555555556\n",
      "Epoch generations ( 63 /100) :accuracy(fittest mask) tensor(97.1071, dtype=torch.float64) keep_prob 0.6555555555555556\n",
      "Epoch generations ( 64 /100) :accuracy(fittest mask) tensor(97.2857, dtype=torch.float64) keep_prob 0.6722222222222223\n",
      "Epoch generations ( 65 /100) :accuracy(fittest mask) tensor(97.2738, dtype=torch.float64) keep_prob 0.6833333333333333\n",
      "Epoch generations ( 66 /100) :accuracy(fittest mask) tensor(97.3214, dtype=torch.float64) keep_prob 0.6777777777777778\n",
      "Epoch generations ( 67 /100) :accuracy(fittest mask) tensor(97.3214, dtype=torch.float64) keep_prob 0.6777777777777778\n",
      "Epoch generations ( 68 /100) :accuracy(fittest mask) tensor(97.1429, dtype=torch.float64) keep_prob 0.7277777777777777\n",
      "Epoch generations ( 69 /100) :accuracy(fittest mask) tensor(97.1429, dtype=torch.float64) keep_prob 0.7277777777777777\n",
      "Epoch generations ( 70 /100) :accuracy(fittest mask) tensor(97.1190, dtype=torch.float64) keep_prob 0.7111111111111111\n",
      "Epoch 0/29\n",
      "----------\n",
      "********************train*************\n",
      "Epoch [1/30] Loss: 0.1111   Acc: 97.1190\n",
      "********************val*************\n",
      "Epoch [1/30] Loss: 0.1586   Acc: 95.9722\n",
      "Epoch 1/29\n",
      "----------\n",
      "********************train*************\n",
      "Epoch [2/30] Loss: 0.1068   Acc: 97.2262\n",
      "********************val*************\n",
      "Epoch [2/30] Loss: 0.1543   Acc: 96.0000\n",
      "Epoch 2/29\n",
      "----------\n",
      "********************train*************\n",
      "Epoch [3/30] Loss: 0.1030   Acc: 97.3571\n",
      "********************val*************\n",
      "Epoch [3/30] Loss: 0.1518   Acc: 96.1111\n",
      "Epoch 3/29\n",
      "----------\n",
      "********************train*************\n",
      "Epoch [4/30] Loss: 0.1013   Acc: 97.5238\n",
      "********************val*************\n",
      "Epoch [4/30] Loss: 0.1505   Acc: 96.2778\n",
      "Epoch 4/29\n",
      "----------\n",
      "********************train*************\n",
      "Epoch [5/30] Loss: 0.1003   Acc: 97.4286\n",
      "********************val*************\n",
      "Epoch [5/30] Loss: 0.1497   Acc: 96.1667\n",
      "Epoch 5/29\n",
      "----------\n",
      "********************train*************\n",
      "Epoch [6/30] Loss: 0.0987   Acc: 97.5000\n",
      "********************val*************\n",
      "Epoch [6/30] Loss: 0.1490   Acc: 96.2222\n",
      "Epoch 6/29\n",
      "----------\n",
      "********************train*************\n",
      "Epoch [7/30] Loss: 0.0964   Acc: 97.6071\n",
      "********************val*************\n",
      "Epoch [7/30] Loss: 0.1490   Acc: 96.1389\n",
      "Epoch 7/29\n",
      "----------\n",
      "********************train*************\n",
      "Epoch [8/30] Loss: 0.0944   Acc: 97.5952\n",
      "********************val*************\n",
      "Epoch [8/30] Loss: 0.1497   Acc: 96.1389\n",
      "Epoch 8/29\n",
      "----------\n",
      "********************train*************\n",
      "Epoch [9/30] Loss: 0.0934   Acc: 97.6786\n",
      "********************val*************\n",
      "Epoch [9/30] Loss: 0.1504   Acc: 96.0278\n",
      "Epoch 9/29\n",
      "----------\n",
      "********************train*************\n",
      "Epoch [10/30] Loss: 0.0930   Acc: 97.8333\n",
      "********************val*************\n",
      "Epoch [10/30] Loss: 0.1501   Acc: 96.0556\n",
      "Epoch 10/29\n",
      "----------\n",
      "********************train*************\n",
      "Epoch [11/30] Loss: 0.0923   Acc: 97.8690\n",
      "********************val*************\n",
      "Epoch [11/30] Loss: 0.1484   Acc: 96.1944\n",
      "Epoch 11/29\n",
      "----------\n",
      "********************train*************\n",
      "Epoch [12/30] Loss: 0.0908   Acc: 97.9643\n",
      "********************val*************\n",
      "Epoch [12/30] Loss: 0.1460   Acc: 96.1944\n",
      "Epoch 12/29\n",
      "----------\n",
      "********************train*************\n",
      "Epoch [13/30] Loss: 0.0892   Acc: 97.9881\n",
      "********************val*************\n",
      "Epoch [13/30] Loss: 0.1438   Acc: 96.4444\n",
      "Epoch 13/29\n",
      "----------\n",
      "********************train*************\n",
      "Epoch [14/30] Loss: 0.0880   Acc: 98.0238\n",
      "********************val*************\n",
      "Epoch [14/30] Loss: 0.1421   Acc: 96.4444\n",
      "Epoch 14/29\n",
      "----------\n",
      "********************train*************\n",
      "Epoch [15/30] Loss: 0.0873   Acc: 98.0357\n",
      "********************val*************\n",
      "Epoch [15/30] Loss: 0.1409   Acc: 96.5278\n",
      "Epoch 15/29\n",
      "----------\n",
      "********************train*************\n",
      "Epoch [16/30] Loss: 0.0865   Acc: 98.0833\n",
      "********************val*************\n",
      "Epoch [16/30] Loss: 0.1399   Acc: 96.5556\n",
      "Epoch 16/29\n",
      "----------\n",
      "********************train*************\n",
      "Epoch [17/30] Loss: 0.0855   Acc: 98.0952\n",
      "********************val*************\n",
      "Epoch [17/30] Loss: 0.1394   Acc: 96.5833\n",
      "Epoch 17/29\n",
      "----------\n",
      "********************train*************\n",
      "Epoch [18/30] Loss: 0.0843   Acc: 98.0476\n",
      "********************val*************\n",
      "Epoch [18/30] Loss: 0.1393   Acc: 96.5556\n",
      "Epoch 18/29\n",
      "----------\n",
      "********************train*************\n",
      "Epoch [19/30] Loss: 0.0832   Acc: 98.0119\n",
      "********************val*************\n",
      "Epoch [19/30] Loss: 0.1395   Acc: 96.5000\n",
      "Epoch 19/29\n",
      "----------\n",
      "********************train*************\n",
      "Epoch [20/30] Loss: 0.0823   Acc: 98.0833\n",
      "********************val*************\n",
      "Epoch [20/30] Loss: 0.1396   Acc: 96.4722\n",
      "Epoch 20/29\n",
      "----------\n",
      "********************train*************\n",
      "Epoch [21/30] Loss: 0.0816   Acc: 98.1786\n",
      "********************val*************\n",
      "Epoch [21/30] Loss: 0.1391   Acc: 96.4444\n",
      "Epoch 21/29\n",
      "----------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "********************train*************\n",
      "Epoch [22/30] Loss: 0.0808   Acc: 98.2381\n",
      "********************val*************\n",
      "Epoch [22/30] Loss: 0.1382   Acc: 96.4722\n",
      "Epoch 22/29\n",
      "----------\n",
      "********************train*************\n",
      "Epoch [23/30] Loss: 0.0799   Acc: 98.2500\n",
      "********************val*************\n",
      "Epoch [23/30] Loss: 0.1368   Acc: 96.5278\n",
      "Epoch 23/29\n",
      "----------\n",
      "********************train*************\n",
      "Epoch [24/30] Loss: 0.0789   Acc: 98.3095\n",
      "********************val*************\n",
      "Epoch [24/30] Loss: 0.1352   Acc: 96.6944\n",
      "Epoch 24/29\n",
      "----------\n",
      "********************train*************\n",
      "Epoch [25/30] Loss: 0.0781   Acc: 98.2976\n",
      "********************val*************\n",
      "Epoch [25/30] Loss: 0.1338   Acc: 96.7222\n",
      "Epoch 25/29\n",
      "----------\n",
      "********************train*************\n",
      "Epoch [26/30] Loss: 0.0774   Acc: 98.4286\n",
      "********************val*************\n",
      "Epoch [26/30] Loss: 0.1326   Acc: 96.7500\n",
      "Epoch 26/29\n",
      "----------\n",
      "********************train*************\n",
      "Epoch [27/30] Loss: 0.0766   Acc: 98.4762\n",
      "********************val*************\n",
      "Epoch [27/30] Loss: 0.1316   Acc: 96.7500\n",
      "Epoch 27/29\n",
      "----------\n",
      "********************train*************\n",
      "Epoch [28/30] Loss: 0.0758   Acc: 98.4405\n",
      "********************val*************\n",
      "Epoch [28/30] Loss: 0.1308   Acc: 96.7500\n",
      "Epoch 28/29\n",
      "----------\n",
      "********************train*************\n",
      "Epoch [29/30] Loss: 0.0751   Acc: 98.4167\n",
      "********************val*************\n",
      "Epoch [29/30] Loss: 0.1302   Acc: 96.7222\n",
      "Epoch 29/29\n",
      "----------\n",
      "********************train*************\n",
      "Epoch [30/30] Loss: 0.0743   Acc: 98.4643\n",
      "********************val*************\n",
      "Epoch [30/30] Loss: 0.1297   Acc: 96.7778\n",
      "Training complete in 0m 3s\n",
      "Best val Acc: 96.777778\n",
      "Epoch generations ( 71 /100) :accuracy(fittest mask) tensor(98.0119, dtype=torch.float64) keep_prob 0.7222222222222222\n",
      "Epoch generations ( 72 /100) :accuracy(fittest mask) tensor(98.0119, dtype=torch.float64) keep_prob 0.7444444444444445\n",
      "Epoch generations ( 73 /100) :accuracy(fittest mask) tensor(98.0476, dtype=torch.float64) keep_prob 0.7666666666666667\n",
      "Epoch generations ( 74 /100) :accuracy(fittest mask) tensor(98.0357, dtype=torch.float64) keep_prob 0.7555555555555555\n",
      "Epoch generations ( 75 /100) :accuracy(fittest mask) tensor(97.9405, dtype=torch.float64) keep_prob 0.7777777777777778\n",
      "Epoch generations ( 76 /100) :accuracy(fittest mask) tensor(98.0119, dtype=torch.float64) keep_prob 0.7555555555555555\n",
      "Epoch generations ( 77 /100) :accuracy(fittest mask) tensor(98.2738, dtype=torch.float64) keep_prob 0.7333333333333333\n",
      "Epoch generations ( 78 /100) :accuracy(fittest mask) tensor(98.2500, dtype=torch.float64) keep_prob 0.7333333333333333\n",
      "Epoch generations ( 79 /100) :accuracy(fittest mask) tensor(98.1071, dtype=torch.float64) keep_prob 0.7388888888888889\n",
      "Epoch generations ( 80 /100) :accuracy(fittest mask) tensor(97.9643, dtype=torch.float64) keep_prob 0.7388888888888889\n",
      "Epoch 0/29\n",
      "----------\n",
      "********************train*************\n",
      "Epoch [1/30] Loss: 0.0899   Acc: 97.9643\n",
      "********************val*************\n",
      "Epoch [1/30] Loss: 0.1292   Acc: 96.7500\n",
      "Epoch 1/29\n",
      "----------\n",
      "********************train*************\n",
      "Epoch [2/30] Loss: 0.0875   Acc: 98.0238\n",
      "********************val*************\n",
      "Epoch [2/30] Loss: 0.1288   Acc: 96.7222\n",
      "Epoch 2/29\n",
      "----------\n",
      "********************train*************\n",
      "Epoch [3/30] Loss: 0.0846   Acc: 98.1548\n",
      "********************val*************\n",
      "Epoch [3/30] Loss: 0.1287   Acc: 96.7222\n",
      "Epoch 3/29\n",
      "----------\n",
      "********************train*************\n",
      "Epoch [4/30] Loss: 0.0819   Acc: 98.2262\n",
      "********************val*************\n",
      "Epoch [4/30] Loss: 0.1288   Acc: 96.8611\n",
      "Epoch 4/29\n",
      "----------\n",
      "********************train*************\n",
      "Epoch [5/30] Loss: 0.0798   Acc: 98.2738\n",
      "********************val*************\n",
      "Epoch [5/30] Loss: 0.1292   Acc: 96.8056\n",
      "Epoch 5/29\n",
      "----------\n",
      "********************train*************\n",
      "Epoch [6/30] Loss: 0.0783   Acc: 98.3095\n",
      "********************val*************\n",
      "Epoch [6/30] Loss: 0.1297   Acc: 96.8611\n",
      "Epoch 6/29\n",
      "----------\n",
      "********************train*************\n",
      "Epoch [7/30] Loss: 0.0772   Acc: 98.3214\n",
      "********************val*************\n",
      "Epoch [7/30] Loss: 0.1300   Acc: 96.7500\n",
      "Epoch 7/29\n",
      "----------\n",
      "********************train*************\n",
      "Epoch [8/30] Loss: 0.0764   Acc: 98.3929\n",
      "********************val*************\n",
      "Epoch [8/30] Loss: 0.1302   Acc: 96.7222\n",
      "Epoch 8/29\n",
      "----------\n",
      "********************train*************\n",
      "Epoch [9/30] Loss: 0.0756   Acc: 98.4167\n",
      "********************val*************\n",
      "Epoch [9/30] Loss: 0.1302   Acc: 96.7500\n",
      "Epoch 9/29\n",
      "----------\n",
      "********************train*************\n",
      "Epoch [10/30] Loss: 0.0747   Acc: 98.4048\n",
      "********************val*************\n",
      "Epoch [10/30] Loss: 0.1299   Acc: 96.7222\n",
      "Epoch 10/29\n",
      "----------\n",
      "********************train*************\n",
      "Epoch [11/30] Loss: 0.0737   Acc: 98.4881\n",
      "********************val*************\n",
      "Epoch [11/30] Loss: 0.1294   Acc: 96.6944\n",
      "Epoch 11/29\n",
      "----------\n",
      "********************train*************\n",
      "Epoch [12/30] Loss: 0.0729   Acc: 98.4762\n",
      "********************val*************\n",
      "Epoch [12/30] Loss: 0.1287   Acc: 96.8056\n",
      "Epoch 12/29\n",
      "----------\n",
      "********************train*************\n",
      "Epoch [13/30] Loss: 0.0720   Acc: 98.5119\n",
      "********************val*************\n",
      "Epoch [13/30] Loss: 0.1277   Acc: 96.7222\n",
      "Epoch 13/29\n",
      "----------\n",
      "********************train*************\n",
      "Epoch [14/30] Loss: 0.0712   Acc: 98.5833\n",
      "********************val*************\n",
      "Epoch [14/30] Loss: 0.1266   Acc: 96.8333\n",
      "Epoch 14/29\n",
      "----------\n",
      "********************train*************\n",
      "Epoch [15/30] Loss: 0.0704   Acc: 98.6429\n",
      "********************val*************\n",
      "Epoch [15/30] Loss: 0.1254   Acc: 96.8333\n",
      "Epoch 15/29\n",
      "----------\n",
      "********************train*************\n",
      "Epoch [16/30] Loss: 0.0696   Acc: 98.6548\n",
      "********************val*************\n",
      "Epoch [16/30] Loss: 0.1243   Acc: 96.8611\n",
      "Epoch 16/29\n",
      "----------\n",
      "********************train*************\n",
      "Epoch [17/30] Loss: 0.0688   Acc: 98.7619\n",
      "********************val*************\n",
      "Epoch [17/30] Loss: 0.1235   Acc: 96.8333\n",
      "Epoch 17/29\n",
      "----------\n",
      "********************train*************\n",
      "Epoch [18/30] Loss: 0.0680   Acc: 98.7619\n",
      "********************val*************\n",
      "Epoch [18/30] Loss: 0.1229   Acc: 96.8889\n",
      "Epoch 18/29\n",
      "----------\n",
      "********************train*************\n",
      "Epoch [19/30] Loss: 0.0673   Acc: 98.7143\n",
      "********************val*************\n",
      "Epoch [19/30] Loss: 0.1225   Acc: 96.8889\n",
      "Epoch 19/29\n",
      "----------\n",
      "********************train*************\n",
      "Epoch [20/30] Loss: 0.0666   Acc: 98.7143\n",
      "********************val*************\n",
      "Epoch [20/30] Loss: 0.1222   Acc: 96.9444\n",
      "Epoch 20/29\n",
      "----------\n",
      "********************train*************\n",
      "Epoch [21/30] Loss: 0.0659   Acc: 98.7381\n",
      "********************val*************\n",
      "Epoch [21/30] Loss: 0.1219   Acc: 96.9722\n",
      "Epoch 21/29\n",
      "----------\n",
      "********************train*************\n",
      "Epoch [22/30] Loss: 0.0652   Acc: 98.7857\n",
      "********************val*************\n",
      "Epoch [22/30] Loss: 0.1215   Acc: 96.8889\n",
      "Epoch 22/29\n",
      "----------\n",
      "********************train*************\n",
      "Epoch [23/30] Loss: 0.0646   Acc: 98.7857\n",
      "********************val*************\n",
      "Epoch [23/30] Loss: 0.1209   Acc: 96.9167\n",
      "Epoch 23/29\n",
      "----------\n",
      "********************train*************\n",
      "Epoch [24/30] Loss: 0.0639   Acc: 98.8214\n",
      "********************val*************\n",
      "Epoch [24/30] Loss: 0.1201   Acc: 96.9167\n",
      "Epoch 24/29\n",
      "----------\n",
      "********************train*************\n",
      "Epoch [25/30] Loss: 0.0632   Acc: 98.8810\n",
      "********************val*************\n",
      "Epoch [25/30] Loss: 0.1192   Acc: 96.8889\n",
      "Epoch 25/29\n",
      "----------\n",
      "********************train*************\n",
      "Epoch [26/30] Loss: 0.0625   Acc: 98.9048\n",
      "********************val*************\n",
      "Epoch [26/30] Loss: 0.1182   Acc: 97.0000\n",
      "Epoch 26/29\n",
      "----------\n",
      "********************train*************\n",
      "Epoch [27/30] Loss: 0.0618   Acc: 98.8810\n",
      "********************val*************\n",
      "Epoch [27/30] Loss: 0.1173   Acc: 97.0278\n",
      "Epoch 27/29\n",
      "----------\n",
      "********************train*************\n",
      "Epoch [28/30] Loss: 0.0612   Acc: 98.9286\n",
      "********************val*************\n",
      "Epoch [28/30] Loss: 0.1165   Acc: 97.1111\n",
      "Epoch 28/29\n",
      "----------\n",
      "********************train*************\n",
      "Epoch [29/30] Loss: 0.0606   Acc: 98.9524\n",
      "********************val*************\n",
      "Epoch [29/30] Loss: 0.1158   Acc: 97.0833\n",
      "Epoch 29/29\n",
      "----------\n",
      "********************train*************\n",
      "Epoch [30/30] Loss: 0.0601   Acc: 98.9762\n",
      "********************val*************\n",
      "Epoch [30/30] Loss: 0.1152   Acc: 97.1111\n",
      "Training complete in 0m 2s\n",
      "Best val Acc: 97.111111\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch generations ( 81 /100) :accuracy(fittest mask) tensor(98.9405, dtype=torch.float64) keep_prob 0.7611111111111111\n",
      "Epoch generations ( 82 /100) :accuracy(fittest mask) tensor(98.6190, dtype=torch.float64) keep_prob 0.7888888888888889\n",
      "Epoch generations ( 83 /100) :accuracy(fittest mask) tensor(98.4881, dtype=torch.float64) keep_prob 0.7277777777777777\n",
      "Epoch generations ( 84 /100) :accuracy(fittest mask) tensor(98.4881, dtype=torch.float64) keep_prob 0.7333333333333333\n",
      "Epoch generations ( 85 /100) :accuracy(fittest mask) tensor(98.5595, dtype=torch.float64) keep_prob 0.7555555555555555\n",
      "Epoch generations ( 86 /100) :accuracy(fittest mask) tensor(98.5476, dtype=torch.float64) keep_prob 0.7388888888888889\n",
      "Epoch generations ( 87 /100) :accuracy(fittest mask) tensor(98.5595, dtype=torch.float64) keep_prob 0.7333333333333333\n",
      "Epoch generations ( 88 /100) :accuracy(fittest mask) tensor(98.5476, dtype=torch.float64) keep_prob 0.7388888888888889\n",
      "Epoch generations ( 89 /100) :accuracy(fittest mask) tensor(98.6190, dtype=torch.float64) keep_prob 0.75\n",
      "Epoch generations ( 90 /100) :accuracy(fittest mask) tensor(98.7500, dtype=torch.float64) keep_prob 0.7666666666666667\n",
      "Epoch 0/29\n",
      "----------\n",
      "********************train*************\n",
      "Epoch [1/30] Loss: 0.0715   Acc: 98.7500\n",
      "********************val*************\n",
      "Epoch [1/30] Loss: 0.1167   Acc: 97.0000\n",
      "Epoch 1/29\n",
      "----------\n",
      "********************train*************\n",
      "Epoch [2/30] Loss: 0.0689   Acc: 98.7738\n",
      "********************val*************\n",
      "Epoch [2/30] Loss: 0.1180   Acc: 96.9722\n",
      "Epoch 2/29\n",
      "----------\n",
      "********************train*************\n",
      "Epoch [3/30] Loss: 0.0667   Acc: 98.8333\n",
      "********************val*************\n",
      "Epoch [3/30] Loss: 0.1198   Acc: 96.7222\n",
      "Epoch 3/29\n",
      "----------\n",
      "********************train*************\n",
      "Epoch [4/30] Loss: 0.0657   Acc: 98.7976\n",
      "********************val*************\n",
      "Epoch [4/30] Loss: 0.1203   Acc: 96.6667\n",
      "Epoch 4/29\n",
      "----------\n",
      "********************train*************\n",
      "Epoch [5/30] Loss: 0.0648   Acc: 98.8452\n",
      "********************val*************\n",
      "Epoch [5/30] Loss: 0.1190   Acc: 96.7222\n",
      "Epoch 5/29\n",
      "----------\n",
      "********************train*************\n",
      "Epoch [6/30] Loss: 0.0635   Acc: 98.8214\n",
      "********************val*************\n",
      "Epoch [6/30] Loss: 0.1167   Acc: 96.8333\n",
      "Epoch 6/29\n",
      "----------\n",
      "********************train*************\n",
      "Epoch [7/30] Loss: 0.0623   Acc: 98.8571\n",
      "********************val*************\n",
      "Epoch [7/30] Loss: 0.1146   Acc: 96.8611\n",
      "Epoch 7/29\n",
      "----------\n",
      "********************train*************\n",
      "Epoch [8/30] Loss: 0.0616   Acc: 98.9048\n",
      "********************val*************\n",
      "Epoch [8/30] Loss: 0.1132   Acc: 96.8333\n",
      "Epoch 8/29\n",
      "----------\n",
      "********************train*************\n",
      "Epoch [9/30] Loss: 0.0612   Acc: 98.9405\n",
      "********************val*************\n",
      "Epoch [9/30] Loss: 0.1125   Acc: 97.0000\n",
      "Epoch 9/29\n",
      "----------\n",
      "********************train*************\n",
      "Epoch [10/30] Loss: 0.0605   Acc: 98.9643\n",
      "********************val*************\n",
      "Epoch [10/30] Loss: 0.1124   Acc: 96.9444\n",
      "Epoch 10/29\n",
      "----------\n",
      "********************train*************\n",
      "Epoch [11/30] Loss: 0.0595   Acc: 98.9524\n",
      "********************val*************\n",
      "Epoch [11/30] Loss: 0.1130   Acc: 96.8611\n",
      "Epoch 11/29\n",
      "----------\n",
      "********************train*************\n",
      "Epoch [12/30] Loss: 0.0587   Acc: 99.0357\n",
      "********************val*************\n",
      "Epoch [12/30] Loss: 0.1138   Acc: 96.8611\n",
      "Epoch 12/29\n",
      "----------\n",
      "********************train*************\n",
      "Epoch [13/30] Loss: 0.0582   Acc: 99.1548\n",
      "********************val*************\n",
      "Epoch [13/30] Loss: 0.1142   Acc: 96.8333\n",
      "Epoch 13/29\n",
      "----------\n",
      "********************train*************\n",
      "Epoch [14/30] Loss: 0.0579   Acc: 99.0833\n",
      "********************val*************\n",
      "Epoch [14/30] Loss: 0.1136   Acc: 96.8056\n",
      "Epoch 14/29\n",
      "----------\n",
      "********************train*************\n",
      "Epoch [15/30] Loss: 0.0574   Acc: 99.0833\n",
      "********************val*************\n",
      "Epoch [15/30] Loss: 0.1121   Acc: 96.8611\n",
      "Epoch 15/29\n",
      "----------\n",
      "********************train*************\n",
      "Epoch [16/30] Loss: 0.0568   Acc: 99.1429\n",
      "********************val*************\n",
      "Epoch [16/30] Loss: 0.1103   Acc: 96.8889\n",
      "Epoch 16/29\n",
      "----------\n",
      "********************train*************\n",
      "Epoch [17/30] Loss: 0.0562   Acc: 99.1310\n",
      "********************val*************\n",
      "Epoch [17/30] Loss: 0.1087   Acc: 97.0833\n",
      "Epoch 17/29\n",
      "----------\n",
      "********************train*************\n",
      "Epoch [18/30] Loss: 0.0558   Acc: 99.1310\n",
      "********************val*************\n",
      "Epoch [18/30] Loss: 0.1075   Acc: 97.0833\n",
      "Epoch 18/29\n",
      "----------\n",
      "********************train*************\n",
      "Epoch [19/30] Loss: 0.0553   Acc: 99.1429\n",
      "********************val*************\n",
      "Epoch [19/30] Loss: 0.1067   Acc: 97.1667\n",
      "Epoch 19/29\n",
      "----------\n",
      "********************train*************\n",
      "Epoch [20/30] Loss: 0.0547   Acc: 99.1429\n",
      "********************val*************\n",
      "Epoch [20/30] Loss: 0.1064   Acc: 97.2500\n",
      "Epoch 20/29\n",
      "----------\n",
      "********************train*************\n",
      "Epoch [21/30] Loss: 0.0540   Acc: 99.1667\n",
      "********************val*************\n",
      "Epoch [21/30] Loss: 0.1064   Acc: 97.1944\n",
      "Epoch 21/29\n",
      "----------\n",
      "********************train*************\n",
      "Epoch [22/30] Loss: 0.0534   Acc: 99.1786\n",
      "********************val*************\n",
      "Epoch [22/30] Loss: 0.1064   Acc: 97.1667\n",
      "Epoch 22/29\n",
      "----------\n",
      "********************train*************\n",
      "Epoch [23/30] Loss: 0.0529   Acc: 99.2262\n",
      "********************val*************\n",
      "Epoch [23/30] Loss: 0.1060   Acc: 97.1389\n",
      "Epoch 23/29\n",
      "----------\n",
      "********************train*************\n",
      "Epoch [24/30] Loss: 0.0525   Acc: 99.2381\n",
      "********************val*************\n",
      "Epoch [24/30] Loss: 0.1052   Acc: 97.1944\n",
      "Epoch 24/29\n",
      "----------\n",
      "********************train*************\n",
      "Epoch [25/30] Loss: 0.0519   Acc: 99.2381\n",
      "********************val*************\n",
      "Epoch [25/30] Loss: 0.1040   Acc: 97.1944\n",
      "Epoch 25/29\n",
      "----------\n",
      "********************train*************\n",
      "Epoch [26/30] Loss: 0.0514   Acc: 99.2262\n",
      "********************val*************\n",
      "Epoch [26/30] Loss: 0.1029   Acc: 97.2500\n",
      "Epoch 26/29\n",
      "----------\n",
      "********************train*************\n",
      "Epoch [27/30] Loss: 0.0509   Acc: 99.2262\n",
      "********************val*************\n",
      "Epoch [27/30] Loss: 0.1020   Acc: 97.2222\n",
      "Epoch 27/29\n",
      "----------\n",
      "********************train*************\n",
      "Epoch [28/30] Loss: 0.0505   Acc: 99.1905\n",
      "********************val*************\n",
      "Epoch [28/30] Loss: 0.1015   Acc: 97.2222\n",
      "Epoch 28/29\n",
      "----------\n",
      "********************train*************\n",
      "Epoch [29/30] Loss: 0.0500   Acc: 99.1786\n",
      "********************val*************\n",
      "Epoch [29/30] Loss: 0.1013   Acc: 97.2778\n",
      "Epoch 29/29\n",
      "----------\n",
      "********************train*************\n",
      "Epoch [30/30] Loss: 0.0495   Acc: 99.2143\n",
      "********************val*************\n",
      "Epoch [30/30] Loss: 0.1013   Acc: 97.3056\n",
      "Training complete in 0m 2s\n",
      "Best val Acc: 97.305556\n",
      "Epoch generations ( 91 /100) :accuracy(fittest mask) tensor(99.1905, dtype=torch.float64) keep_prob 0.7611111111111111\n",
      "Epoch generations ( 92 /100) :accuracy(fittest mask) tensor(98.9881, dtype=torch.float64) keep_prob 0.7444444444444445\n",
      "Epoch generations ( 93 /100) :accuracy(fittest mask) tensor(99.0238, dtype=torch.float64) keep_prob 0.75\n",
      "Epoch generations ( 94 /100) :accuracy(fittest mask) tensor(99.0357, dtype=torch.float64) keep_prob 0.75\n",
      "Epoch generations ( 95 /100) :accuracy(fittest mask) tensor(99., dtype=torch.float64) keep_prob 0.7611111111111111\n",
      "Epoch generations ( 96 /100) :accuracy(fittest mask) tensor(98.9881, dtype=torch.float64) keep_prob 0.7555555555555555\n",
      "Epoch generations ( 97 /100) :accuracy(fittest mask) tensor(99., dtype=torch.float64) keep_prob 0.7555555555555555\n",
      "Epoch generations ( 98 /100) :accuracy(fittest mask) tensor(99.0833, dtype=torch.float64) keep_prob 0.7555555555555555\n",
      "Epoch generations ( 99 /100) :accuracy(fittest mask) tensor(99., dtype=torch.float64) keep_prob 0.7611111111111111\n",
      "Epoch generations ( 100 /100) :accuracy(fittest mask) tensor(99., dtype=torch.float64) keep_prob 0.7611111111111111\n",
      "Epoch 0/29\n",
      "----------\n",
      "********************train*************\n",
      "Epoch [1/30] Loss: 0.0542   Acc: 99.0000\n",
      "********************val*************\n",
      "Epoch [1/30] Loss: 0.1002   Acc: 97.3611\n",
      "Epoch 1/29\n",
      "----------\n",
      "********************train*************\n",
      "Epoch [2/30] Loss: 0.0532   Acc: 99.0119\n",
      "********************val*************\n",
      "Epoch [2/30] Loss: 0.0986   Acc: 97.2222\n",
      "Epoch 2/29\n",
      "----------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "********************train*************\n",
      "Epoch [3/30] Loss: 0.0519   Acc: 99.0476\n",
      "********************val*************\n",
      "Epoch [3/30] Loss: 0.0973   Acc: 97.3333\n",
      "Epoch 3/29\n",
      "----------\n",
      "********************train*************\n",
      "Epoch [4/30] Loss: 0.0507   Acc: 99.1190\n",
      "********************val*************\n",
      "Epoch [4/30] Loss: 0.0967   Acc: 97.4167\n",
      "Epoch 4/29\n",
      "----------\n",
      "********************train*************\n",
      "Epoch [5/30] Loss: 0.0499   Acc: 99.1310\n",
      "********************val*************\n",
      "Epoch [5/30] Loss: 0.0967   Acc: 97.3889\n",
      "Epoch 5/29\n",
      "----------\n",
      "********************train*************\n",
      "Epoch [6/30] Loss: 0.0493   Acc: 99.1786\n",
      "********************val*************\n",
      "Epoch [6/30] Loss: 0.0972   Acc: 97.4444\n",
      "Epoch 6/29\n",
      "----------\n",
      "********************train*************\n",
      "Epoch [7/30] Loss: 0.0488   Acc: 99.1786\n",
      "********************val*************\n",
      "Epoch [7/30] Loss: 0.0979   Acc: 97.3611\n",
      "Epoch 7/29\n",
      "----------\n",
      "********************train*************\n",
      "Epoch [8/30] Loss: 0.0483   Acc: 99.1786\n",
      "********************val*************\n",
      "Epoch [8/30] Loss: 0.0985   Acc: 97.3611\n",
      "Epoch 8/29\n",
      "----------\n",
      "********************train*************\n",
      "Epoch [9/30] Loss: 0.0479   Acc: 99.1905\n",
      "********************val*************\n",
      "Epoch [9/30] Loss: 0.0989   Acc: 97.3333\n",
      "Epoch 9/29\n",
      "----------\n",
      "********************train*************\n",
      "Epoch [10/30] Loss: 0.0475   Acc: 99.1786\n",
      "********************val*************\n",
      "Epoch [10/30] Loss: 0.0987   Acc: 97.3333\n",
      "Epoch 10/29\n",
      "----------\n",
      "********************train*************\n",
      "Epoch [11/30] Loss: 0.0471   Acc: 99.2024\n",
      "********************val*************\n",
      "Epoch [11/30] Loss: 0.0981   Acc: 97.3333\n",
      "Epoch 11/29\n",
      "----------\n",
      "********************train*************\n",
      "Epoch [12/30] Loss: 0.0467   Acc: 99.2024\n",
      "********************val*************\n",
      "Epoch [12/30] Loss: 0.0971   Acc: 97.3889\n",
      "Epoch 12/29\n",
      "----------\n",
      "********************train*************\n",
      "Epoch [13/30] Loss: 0.0462   Acc: 99.2024\n",
      "********************val*************\n",
      "Epoch [13/30] Loss: 0.0962   Acc: 97.3889\n",
      "Epoch 13/29\n",
      "----------\n",
      "********************train*************\n",
      "Epoch [14/30] Loss: 0.0458   Acc: 99.1786\n",
      "********************val*************\n",
      "Epoch [14/30] Loss: 0.0954   Acc: 97.4444\n",
      "Epoch 14/29\n",
      "----------\n",
      "********************train*************\n",
      "Epoch [15/30] Loss: 0.0454   Acc: 99.1786\n",
      "********************val*************\n",
      "Epoch [15/30] Loss: 0.0950   Acc: 97.4444\n",
      "Epoch 15/29\n",
      "----------\n",
      "********************train*************\n",
      "Epoch [16/30] Loss: 0.0450   Acc: 99.2381\n",
      "********************val*************\n",
      "Epoch [16/30] Loss: 0.0948   Acc: 97.4167\n",
      "Epoch 16/29\n",
      "----------\n",
      "********************train*************\n",
      "Epoch [17/30] Loss: 0.0445   Acc: 99.3333\n",
      "********************val*************\n",
      "Epoch [17/30] Loss: 0.0949   Acc: 97.4444\n",
      "Epoch 17/29\n",
      "----------\n",
      "********************train*************\n",
      "Epoch [18/30] Loss: 0.0441   Acc: 99.3333\n",
      "********************val*************\n",
      "Epoch [18/30] Loss: 0.0949   Acc: 97.4722\n",
      "Epoch 18/29\n",
      "----------\n",
      "********************train*************\n",
      "Epoch [19/30] Loss: 0.0437   Acc: 99.3571\n",
      "********************val*************\n",
      "Epoch [19/30] Loss: 0.0948   Acc: 97.5000\n",
      "Epoch 19/29\n",
      "----------\n",
      "********************train*************\n",
      "Epoch [20/30] Loss: 0.0434   Acc: 99.3810\n",
      "********************val*************\n",
      "Epoch [20/30] Loss: 0.0944   Acc: 97.5000\n",
      "Epoch 20/29\n",
      "----------\n",
      "********************train*************\n",
      "Epoch [21/30] Loss: 0.0430   Acc: 99.3690\n",
      "********************val*************\n",
      "Epoch [21/30] Loss: 0.0938   Acc: 97.5000\n",
      "Epoch 21/29\n",
      "----------\n",
      "********************train*************\n",
      "Epoch [22/30] Loss: 0.0427   Acc: 99.3690\n",
      "********************val*************\n",
      "Epoch [22/30] Loss: 0.0932   Acc: 97.5278\n",
      "Epoch 22/29\n",
      "----------\n",
      "********************train*************\n",
      "Epoch [23/30] Loss: 0.0424   Acc: 99.3810\n",
      "********************val*************\n",
      "Epoch [23/30] Loss: 0.0925   Acc: 97.5278\n",
      "Epoch 23/29\n",
      "----------\n",
      "********************train*************\n",
      "Epoch [24/30] Loss: 0.0420   Acc: 99.3810\n",
      "********************val*************\n",
      "Epoch [24/30] Loss: 0.0920   Acc: 97.5278\n",
      "Epoch 24/29\n",
      "----------\n",
      "********************train*************\n",
      "Epoch [25/30] Loss: 0.0417   Acc: 99.4048\n",
      "********************val*************\n",
      "Epoch [25/30] Loss: 0.0916   Acc: 97.5278\n",
      "Epoch 25/29\n",
      "----------\n",
      "********************train*************\n",
      "Epoch [26/30] Loss: 0.0413   Acc: 99.3571\n",
      "********************val*************\n",
      "Epoch [26/30] Loss: 0.0913   Acc: 97.5833\n",
      "Epoch 26/29\n",
      "----------\n",
      "********************train*************\n",
      "Epoch [27/30] Loss: 0.0410   Acc: 99.3690\n",
      "********************val*************\n",
      "Epoch [27/30] Loss: 0.0909   Acc: 97.5833\n",
      "Epoch 27/29\n",
      "----------\n",
      "********************train*************\n",
      "Epoch [28/30] Loss: 0.0407   Acc: 99.3571\n",
      "********************val*************\n",
      "Epoch [28/30] Loss: 0.0906   Acc: 97.5833\n",
      "Epoch 28/29\n",
      "----------\n",
      "********************train*************\n",
      "Epoch [29/30] Loss: 0.0404   Acc: 99.3690\n",
      "********************val*************\n",
      "Epoch [29/30] Loss: 0.0902   Acc: 97.5556\n",
      "Epoch 29/29\n",
      "----------\n",
      "********************train*************\n",
      "Epoch [30/30] Loss: 0.0401   Acc: 99.4048\n",
      "********************val*************\n",
      "Epoch [30/30] Loss: 0.0897   Acc: 97.5833\n",
      "Training complete in 0m 3s\n",
      "Best val Acc: 97.583333\n"
     ]
    }
   ],
   "source": [
    "#control block controls the epochs and the generations of mask\n",
    "#step 1 an object of the population class randomly generating the first population \n",
    "#step2 :calculate fitness of each entitiy of the population \n",
    "#step3: creates a mating pool of the population based on the worst two performing parent \n",
    "#step 4 :fittest mask of the generating along with keep_prob found \n",
    "#step 5: if 0th ,10th ,20th, the epochs starts training on the worst performing mask /other wise new generation is created \n",
    "\n",
    "epochgens=0\n",
    "population =Population(mutation_rate,max_population,maskLength)\n",
    "total_acc=[]\n",
    "p=0\n",
    "while (epochgens<=num_epochs):\n",
    "    print ('Epoch generations (',epochgens,'/{})'.format(num_epochs),end=' :')\n",
    "    if p < 0.8:\n",
    "      population .calcFitness(model)\n",
    "      population.naturalSelection()\n",
    "      fittestmask,p = population .fittest()\n",
    "      accuracy=fittestmask.fitness(model)\n",
    "      print (\"accuracy(fittest mask)\",accuracy,\"keep_prob\",p,end='\\n')\n",
    "    if (epochgens%10==0):\n",
    "        model,losses,accuracies=train_model(model,criterion,optimizer,fittestmask.gene,p,30)\n",
    "        total_acc=total_acc+accuracies\n",
    "    population.generate()\n",
    "    epochgens+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "dZvvW_lI5j67"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "310"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "0+(31*10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "name": "SimpleANN_ga.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
