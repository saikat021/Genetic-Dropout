{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import random\n",
    "import torch\n",
    "import torchvision\n",
    "import time \n",
    "from torchvision import datasets ,transforms\n",
    "from torch.utils.data import DataLoader\n",
    "from torch import nn \n",
    "from torch.nn.functional import relu ,softmax \n",
    "import copy "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_transform_train=transforms.Compose ([\n",
    "     transforms.ToTensor(),\n",
    "                                        \n",
    "                                          \n",
    "])\n",
    "data_transform_test=transforms.Compose ([transforms .ToTensor(),\n",
    "                                          \n",
    "])\n",
    "data_transform={'train':data_transform_train,\n",
    "                'val':data_transform_test\n",
    "                }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "60000\n",
      "10000\n"
     ]
    }
   ],
   "source": [
    "mnist_data_train=datasets.MNIST('MNIST_data',train=True,transform=data_transform['train'] ,download=True)\n",
    "mnist_data_test=datasets.MNIST('MNIST_data',train=False ,download=True ,transform=data_transform[\"val\"])\n",
    "dataloader={'train':DataLoader(mnist_data_train,shuffle=True ,batch_size=16),\n",
    "            'val':DataLoader(mnist_data_test,shuffle=True,batch_size=16)}\n",
    "dataset_sizes={'train':len(mnist_data_train),\n",
    "               'val':len(mnist_data_test)}\n",
    "device=torch.device( \"cpu\")\n",
    "print(dataset_sizes[\"train\"])\n",
    "print (dataset_sizes[\"val\"])\n",
    "#for images,labels in dataloader[\"train\"]:\n",
    "  #print(images.shape ,type(images ),labels)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class Model(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super (Model,self ).__init__()\n",
    "        self .linear1=nn.Linear (28*28,3000)\n",
    "        self.linear2=nn.Linear (3000,10)\n",
    "        \n",
    "    def Forward (self,x,mask,p):\n",
    "        #feed forward function \n",
    "        x=x.view(-1,28*28)\n",
    "        act1=relu(self.linear1(x))\n",
    "        act1_masked =self.masking (act1,mask,p)\n",
    "        act2=softmax(self.linear2(act1_masked ))\n",
    "        return act2\n",
    "    \n",
    "    \n",
    "    \n",
    "    def masking (self,act1,mask,p):\n",
    "        if (self.training ==True ):\n",
    "            return ((act1*mask)/p)\n",
    "        else :\n",
    "            return (act1)\n",
    "        \n",
    "\n",
    "\n",
    "\n",
    "  \n",
    "  \n",
    "  \n",
    "\n",
    "\n",
    "      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "model=Model()#creating the object of the class\n",
    "model.to(device)\n",
    "criterion=nn.CrossEntropyLoss()\n",
    "optimizer=torch.optim.SGD(model.parameters(),lr=0.001,momentum=0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, criterion, optimizer,mask,p, num_epochs=25):\n",
    "  # the mask has to be specified \n",
    "\n",
    "      since = time.time()\n",
    "\n",
    "      best_model_wts = copy.deepcopy(model.state_dict())\n",
    "      best_acc = 0.0\n",
    "      losses=[]\n",
    "      accuracies=[]\n",
    "\n",
    "      for epoch in range(num_epochs):\n",
    "          print('Epoch {}/{}'.format(epoch, num_epochs - 1))\n",
    "          print('-' * 10)\n",
    "\n",
    "          # Each epoch has a training and validation phase\n",
    "          for phase in ['train', 'val']:\n",
    "              if phase == 'train':\n",
    "                  \n",
    "                  model.train()  # Set model to training mode\n",
    "              else:\n",
    "\n",
    "                  model.eval()   # Set model to evaluate mode\n",
    "\n",
    "              running_loss = 0.0\n",
    "              running_corrects = 0\n",
    "\n",
    "              # Iterate over data.\n",
    "              for inputs, labels in dataloader[phase]:\n",
    "                inputs=inputs.to(device)\n",
    "                labels=labels.to(device)\n",
    "                # zero the parameter gradients\n",
    "                optimizer.zero_grad()\n",
    "\n",
    "                # forward\n",
    "                # track history if only in train\n",
    "                with torch.set_grad_enabled(phase == 'train'):\n",
    "                    outputs= model.Forward(inputs,mask,p)\n",
    "\n",
    "                    _, preds = torch.max(outputs, 1)\n",
    "                    loss = criterion(outputs, labels)# backward + optimize only if in training phase\n",
    "                    if phase == 'train':\n",
    "                        loss.backward()\n",
    "                        optimizer.step()\n",
    "                        #print (model.linear1.weight.grad)\n",
    "                        #print (model.linear2.weight.grad)\n",
    "\n",
    "                  # statistics\n",
    "                running_loss += loss.item() * inputs.size(0)\n",
    "                running_corrects += torch.sum(preds == labels.data)\n",
    "                \n",
    "                  \n",
    "\n",
    "              epoch_loss = running_loss / dataset_sizes[phase]\n",
    "              epoch_acc = running_corrects.double() / dataset_sizes[phase]\n",
    "            \n",
    "\n",
    "              print('{} Loss: {:.4f} Acc: {:.4f} '.format(\n",
    "                  phase, epoch_loss, epoch_acc))\n",
    "\n",
    "              # deep copy the model\n",
    "              if phase == 'val' and epoch_acc > best_acc:\n",
    "                  best_acc = epoch_acc\n",
    "                  best_model_wts = copy.deepcopy(model.state_dict())\n",
    "              if phase=='train':\n",
    "                losses.append(epoch_loss)\n",
    "                accuracies.append(epoch_acc)\n",
    "\n",
    "\n",
    "          \n",
    "\n",
    "      time_elapsed = time.time() - since\n",
    "      print('Training complete in {:.0f}m {:.0f}s'.format(\n",
    "          time_elapsed // 60, time_elapsed % 60))\n",
    "      print('Best val Acc: {:4f}'.format(best_acc))\n",
    "\n",
    "      # load best model weights\n",
    "      model.load_state_dict(best_model_wts)\n",
    "      return model,losses,accuracies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Population:\n",
    "    \n",
    "    def __init__(self,m,num,maskLength):\n",
    "        # constructor for initialising the population list\n",
    "        #list of DNA objects\n",
    "        self.population=[]\n",
    "        #muation rate for mutation\n",
    "        self.mutation_rate=m\n",
    "        #maximum number of entities in the population\n",
    "        self.popmax=num\n",
    "\n",
    "        self.maskLength=maskLength\n",
    "        for i in range (num):\n",
    "            #creating a dna object\n",
    "            #an initial random population created \n",
    "            dna =DNA(self.maskLength)\n",
    "            self.population.append (dna)\n",
    "      \n",
    "        self.matingPool=[]\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    def calcFitness (self,model):\n",
    "        # going through all the entities of population \n",
    "        #finding fitness of all population entities \n",
    "        for i in range(0,self.popmax):\n",
    "            self.population[i].fitness (model)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    def naturalSelection(self):\n",
    "        self.matingPool=[]\n",
    "        maxFitness=0\n",
    "        for i in range (self.popmax):\n",
    "            # moving throught the entire population \n",
    "            if (self.population[i].fit>maxFitness):\n",
    "                maxFitness=self.population[i].fit\n",
    "       \n",
    "        # max Fitness has the maximum loss score of the entire population  \n",
    "        for i in range (self.popmax ):\n",
    "        # iterating through the all inviduals of the population\n",
    "            n=self.Mymap(self.population[i].fit,0,maxFitness,0,1)\n",
    "        \n",
    "            n=math.floor(n*100)\n",
    "            \n",
    "            for j in range (n):\n",
    "                #creating mating pool\n",
    "                self.matingPool.append (self.population[i])\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    def Mymap(self,num,prevlow,prevhigh,nextlow,nexthigh):\n",
    "        \n",
    "        prevrange =float((num-prevlow)/(prevhigh-prevlow))\n",
    "        return nextlow+(nexthigh-nextlow)*prevrange\n",
    "\n",
    "\n",
    "\n",
    "    def   generate (self):\n",
    "        for i in range (self.popmax ):\n",
    "            index_1=math.floor(random.randint  (0,len(self.matingPool)-1))\n",
    "            index_2=math.floor (random.randint (0,len(self.matingPool)-1))\n",
    "            parent1=self.matingPool[index_1]\n",
    "            parent2=self.matingPool[index_2]\n",
    "            child=parent1.crossover(parent2)\n",
    "            child.mutate(self.mutation_rate)\n",
    "            self.population[i]=child \n",
    "\n",
    "\n",
    "    def fittest(self):\n",
    "        #returns the fiitest individual mask of the population \n",
    "        #also returns the keeping probability of the fittest mask \n",
    "        fittest=self.population[0]\n",
    "        for i  in range (self.popmax):\n",
    "            if (fittest.fit<self.population[i].fit):\n",
    "                fittest=self.population[i]\n",
    "        return fittest,fittest.keep_prob()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "class DNA:\n",
    "    \n",
    "    \n",
    "    def __init__(self,maskLength):\n",
    "        #constructor for the creation of the mask as a gene object \n",
    "        self.maskLength=maskLength\n",
    "        #creation of mask \n",
    "        self.gene=torch.bernoulli(torch.empty(1,maskLength).uniform_(0,1))\n",
    "        self.fit=0\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    def keep_prob (self):\n",
    "\n",
    "        num_one =0\n",
    "        for i in range (self.maskLength):\n",
    "            if (self.gene[0,i]==1):\n",
    "                num_one=num_one+1\n",
    "        return float(num_one/self.maskLength)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    def fitness(self,model):\n",
    "        # finding the fitness of a particular mask\n",
    "        #accuracy of all training set is the fitness in one epoch\n",
    "        #putting model in train mode \n",
    "        running_loss=0\n",
    "        model.train()\n",
    "        \n",
    "        for inputs,labels in dataloader ['train']:\n",
    "            inputs=inputs.to(device)\n",
    "            labels=labels.to(device )\n",
    "            outputs=model.Forward(inputs,self.gene,self.keep_prob())\n",
    "            _,preds=torch.max(outputs,1)\n",
    "            loss=criterion (outputs,labels)\n",
    "            running_loss += loss.item() * inputs.size(0)\n",
    "        epoch_loss = running_loss / dataset_sizes['train']\n",
    "        \n",
    "        self.fit=epoch_loss\n",
    "        return epoch_loss\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    def crossover (self,parent2):\n",
    "        #one parent is the passed in the argument \n",
    "        #another parent is the one from which this function is called \n",
    "        #another parent is self.gene\n",
    "        child =DNA(self.maskLength)\n",
    "        midpoint =random .randint (0,self.maskLength-1)\n",
    "        for i in range (0,self.maskLength):\n",
    "            if (i>midpoint):\n",
    "                child.gene [0,i]=self.gene[0,i]\n",
    "            else :\n",
    "                child.gene [0,i]=parent2.gene[0,i]\n",
    "        \n",
    "        return child \n",
    "\n",
    "    def mutate(self,mutation_rate):\n",
    "        #randomly activate some of the nodes  \n",
    "        #mutate some of the genes \n",
    "        for i in range (self.maskLength):\n",
    "            if (random.randint (0,99)<=mutation_rate*100):\n",
    "                self.gene[0,i]=1\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "mutation_rate =0.15\n",
    "max_population=40\n",
    "maskLength=3000\n",
    "model=Model()#creating the object of the class\n",
    "model.to(device)\n",
    "criterion=nn.CrossEntropyLoss()\n",
    "optimizer=torch.optim.SGD(model.parameters(),lr=0.001,momentum=0.9)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch generations ( 0 /300) :"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/cmater/anaconda3/envs/cmater/lib/python3.6/site-packages/ipykernel_launcher.py:12: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  if sys.path[0] == '':\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss (fittest mask) 2.3041300769170125\n",
      "Epoch 0/24\n",
      "----------\n",
      "train Loss: 1.8198 Acc: 0.7152 \n",
      "val Loss: 1.7668 Acc: 0.8323 \n",
      "Epoch 1/24\n",
      "----------\n",
      "train Loss: 1.6458 Acc: 0.8338 \n",
      "val Loss: 1.7058 Acc: 0.8470 \n",
      "Epoch 2/24\n",
      "----------\n",
      "train Loss: 1.6010 Acc: 0.8807 \n",
      "val Loss: 1.6487 Acc: 0.9142 \n",
      "Epoch 3/24\n",
      "----------\n",
      "train Loss: 1.5628 Acc: 0.9154 \n",
      "val Loss: 1.6193 Acc: 0.9243 \n",
      "Epoch 4/24\n",
      "----------\n",
      "train Loss: 1.5519 Acc: 0.9225 \n",
      "val Loss: 1.6051 Acc: 0.9286 \n",
      "Epoch 5/24\n",
      "----------\n",
      "train Loss: 1.5447 Acc: 0.9278 \n",
      "val Loss: 1.5946 Acc: 0.9319 \n",
      "Epoch 6/24\n",
      "----------\n",
      "train Loss: 1.5391 Acc: 0.9331 \n",
      "val Loss: 1.5862 Acc: 0.9357 \n",
      "Epoch 7/24\n",
      "----------\n",
      "train Loss: 1.5345 Acc: 0.9365 \n",
      "val Loss: 1.5790 Acc: 0.9392 \n",
      "Epoch 8/24\n",
      "----------\n",
      "train Loss: 1.5304 Acc: 0.9401 \n",
      "val Loss: 1.5744 Acc: 0.9423 \n",
      "Epoch 9/24\n",
      "----------\n",
      "train Loss: 1.5271 Acc: 0.9428 \n",
      "val Loss: 1.5690 Acc: 0.9433 \n",
      "Epoch 10/24\n",
      "----------\n",
      "train Loss: 1.5239 Acc: 0.9456 \n",
      "val Loss: 1.5669 Acc: 0.9446 \n",
      "Epoch 11/24\n",
      "----------\n",
      "train Loss: 1.5211 Acc: 0.9486 \n",
      "val Loss: 1.5606 Acc: 0.9468 \n",
      "Epoch 12/24\n",
      "----------\n",
      "train Loss: 1.5186 Acc: 0.9507 \n",
      "val Loss: 1.5583 Acc: 0.9483 \n",
      "Epoch 13/24\n",
      "----------\n",
      "train Loss: 1.5163 Acc: 0.9526 \n",
      "val Loss: 1.5576 Acc: 0.9502 \n",
      "Epoch 14/24\n",
      "----------\n",
      "train Loss: 1.5142 Acc: 0.9542 \n",
      "val Loss: 1.5536 Acc: 0.9508 \n",
      "Epoch 15/24\n",
      "----------\n",
      "train Loss: 1.5122 Acc: 0.9562 \n",
      "val Loss: 1.5505 Acc: 0.9522 \n",
      "Epoch 16/24\n",
      "----------\n",
      "train Loss: 1.5103 Acc: 0.9584 \n",
      "val Loss: 1.5493 Acc: 0.9544 \n",
      "Epoch 17/24\n",
      "----------\n",
      "train Loss: 1.5086 Acc: 0.9595 \n",
      "val Loss: 1.5461 Acc: 0.9547 \n",
      "Epoch 18/24\n",
      "----------\n",
      "train Loss: 1.5069 Acc: 0.9610 \n",
      "val Loss: 1.5446 Acc: 0.9561 \n",
      "Epoch 19/24\n",
      "----------\n",
      "train Loss: 1.5054 Acc: 0.9625 \n",
      "val Loss: 1.5420 Acc: 0.9570 \n",
      "Epoch 20/24\n",
      "----------\n",
      "train Loss: 1.5040 Acc: 0.9637 \n",
      "val Loss: 1.5402 Acc: 0.9587 \n",
      "Epoch 21/24\n",
      "----------\n",
      "train Loss: 1.5027 Acc: 0.9647 \n",
      "val Loss: 1.5387 Acc: 0.9587 \n",
      "Epoch 22/24\n",
      "----------\n",
      "train Loss: 1.5013 Acc: 0.9659 \n",
      "val Loss: 1.5375 Acc: 0.9612 \n",
      "Epoch 23/24\n",
      "----------\n",
      "train Loss: 1.5001 Acc: 0.9673 \n",
      "val Loss: 1.5356 Acc: 0.9617 \n",
      "Epoch 24/24\n",
      "----------\n",
      "train Loss: 1.4989 Acc: 0.9685 \n",
      "val Loss: 1.5350 Acc: 0.9631 \n",
      "Training complete in 21m 53s\n",
      "Best val Acc: 0.963100\n",
      "Epoch generations ( 1 /300) :loss (fittest mask) 1.576093652566274\n",
      "Epoch generations ( 2 /300) :loss (fittest mask) 1.5578557957967123\n",
      "Epoch generations ( 3 /300) :loss (fittest mask) 1.5553206120491028\n",
      "Epoch generations ( 4 /300) :loss (fittest mask) 1.5497650616645813\n",
      "Epoch generations ( 5 /300) :loss (fittest mask) 1.5457107919375102\n",
      "Epoch generations ( 6 /300) :loss (fittest mask) 1.5426183661778767\n",
      "Epoch generations ( 7 /300) :loss (fittest mask) 1.5414290545463563\n",
      "Epoch generations ( 8 /300) :loss (fittest mask) 1.5396748207092286\n",
      "Epoch generations ( 9 /300) :loss (fittest mask) 1.539163761774699\n",
      "Epoch generations ( 10 /300) :loss (fittest mask) 1.5384893550872802\n",
      "Epoch generations ( 11 /300) :loss (fittest mask) 1.537639849948883\n",
      "Epoch generations ( 12 /300) :loss (fittest mask) 1.5369914956092834\n",
      "Epoch generations ( 13 /300) :loss (fittest mask) 1.536696068986257\n",
      "Epoch generations ( 14 /300) :loss (fittest mask) 1.535523256746928\n",
      "Epoch generations ( 15 /300) :loss (fittest mask) 1.5353849982261658\n",
      "Epoch generations ( 16 /300) :loss (fittest mask) 1.5347413553237914\n",
      "Epoch generations ( 17 /300) :loss (fittest mask) 1.5357322187423705\n",
      "Epoch generations ( 18 /300) :loss (fittest mask) 1.5359318793932597\n",
      "Epoch generations ( 19 /300) :loss (fittest mask) 1.5345068888346354\n",
      "Epoch generations ( 20 /300) :loss (fittest mask) 1.5342742714881896\n",
      "Epoch generations ( 21 /300) :loss (fittest mask) 1.5339633778572082\n",
      "Epoch generations ( 22 /300) :loss (fittest mask) 1.5340413405418396\n",
      "Epoch generations ( 23 /300) :loss (fittest mask) 1.5341350407282512\n",
      "Epoch generations ( 24 /300) :loss (fittest mask) 1.533921354230245\n",
      "Epoch generations ( 25 /300) :loss (fittest mask) 1.5340979991912842\n",
      "Epoch generations ( 26 /300) :loss (fittest mask) 1.53401425298055\n",
      "Epoch generations ( 27 /300) :loss (fittest mask) 1.5338078002611797\n",
      "Epoch generations ( 28 /300) :loss (fittest mask) 1.533663771502177\n",
      "Epoch generations ( 29 /300) :loss (fittest mask) 1.5337798864046732\n",
      "Epoch generations ( 30 /300) :loss (fittest mask) 1.533633268292745\n",
      "Epoch generations ( 31 /300) :loss (fittest mask) 1.5336219760576884\n",
      "Epoch generations ( 32 /300) :loss (fittest mask) 1.5336695547421773\n",
      "Epoch generations ( 33 /300) :loss (fittest mask) 1.5337603490829468\n",
      "Epoch generations ( 34 /300) :loss (fittest mask) 1.533540407148997\n",
      "Epoch generations ( 35 /300) :loss (fittest mask) 1.533826580174764\n",
      "Epoch generations ( 36 /300) :loss (fittest mask) 1.5336943805376688\n",
      "Epoch generations ( 37 /300) :loss (fittest mask) 1.5336557865142821\n",
      "Epoch generations ( 38 /300) :loss (fittest mask) 1.5337987745602926\n",
      "Epoch generations ( 39 /300) :loss (fittest mask) 1.5337635658899944\n",
      "Epoch generations ( 40 /300) :loss (fittest mask) 1.5337987727483113\n",
      "Epoch generations ( 41 /300) :loss (fittest mask) 1.5337987728436788\n",
      "Epoch generations ( 42 /300) :loss (fittest mask) 1.5335542226155598\n",
      "Epoch generations ( 43 /300) :loss (fittest mask) 1.5335013834635416\n",
      "Epoch generations ( 44 /300) :loss (fittest mask) 1.5335143130302429\n",
      "Epoch generations ( 45 /300) :loss (fittest mask) 1.5334690382957459\n",
      "Epoch generations ( 46 /300) :loss (fittest mask) 1.5334690354029337\n",
      "Epoch generations ( 47 /300) :loss (fittest mask) 1.5334690380096436\n",
      "Epoch generations ( 48 /300) :loss (fittest mask) 1.5334690351804097\n",
      "Epoch generations ( 49 /300) :loss (fittest mask) 1.5334690391540526\n",
      "Epoch generations ( 50 /300) :loss (fittest mask) 1.5335489835739136\n",
      "Epoch 0/24\n",
      "----------\n",
      "train Loss: 1.5241 Acc: 0.9667 \n",
      "val Loss: 1.5216 Acc: 0.9589 \n",
      "Epoch 1/24\n",
      "----------\n",
      "train Loss: 1.5173 Acc: 0.9653 \n",
      "val Loss: 1.5176 Acc: 0.9589 \n",
      "Epoch 2/24\n",
      "----------\n",
      "train Loss: 1.5138 Acc: 0.9657 \n",
      "val Loss: 1.5151 Acc: 0.9592 \n",
      "Epoch 3/24\n",
      "----------\n",
      "train Loss: 1.5113 Acc: 0.9661 \n",
      "val Loss: 1.5135 Acc: 0.9598 \n",
      "Epoch 4/24\n",
      "----------\n",
      "train Loss: 1.5093 Acc: 0.9669 \n",
      "val Loss: 1.5118 Acc: 0.9597 \n",
      "Epoch 5/24\n",
      "----------\n",
      "train Loss: 1.5076 Acc: 0.9675 \n",
      "val Loss: 1.5104 Acc: 0.9614 \n",
      "Epoch 6/24\n",
      "----------\n",
      "train Loss: 1.5061 Acc: 0.9678 \n",
      "val Loss: 1.5092 Acc: 0.9619 \n",
      "Epoch 7/24\n",
      "----------\n",
      "train Loss: 1.5048 Acc: 0.9686 \n",
      "val Loss: 1.5078 Acc: 0.9622 \n",
      "Epoch 8/24\n",
      "----------\n",
      "train Loss: 1.5036 Acc: 0.9693 \n",
      "val Loss: 1.5070 Acc: 0.9635 \n",
      "Epoch 9/24\n",
      "----------\n",
      "train Loss: 1.5025 Acc: 0.9698 \n",
      "val Loss: 1.5061 Acc: 0.9638 \n",
      "Epoch 10/24\n",
      "----------\n",
      "train Loss: 1.5015 Acc: 0.9702 \n",
      "val Loss: 1.5052 Acc: 0.9639 \n",
      "Epoch 11/24\n",
      "----------\n",
      "train Loss: 1.5006 Acc: 0.9710 \n",
      "val Loss: 1.5044 Acc: 0.9649 \n",
      "Epoch 12/24\n",
      "----------\n",
      "train Loss: 1.4997 Acc: 0.9715 \n",
      "val Loss: 1.5037 Acc: 0.9661 \n",
      "Epoch 13/24\n",
      "----------\n",
      "train Loss: 1.4988 Acc: 0.9721 \n",
      "val Loss: 1.5030 Acc: 0.9660 \n",
      "Epoch 14/24\n",
      "----------\n",
      "train Loss: 1.4980 Acc: 0.9726 \n",
      "val Loss: 1.5022 Acc: 0.9670 \n",
      "Epoch 15/24\n",
      "----------\n",
      "train Loss: 1.4973 Acc: 0.9733 \n",
      "val Loss: 1.5019 Acc: 0.9662 \n",
      "Epoch 16/24\n",
      "----------\n",
      "train Loss: 1.4966 Acc: 0.9737 \n",
      "val Loss: 1.5014 Acc: 0.9668 \n",
      "Epoch 17/24\n",
      "----------\n",
      "train Loss: 1.4959 Acc: 0.9739 \n",
      "val Loss: 1.5007 Acc: 0.9677 \n",
      "Epoch 18/24\n",
      "----------\n",
      "train Loss: 1.4953 Acc: 0.9746 \n",
      "val Loss: 1.5003 Acc: 0.9672 \n",
      "Epoch 19/24\n",
      "----------\n",
      "train Loss: 1.4946 Acc: 0.9751 \n",
      "val Loss: 1.4999 Acc: 0.9682 \n",
      "Epoch 20/24\n",
      "----------\n",
      "train Loss: 1.4941 Acc: 0.9751 \n",
      "val Loss: 1.4994 Acc: 0.9682 \n",
      "Epoch 21/24\n",
      "----------\n",
      "train Loss: 1.4935 Acc: 0.9756 \n",
      "val Loss: 1.4990 Acc: 0.9686 \n",
      "Epoch 22/24\n",
      "----------\n",
      "train Loss: 1.4930 Acc: 0.9760 \n",
      "val Loss: 1.4986 Acc: 0.9691 \n",
      "Epoch 23/24\n",
      "----------\n",
      "train Loss: 1.4924 Acc: 0.9764 \n",
      "val Loss: 1.4982 Acc: 0.9691 \n",
      "Epoch 24/24\n",
      "----------\n",
      "train Loss: 1.4919 Acc: 0.9768 \n",
      "val Loss: 1.4978 Acc: 0.9699 \n",
      "Training complete in 30m 28s\n",
      "Best val Acc: 0.969900\n",
      "Epoch generations ( 51 /300) :loss (fittest mask) 1.491377433681488\n",
      "Epoch generations ( 52 /300) :loss (fittest mask) 1.491377433681488\n",
      "Epoch generations ( 53 /300) :loss (fittest mask) 1.4913774325052898\n",
      "Epoch generations ( 54 /300) :loss (fittest mask) 1.491377433427175\n",
      "Epoch generations ( 55 /300) :loss (fittest mask) 1.4913756981213888\n",
      "Epoch generations ( 56 /300) :loss (fittest mask) 1.4913756957372029\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch generations ( 57 /300) :loss (fittest mask) 1.4913756962458293\n",
      "Epoch generations ( 58 /300) :loss (fittest mask) 1.4913756969769796\n",
      "Epoch generations ( 59 /300) :loss (fittest mask) 1.4913756957372029\n",
      "Epoch generations ( 60 /300) :loss (fittest mask) 1.4913756986618043\n",
      "Epoch generations ( 61 /300) :loss (fittest mask) 1.491375697072347\n",
      "Epoch generations ( 62 /300) :loss (fittest mask) 1.4913756990432738\n",
      "Epoch generations ( 63 /300) :loss (fittest mask) 1.4913756976445516\n",
      "Epoch generations ( 64 /300) :loss (fittest mask) 1.4913756992022196\n",
      "Epoch generations ( 65 /300) :loss (fittest mask) 1.4913756972312928\n",
      "Epoch generations ( 66 /300) :loss (fittest mask) 1.4913756965955098\n",
      "Epoch generations ( 67 /300) :loss (fittest mask) 1.4913756948471069\n",
      "Epoch generations ( 68 /300) :loss (fittest mask) 1.4913756981531778\n",
      "Epoch generations ( 69 /300) :loss (fittest mask) 1.4913756994565328\n",
      "Epoch generations ( 70 /300) :loss (fittest mask) 1.4913756985982258\n",
      "Epoch generations ( 71 /300) :loss (fittest mask) 1.4913756976127623\n",
      "Epoch generations ( 72 /300) :loss (fittest mask) 1.4913756985664368\n",
      "Epoch generations ( 73 /300) :loss (fittest mask) 1.4913756970405578\n",
      "Epoch generations ( 74 /300) :loss (fittest mask) 1.491375697294871\n",
      "Epoch generations ( 75 /300) :loss (fittest mask) 1.4913756967862448\n",
      "Epoch generations ( 76 /300) :loss (fittest mask) 1.4913756971359253\n",
      "Epoch generations ( 77 /300) :loss (fittest mask) 1.4913756990432738\n",
      "Epoch generations ( 78 /300) :loss (fittest mask) 1.4913756958961486\n",
      "Epoch generations ( 79 /300) :loss (fittest mask) 1.4913756994565328\n",
      "Epoch generations ( 80 /300) :loss (fittest mask) 1.49137569843928\n",
      "Epoch generations ( 81 /300) :loss (fittest mask) 1.4913756974538168\n",
      "Epoch generations ( 82 /300) :loss (fittest mask) 1.4913756971995036\n",
      "Epoch generations ( 83 /300) :loss (fittest mask) 1.4913756963411966\n",
      "Epoch generations ( 84 /300) :loss (fittest mask) 1.491375698630015\n",
      "Epoch generations ( 85 /300) :loss (fittest mask) 1.4913757000923156\n",
      "Epoch generations ( 86 /300) :loss (fittest mask) 1.4913756982803346\n",
      "Epoch generations ( 87 /300) :loss (fittest mask) 1.4913756971359253\n",
      "Epoch generations ( 88 /300) :loss (fittest mask) 1.4913756982803346\n",
      "Epoch generations ( 89 /300) :loss (fittest mask) 1.4913757002830506\n",
      "Epoch generations ( 90 /300) :loss (fittest mask) 1.4913756991386413\n",
      "Epoch generations ( 91 /300) :loss (fittest mask) 1.4913756984710693\n",
      "Epoch generations ( 92 /300) :loss (fittest mask) 1.4913756995836893\n",
      "Epoch generations ( 93 /300) :loss (fittest mask) 1.4913756971995036\n",
      "Epoch generations ( 94 /300) :loss (fittest mask) 1.49137569732666\n",
      "Epoch generations ( 95 /300) :loss (fittest mask) 1.4913756986935933\n",
      "Epoch generations ( 96 /300) :loss (fittest mask) 1.4913756998062133\n",
      "Epoch generations ( 97 /300) :loss (fittest mask) 1.491375693766276\n",
      "Epoch generations ( 98 /300) :loss (fittest mask) 1.4913756956418356\n",
      "Epoch generations ( 99 /300) :loss (fittest mask) 1.4913756987571716\n",
      "Epoch generations ( 100 /300) :loss (fittest mask) 1.4913756954828898\n",
      "Epoch 0/24\n",
      "----------\n",
      "train Loss: 1.4915 Acc: 0.9772 \n",
      "val Loss: 1.4973 Acc: 0.9701 \n",
      "Epoch 1/24\n",
      "----------\n",
      "train Loss: 1.4910 Acc: 0.9776 \n",
      "val Loss: 1.4971 Acc: 0.9704 \n",
      "Epoch 2/24\n",
      "----------\n",
      "train Loss: 1.4905 Acc: 0.9779 \n",
      "val Loss: 1.4967 Acc: 0.9708 \n",
      "Epoch 3/24\n",
      "----------\n",
      "train Loss: 1.4901 Acc: 0.9783 \n",
      "val Loss: 1.4966 Acc: 0.9707 \n",
      "Epoch 4/24\n",
      "----------\n",
      "train Loss: 1.4897 Acc: 0.9786 \n",
      "val Loss: 1.4959 Acc: 0.9712 \n",
      "Epoch 5/24\n",
      "----------\n",
      "train Loss: 1.4893 Acc: 0.9787 \n",
      "val Loss: 1.4956 Acc: 0.9716 \n",
      "Epoch 6/24\n",
      "----------\n",
      "train Loss: 1.4889 Acc: 0.9789 \n",
      "val Loss: 1.4954 Acc: 0.9720 \n",
      "Epoch 7/24\n",
      "----------\n",
      "train Loss: 1.4885 Acc: 0.9795 \n",
      "val Loss: 1.4951 Acc: 0.9728 \n",
      "Epoch 8/24\n",
      "----------\n",
      "train Loss: 1.4881 Acc: 0.9798 \n",
      "val Loss: 1.4948 Acc: 0.9723 \n",
      "Epoch 9/24\n",
      "----------\n",
      "train Loss: 1.4878 Acc: 0.9798 \n",
      "val Loss: 1.4945 Acc: 0.9722 \n",
      "Epoch 10/24\n",
      "----------\n",
      "train Loss: 1.4874 Acc: 0.9802 \n",
      "val Loss: 1.4945 Acc: 0.9726 \n",
      "Epoch 11/24\n",
      "----------\n",
      "train Loss: 1.4871 Acc: 0.9804 \n",
      "val Loss: 1.4942 Acc: 0.9729 \n",
      "Epoch 12/24\n",
      "----------\n",
      "train Loss: 1.4868 Acc: 0.9807 \n",
      "val Loss: 1.4939 Acc: 0.9729 \n",
      "Epoch 13/24\n",
      "----------\n",
      "train Loss: 1.4864 Acc: 0.9811 \n",
      "val Loss: 1.4938 Acc: 0.9730 \n",
      "Epoch 14/24\n",
      "----------\n",
      "train Loss: 1.4861 Acc: 0.9811 \n",
      "val Loss: 1.4934 Acc: 0.9731 \n",
      "Epoch 15/24\n",
      "----------\n",
      "train Loss: 1.4858 Acc: 0.9815 \n",
      "val Loss: 1.4932 Acc: 0.9732 \n",
      "Epoch 16/24\n",
      "----------\n",
      "train Loss: 1.4855 Acc: 0.9817 \n",
      "val Loss: 1.4930 Acc: 0.9735 \n",
      "Epoch 17/24\n",
      "----------\n",
      "train Loss: 1.4852 Acc: 0.9819 \n",
      "val Loss: 1.4929 Acc: 0.9737 \n",
      "Epoch 18/24\n",
      "----------\n",
      "train Loss: 1.4849 Acc: 0.9820 \n",
      "val Loss: 1.4928 Acc: 0.9738 \n",
      "Epoch 19/24\n",
      "----------\n",
      "train Loss: 1.4846 Acc: 0.9824 \n",
      "val Loss: 1.4926 Acc: 0.9741 \n",
      "Epoch 20/24\n",
      "----------\n",
      "train Loss: 1.4844 Acc: 0.9828 \n",
      "val Loss: 1.4925 Acc: 0.9742 \n",
      "Epoch 21/24\n",
      "----------\n",
      "train Loss: 1.4841 Acc: 0.9829 \n",
      "val Loss: 1.4921 Acc: 0.9745 \n",
      "Epoch 22/24\n",
      "----------\n",
      "train Loss: 1.4838 Acc: 0.9831 \n",
      "val Loss: 1.4920 Acc: 0.9738 \n",
      "Epoch 23/24\n",
      "----------\n",
      "train Loss: 1.4836 Acc: 0.9832 \n",
      "val Loss: 1.4920 Acc: 0.9745 \n",
      "Epoch 24/24\n",
      "----------\n",
      "train Loss: 1.4833 Acc: 0.9834 \n",
      "val Loss: 1.4916 Acc: 0.9745 \n",
      "Training complete in 20m 59s\n",
      "Best val Acc: 0.974500\n",
      "Epoch generations ( 101 /300) :loss (fittest mask) 1.4836777234713237\n",
      "Epoch generations ( 102 /300) :loss (fittest mask) 1.4836777268727621\n",
      "Epoch generations ( 103 /300) :loss (fittest mask) 1.4836777243932089\n",
      "Epoch generations ( 104 /300) :loss (fittest mask) 1.4836777270634969\n",
      "Epoch generations ( 105 /300) :loss (fittest mask) 1.4836777267456054\n",
      "Epoch generations ( 106 /300) :loss (fittest mask) 1.4836777250607809\n",
      "Epoch generations ( 107 /300) :loss (fittest mask) 1.4836777233441671\n",
      "Epoch generations ( 108 /300) :loss (fittest mask) 1.4836777232170104\n",
      "Epoch generations ( 109 /300) :loss (fittest mask) 1.4836777272860209\n",
      "Epoch generations ( 110 /300) :loss (fittest mask) 1.4836777260144551\n",
      "Epoch generations ( 111 /300) :loss (fittest mask) 1.4836777255058289\n",
      "Epoch generations ( 112 /300) :loss (fittest mask) 1.4836777266820271\n",
      "Epoch generations ( 113 /300) :loss (fittest mask) 1.4836777242978414\n",
      "Epoch generations ( 114 /300) :loss (fittest mask) 1.483677723089854\n",
      "Epoch generations ( 115 /300) :loss (fittest mask) 1.4836777267773946\n",
      "Epoch generations ( 116 /300) :loss (fittest mask) 1.4836777249654134\n",
      "Epoch generations ( 117 /300) :loss (fittest mask) 1.4836777256011964\n",
      "Epoch generations ( 118 /300) :loss (fittest mask) 1.48367772286733\n",
      "Epoch generations ( 119 /300) :loss (fittest mask) 1.4836777232170104\n",
      "Epoch generations ( 120 /300) :loss (fittest mask) 1.4836777261098226\n",
      "Epoch generations ( 121 /300) :loss (fittest mask) 1.4836777238210042\n",
      "Epoch generations ( 122 /300) :loss (fittest mask) 1.4836777247428894\n",
      "Epoch generations ( 123 /300) :loss (fittest mask) 1.4836777252197266\n",
      "Epoch generations ( 124 /300) :loss (fittest mask) 1.4836777249972026\n",
      "Epoch generations ( 125 /300) :loss (fittest mask) 1.4836777258555094\n",
      "Epoch generations ( 126 /300) :loss (fittest mask) 1.4836777262687684\n",
      "Epoch generations ( 127 /300) :loss (fittest mask) 1.4836777257283529\n",
      "Epoch generations ( 128 /300) :loss (fittest mask) 1.4836777224540711\n",
      "Epoch generations ( 129 /300) :loss (fittest mask) 1.4836777252197266\n",
      "Epoch generations ( 130 /300) :loss (fittest mask) 1.4836777245839436\n",
      "Epoch generations ( 131 /300) :loss (fittest mask) 1.4836777255058289\n",
      "Epoch generations ( 132 /300) :loss (fittest mask) 1.4836777245203654\n",
      "Epoch generations ( 133 /300) :loss (fittest mask) 1.4836777242978414\n",
      "Epoch generations ( 134 /300) :loss (fittest mask) 1.4836777243296306\n",
      "Epoch generations ( 135 /300) :loss (fittest mask) 1.4836777270952861\n",
      "Epoch generations ( 136 /300) :loss (fittest mask) 1.4836777235984802\n",
      "Epoch generations ( 137 /300) :loss (fittest mask) 1.483677723534902\n",
      "Epoch generations ( 138 /300) :loss (fittest mask) 1.4836777272224426\n",
      "Epoch generations ( 139 /300) :loss (fittest mask) 1.4836777236620584\n",
      "Epoch generations ( 140 /300) :loss (fittest mask) 1.4836777241706849\n",
      "Epoch generations ( 141 /300) :loss (fittest mask) 1.483677724202474\n",
      "Epoch generations ( 142 /300) :loss (fittest mask) 1.4836777276674906\n",
      "Epoch generations ( 143 /300) :loss (fittest mask) 1.4836777259190876\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch generations ( 144 /300) :loss (fittest mask) 1.4836777241071066\n",
      "Epoch generations ( 145 /300) :loss (fittest mask) 1.4836777256011964\n",
      "Epoch generations ( 146 /300) :loss (fittest mask) 1.4836777249654134\n",
      "Epoch generations ( 147 /300) :loss (fittest mask) 1.4836777243614196\n",
      "Epoch generations ( 148 /300) :loss (fittest mask) 1.4836777240435282\n",
      "Epoch generations ( 149 /300) :loss (fittest mask) 1.4836777215003967\n",
      "Epoch generations ( 150 /300) :loss (fittest mask) 1.4836777216911317\n",
      "Epoch 0/24\n",
      "----------\n",
      "train Loss: 1.4838 Acc: 0.9830 \n",
      "val Loss: 1.4920 Acc: 0.9735 \n",
      "Epoch 1/24\n",
      "----------\n",
      "train Loss: 1.4836 Acc: 0.9832 \n",
      "val Loss: 1.4919 Acc: 0.9739 \n",
      "Epoch 2/24\n",
      "----------\n",
      "train Loss: 1.4833 Acc: 0.9833 \n",
      "val Loss: 1.4916 Acc: 0.9746 \n",
      "Epoch 3/24\n",
      "----------\n",
      "train Loss: 1.4831 Acc: 0.9837 \n",
      "val Loss: 1.4915 Acc: 0.9748 \n",
      "Epoch 4/24\n",
      "----------\n",
      "train Loss: 1.4829 Acc: 0.9837 \n",
      "val Loss: 1.4915 Acc: 0.9740 \n",
      "Epoch 5/24\n",
      "----------\n",
      "train Loss: 1.4826 Acc: 0.9839 \n",
      "val Loss: 1.4913 Acc: 0.9748 \n",
      "Epoch 6/24\n",
      "----------\n",
      "train Loss: 1.4824 Acc: 0.9841 \n",
      "val Loss: 1.4912 Acc: 0.9745 \n",
      "Epoch 7/24\n",
      "----------\n",
      "train Loss: 1.4822 Acc: 0.9843 \n",
      "val Loss: 1.4911 Acc: 0.9749 \n",
      "Epoch 8/24\n",
      "----------\n",
      "train Loss: 1.4819 Acc: 0.9846 \n",
      "val Loss: 1.4908 Acc: 0.9751 \n",
      "Epoch 9/24\n",
      "----------\n",
      "train Loss: 1.4817 Acc: 0.9845 \n",
      "val Loss: 1.4907 Acc: 0.9752 \n",
      "Epoch 10/24\n",
      "----------\n",
      "train Loss: 1.4815 Acc: 0.9848 \n",
      "val Loss: 1.4906 Acc: 0.9754 \n",
      "Epoch 11/24\n",
      "----------\n",
      "train Loss: 1.4813 Acc: 0.9849 \n",
      "val Loss: 1.4903 Acc: 0.9759 \n",
      "Epoch 12/24\n",
      "----------\n",
      "train Loss: 1.4811 Acc: 0.9849 \n",
      "val Loss: 1.4904 Acc: 0.9753 \n",
      "Epoch 13/24\n",
      "----------\n",
      "train Loss: 1.4809 Acc: 0.9852 \n",
      "val Loss: 1.4901 Acc: 0.9753 \n",
      "Epoch 14/24\n",
      "----------\n",
      "train Loss: 1.4807 Acc: 0.9852 \n",
      "val Loss: 1.4901 Acc: 0.9762 \n",
      "Epoch 15/24\n",
      "----------\n",
      "train Loss: 1.4806 Acc: 0.9854 \n",
      "val Loss: 1.4899 Acc: 0.9757 \n",
      "Epoch 16/24\n",
      "----------\n",
      "train Loss: 1.4804 Acc: 0.9857 \n",
      "val Loss: 1.4899 Acc: 0.9760 \n",
      "Epoch 17/24\n",
      "----------\n",
      "train Loss: 1.4802 Acc: 0.9858 \n",
      "val Loss: 1.4898 Acc: 0.9754 \n",
      "Epoch 18/24\n",
      "----------\n",
      "train Loss: 1.4800 Acc: 0.9859 \n",
      "val Loss: 1.4898 Acc: 0.9753 \n",
      "Epoch 19/24\n",
      "----------\n",
      "train Loss: 1.4798 Acc: 0.9859 \n",
      "val Loss: 1.4896 Acc: 0.9759 \n",
      "Epoch 20/24\n",
      "----------\n",
      "train Loss: 1.4797 Acc: 0.9861 \n",
      "val Loss: 1.4894 Acc: 0.9756 \n",
      "Epoch 21/24\n",
      "----------\n",
      "train Loss: 1.4795 Acc: 0.9863 \n",
      "val Loss: 1.4893 Acc: 0.9762 \n",
      "Epoch 22/24\n",
      "----------\n",
      "train Loss: 1.4793 Acc: 0.9864 \n",
      "val Loss: 1.4894 Acc: 0.9753 \n",
      "Epoch 23/24\n",
      "----------\n",
      "train Loss: 1.4792 Acc: 0.9864 \n",
      "val Loss: 1.4891 Acc: 0.9764 \n",
      "Epoch 24/24\n",
      "----------\n",
      "train Loss: 1.4790 Acc: 0.9867 \n",
      "val Loss: 1.4890 Acc: 0.9768 \n",
      "Training complete in 24m 9s\n",
      "Best val Acc: 0.976800\n",
      "Epoch generations ( 151 /300) :loss (fittest mask) 1.4787232414881388\n",
      "Epoch generations ( 152 /300) :loss (fittest mask) 1.4787232439359028\n",
      "Epoch generations ( 153 /300) :loss (fittest mask) 1.4787232438405356\n",
      "Epoch generations ( 154 /300) :loss (fittest mask) 1.4787232412338256\n",
      "Epoch generations ( 155 /300) :loss (fittest mask) 1.4787232437769573\n",
      "Epoch generations ( 156 /300) :loss (fittest mask) 1.4787232420285543\n",
      "Epoch generations ( 157 /300) :loss (fittest mask) 1.4787232410113016\n",
      "Epoch generations ( 158 /300) :loss (fittest mask) 1.4787232420603433\n",
      "Epoch generations ( 159 /300) :loss (fittest mask) 1.478723242441813\n",
      "Epoch generations ( 160 /300) :loss (fittest mask) 1.4787232424736023\n",
      "Epoch generations ( 161 /300) :loss (fittest mask) 1.4787232400894166\n",
      "Epoch generations ( 162 /300) :loss (fittest mask) 1.4787232436498006\n",
      "Epoch generations ( 163 /300) :loss (fittest mask) 1.4787232432683308\n",
      "Epoch generations ( 164 /300) :loss (fittest mask) 1.4787232420921326\n",
      "Epoch generations ( 165 /300) :loss (fittest mask) 1.4787232429504396\n",
      "Epoch generations ( 166 /300) :loss (fittest mask) 1.4787232412656148\n",
      "Epoch generations ( 167 /300) :loss (fittest mask) 1.4787232441266378\n",
      "Epoch generations ( 168 /300) :loss (fittest mask) 1.4787232423146566\n",
      "Epoch generations ( 169 /300) :loss (fittest mask) 1.4787232426007588\n",
      "Epoch generations ( 170 /300) :loss (fittest mask) 1.4787232448895773\n",
      "Epoch generations ( 171 /300) :loss (fittest mask) 1.478723241329193\n",
      "Epoch generations ( 172 /300) :loss (fittest mask) 1.4787232426961263\n",
      "Epoch generations ( 173 /300) :loss (fittest mask) 1.4787232415835063\n",
      "Epoch generations ( 174 /300) :loss (fittest mask) 1.4787232406298318\n",
      "Epoch generations ( 175 /300) :loss (fittest mask) 1.4787232413609823\n",
      "Epoch generations ( 176 /300) :loss (fittest mask) 1.4787232414563496\n",
      "Epoch generations ( 177 /300) :"
     ]
    }
   ],
   "source": [
    "#control block controls the epochs and the generations of mask\n",
    "#step 1 an object of the population class randomly generating the first population \n",
    "#step2 :calculate fitness of each entitiy of the population \n",
    "#step3: creates a mating pool of the population based on the worst two performing parent \n",
    "#step 4 :fittest mask of the generating along with keep_prob found \n",
    "#step 5: if 0th ,10th ,20th, the epochs starts training on the worst performing mask /other wise new generation is created \n",
    "\n",
    "epochgens=0\n",
    "population =Population(mutation_rate,max_population,maskLength)\n",
    "\n",
    "while (epochgens<300):\n",
    "    print ('Epoch generations (',epochgens,'/300)',end=' :')\n",
    "    population .calcFitness(model)\n",
    "    population.naturalSelection()\n",
    "    fittestmask,p=population .fittest()\n",
    "    loss=fittestmask.fitness(model)\n",
    "    print (\"loss (fittest mask)\",loss,end='\\n')\n",
    "    if (epochgens%50==0):\n",
    "        model,losses,accuracies=train_model(model,criterion,optimizer,fittestmask.gene,p,25)\n",
    "    population.generate()\n",
    "    epochgens+=1\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
