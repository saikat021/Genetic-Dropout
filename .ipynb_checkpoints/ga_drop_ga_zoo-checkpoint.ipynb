{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import random\n",
    "import torch\n",
    "import torchvision\n",
    "import time \n",
    "from torchvision import datasets ,transforms\n",
    "from torchvision.datasets import ImageFolder\n",
    "from torch.utils.data import DataLoader,Dataset\n",
    "from torch import nn \n",
    "from torch.nn.functional import relu ,softmax \n",
    "import copy \n",
    "from torch.utils.data import Subset\n",
    "import pandas as pd \n",
    "import numpy as np\n",
    "from PIL import Image\n",
    " \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_transform=transforms.Compose ([transforms.ToTensor()])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'Classifier'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[1;32mD:\\ana\\envs\\packt_torch\\lib\\site-packages\\pandas\\core\\indexes\\base.py\u001b[0m in \u001b[0;36mget_loc\u001b[1;34m(self, key, method, tolerance)\u001b[0m\n\u001b[0;32m   2645\u001b[0m             \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2646\u001b[1;33m                 \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2647\u001b[0m             \u001b[1;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mKeyError\u001b[0m: 'Classifier'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-3-64c5e99f525c>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     22\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mimg_as_tensor\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0msingle_label\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     23\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 24\u001b[1;33m \u001b[0mdataset\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mlight_source_dataset\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'Zoo/Zoo.csv'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mdata_transform\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     25\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     26\u001b[0m \u001b[0mtrain_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mint\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m*\u001b[0m\u001b[1;36m0.6\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-3-64c5e99f525c>\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, path, transform)\u001b[0m\n\u001b[0;32m      2\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mtransform\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdf_data\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdf_data\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'Classifier'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mCategorical\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfactorize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdf_data\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'Classifier'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      5\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlabels\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0masarray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdf_data\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0miloc\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdf_data\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mimage_as_np\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0masarray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdf_data\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0miloc\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdf_data\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'uint8'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\ana\\envs\\packt_torch\\lib\\site-packages\\pandas\\core\\frame.py\u001b[0m in \u001b[0;36m__getitem__\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   2798\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnlevels\u001b[0m \u001b[1;33m>\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2799\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_getitem_multilevel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2800\u001b[1;33m             \u001b[0mindexer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2801\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mis_integer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mindexer\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2802\u001b[0m                 \u001b[0mindexer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mindexer\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\ana\\envs\\packt_torch\\lib\\site-packages\\pandas\\core\\indexes\\base.py\u001b[0m in \u001b[0;36mget_loc\u001b[1;34m(self, key, method, tolerance)\u001b[0m\n\u001b[0;32m   2646\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2647\u001b[0m             \u001b[1;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2648\u001b[1;33m                 \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_maybe_cast_indexer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2649\u001b[0m         \u001b[0mindexer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_indexer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmethod\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mmethod\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtolerance\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtolerance\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2650\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mindexer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mndim\u001b[0m \u001b[1;33m>\u001b[0m \u001b[1;36m1\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mindexer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msize\u001b[0m \u001b[1;33m>\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mKeyError\u001b[0m: 'Classifier'"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "class light_source_dataset(Dataset):\n",
    "    def __init__(self,path,transform):\n",
    "        self.df_data=pd.read_csv(path)\n",
    "        self.df_data['Classifier'] = pd.Categorical(pd.factorize(self.df_data['Classifier'])[0])\n",
    "        self.labels=np.asarray(self.df_data.iloc[:,self.df_data.shape[1]-1])\n",
    "        self.image_as_np=np.asarray(self.df_data.iloc[:,0:self.df_data.shape[1]-1]).astype('uint8')\n",
    "        self.trans=transform\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.df_data.index)\n",
    "    def __getitem__(self,index):\n",
    "        image_np=self.image_as_np[index,:,None]\n",
    "       \n",
    "        pillow_image=Image.fromarray(image_np.astype('uint8'))\n",
    "        \n",
    "        single_label=self.labels[index]\n",
    "        if (self.trans is not None):\n",
    "            img_as_tensor=self.trans(pillow_image)\n",
    "        \n",
    "        \n",
    "            \n",
    "        return (img_as_tensor,single_label)\n",
    "        \n",
    "dataset=light_source_dataset('Zoo/Zoo.csv',data_transform)\n",
    "print(len(dataset))\n",
    "train_size=int (len(dataset)*0.6)\n",
    "test_size=len(dataset)-train_size\n",
    "trainloader=DataLoader(dataset,batch_size=1)\n",
    "torch.manual_seed(1)\n",
    "train_data,test_data=torch.utils.data.random_split(dataset,[int (train_size), int (test_size)])\n",
    "dataloader={'train':DataLoader(train_data,shuffle=False ,batch_size=16),\n",
    "            'val':DataLoader(test_data,shuffle=False,batch_size=16\n",
    "                            )}\n",
    "\n",
    "dataset_sizes={'train':len(train_data),\n",
    "               'val':len(test_data)}\n",
    "\n",
    "device=torch.device(\"cuda:0\"if torch.cuda.is_available () else \"cpu\")\n",
    "print(dataset_sizes[\"train\"])\n",
    "print (dataset_sizes[\"val\"])\n",
    "#for images,labels in dataloader['train']:\n",
    " #   print(labels)\n",
    "\n",
    " \n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class Model(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super (Model,self ).__init__()\n",
    "        self .linear1=nn.Linear (1440,1000)\n",
    "        self.linear2=nn.Linear (1000,7)\n",
    "        \n",
    "    def Forward (self,x,mask,p):\n",
    "        #feed forward function \n",
    "        x=x.view(-1,1440)\n",
    "        act1=relu(self.linear1(x))\n",
    "        act1_masked =self.masking (act1,mask,p)\n",
    "        act2=softmax(self.linear2(act1_masked ))\n",
    "        return act2\n",
    "    \n",
    "    \n",
    "    \n",
    "    def masking (self,act1,mask,p):\n",
    "        if (self.training ==True ):\n",
    "            return ((act1*mask)/p)\n",
    "        else :\n",
    "            return (act1)\n",
    "        \n",
    "\n",
    "\n",
    "\n",
    "  \n",
    "  \n",
    "  \n",
    "\n",
    "\n",
    "      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, criterion, optimizer,mask,p, num_epochs=25):\n",
    "  # the mask has to be specified \n",
    "\n",
    "      \n",
    "      since = time.time()\n",
    "\n",
    "      best_model_wts = copy.deepcopy(model.state_dict())\n",
    "      best_acc = 0.0\n",
    "      losses=[]\n",
    "      accuracies=[]\n",
    "\n",
    "      for epoch in range(num_epochs):\n",
    "          print('Epoch {}/{}'.format(epoch, num_epochs - 1))\n",
    "          print('-' * 10)\n",
    "\n",
    "          # Each epoch has a training and validation phase\n",
    "          for phase in ['train', 'val']:\n",
    "              if phase == 'train':\n",
    "                  \n",
    "                  model.train()  # Set model to training mode\n",
    "              else:\n",
    "\n",
    "                  model.eval()   # Set model to evaluate mode\n",
    "\n",
    "              running_loss = 0.0\n",
    "              running_corrects = 0\n",
    "\n",
    "              # Iterate over data.\n",
    "              for inputs, labels in dataloader[phase]:\n",
    "                inputs=inputs.to(device)\n",
    "                labels=labels.to(device)\n",
    "                # zero the parameter gradients\n",
    "                optimizer.zero_grad()\n",
    "\n",
    "                # forward\n",
    "                # track history if only in train\n",
    "                with torch.set_grad_enabled(phase == 'train'):\n",
    "                    outputs= model.Forward(inputs,mask,p)\n",
    "\n",
    "                    _, preds = torch.max(outputs, 1)\n",
    "                    loss = criterion(outputs, labels)# backward + optimize only if in training phase\n",
    "                    if phase == 'train':\n",
    "                        loss.backward()\n",
    "                        optimizer.step()\n",
    "                        #print (model.linear1.weight.grad)\n",
    "                        #print (model.linear2.weight.grad)\n",
    "\n",
    "                  # statistics\n",
    "                running_loss += loss.item() * inputs.size(0)\n",
    "                running_corrects += torch.sum(preds == labels.data)\n",
    "                \n",
    "                  \n",
    "\n",
    "              epoch_loss = running_loss / dataset_sizes[phase]\n",
    "              epoch_acc = running_corrects.double() / dataset_sizes[phase]\n",
    "            \n",
    "\n",
    "              print('{} Loss: {:.4f} Acc: {:.4f} '.format(\n",
    "                  phase, epoch_loss, epoch_acc))\n",
    "\n",
    "              # deep copy the model\n",
    "              if phase == 'val' and epoch_acc > best_acc:\n",
    "                  best_acc = epoch_acc\n",
    "                  best_model_wts = copy.deepcopy(model.state_dict())\n",
    "              if phase=='train':\n",
    "                losses.append(epoch_loss)\n",
    "                accuracies.append(epoch_acc)\n",
    "\n",
    "\n",
    "          \n",
    "\n",
    "      time_elapsed = time.time() - since\n",
    "      print('Training complete in {:.0f}m {:.0f}s'.format(\n",
    "          time_elapsed // 60, time_elapsed % 60))\n",
    "      print('Best val Acc: {:4f}'.format(best_acc))\n",
    "\n",
    "      # load best model weights\n",
    "      model.load_state_dict(best_model_wts)\n",
    "      return model,losses,accuracies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class Population:\n",
    "    \n",
    "    def __init__(self,m,num,maskLength):\n",
    "        # constructor for initialising the population list\n",
    "        #list of DNA objects\n",
    "        self.population=[]\n",
    "        #muation rate for mutation\n",
    "        self.mutation_rate=m\n",
    "        #maximum number of entities in the population\n",
    "        self.popmax=num\n",
    "\n",
    "        self.maskLength=maskLength\n",
    "        for i in range (num):\n",
    "            #creating a dna object\n",
    "            #an initial random population created \n",
    "            dna =DNA(self.maskLength)\n",
    "            self.population.append (dna)\n",
    "      \n",
    "        self.matingPool=[]\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    def calcFitness (self,model):\n",
    "        # going through all the entities of population \n",
    "        #finding fitness of all population entities \n",
    "        for i in range(0,self.popmax):\n",
    "            self.population[i].fitness (model)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    def naturalSelection(self):\n",
    "        self.matingPool=[]\n",
    "        maxFitness=0\n",
    "        for i in range (self.popmax):\n",
    "            # moving throught the entire population \n",
    "            if (self.population[i].fit>maxFitness):\n",
    "                maxFitness=self.population[i].fit\n",
    "       \n",
    "        # max Fitness has the maximum loss score of the entire population  \n",
    "        for i in range (self.popmax ):\n",
    "        # iterating through the all inviduals of the population\n",
    "            n=self.Mymap(self.population[i].fit,0,maxFitness,0,1)\n",
    "        \n",
    "            n=math.floor(n*100)\n",
    "            \n",
    "            for j in range (n):\n",
    "                #creating mating pool\n",
    "                self.matingPool.append (self.population[i])\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    def Mymap(self,num,prevlow,prevhigh,nextlow,nexthigh):\n",
    "        \n",
    "        prevrange =float((num-prevlow)/(prevhigh-prevlow))\n",
    "        return nextlow+(nexthigh-nextlow)*prevrange\n",
    "\n",
    "\n",
    "\n",
    "    def   generate (self):\n",
    "        for i in range (self.popmax ):\n",
    "            index_1=math.floor(random.randint  (0,len(self.matingPool)-1))\n",
    "            index_2=math.floor (random.randint (0,len(self.matingPool)-1))\n",
    "            parent1=self.matingPool[index_1]\n",
    "            parent2=self.matingPool[index_2]\n",
    "            child=parent1.crossover(parent2)\n",
    "            child.mutate(self.mutation_rate)\n",
    "            self.population[i]=child \n",
    "\n",
    "\n",
    "    def fittest(self):\n",
    "        #returns the fiitest individual mask of the population \n",
    "        #also returns the keeping probability of the fittest mask \n",
    "        fittest=self.population[0]\n",
    "        for i  in range (self.popmax):\n",
    "            if (fittest.fit<self.population[i].fit):\n",
    "                fittest=self.population[i]\n",
    "        return fittest,fittest.keep_prob()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "class DNA:\n",
    "    \n",
    "    \n",
    "    def __init__(self,maskLength):\n",
    "        #constructor for the creation of the mask as a gene object \n",
    "        self.maskLength=maskLength\n",
    "        #creation of mask \n",
    "        self.gene=torch.bernoulli(torch.empty(1,maskLength).uniform_(0,1))\n",
    "        self.fit=0\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    def keep_prob (self):\n",
    "\n",
    "        num_one =0\n",
    "        for i in range (self.maskLength):\n",
    "            if (self.gene[0,i]==1):\n",
    "                num_one=num_one+1\n",
    "        return float(num_one/self.maskLength)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    def fitness(self,model):\n",
    "        # finding the fitness of a particular mask\n",
    "        #accuracy of all training set is the fitness in one epoch\n",
    "        #putting model in train mode \n",
    "        running_loss=0\n",
    "        running_corrects=0\n",
    "        model.train()\n",
    "        \n",
    "        for inputs,labels in dataloader ['train']:\n",
    "            inputs=inputs.to(device)\n",
    "            labels=labels.to(device )\n",
    "            outputs=model.Forward(inputs,self.gene,self.keep_prob())\n",
    "            _,preds=torch.max(outputs,1)\n",
    "            loss=criterion (outputs,labels)\n",
    "            running_loss += loss.item() * inputs.size(0)\n",
    "            running_corrects += torch.sum(preds == labels.data)\n",
    "        epoch_loss = running_loss / dataset_sizes['train']\n",
    "        epoch_acc = running_corrects.double() / dataset_sizes['train']\n",
    "        \n",
    "        self.fit=epoch_acc\n",
    "        return epoch_acc\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    def crossover (self,parent2):\n",
    "        #one parent is the passed in the argument \n",
    "        #another parent is the one from which this function is called \n",
    "        #another parent is self.gene\n",
    "        child =DNA(self.maskLength)\n",
    "        midpoint =random .randint (0,self.maskLength-1)\n",
    "        for i in range (0,self.maskLength):\n",
    "            if (i>midpoint):\n",
    "                child.gene [0,i]=self.gene[0,i]\n",
    "            else :\n",
    "                child.gene [0,i]=parent2.gene[0,i]\n",
    "        \n",
    "        return child \n",
    "\n",
    "    def mutate(self,mutation_rate):\n",
    "        #randomly activate some of the nodes  \n",
    "        #mutate some of the genes \n",
    "        for i in range (self.maskLength):\n",
    "            if (random.randint (0,99)<=mutation_rate*100):\n",
    "                self.gene[0,i]=1\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "mutation_rate =0.20\n",
    "max_population=30\n",
    "maskLength=1000\n",
    "#seeded so that each time same initial weights generated \n",
    "torch.manual_seed(6)\n",
    "model=Model()#creating the object of the class\n",
    "model.to(device)\n",
    "criterion=nn.CrossEntropyLoss()\n",
    "optimizer=torch.optim.SGD(model.parameters(),lr=0.001,momentum=0.9)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch generations ( 0 /200) :"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\ana\\envs\\packt_torch\\lib\\site-packages\\ipykernel_launcher.py:12: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  if sys.path[0] == '':\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy(fittest mask) tensor(0.1567, dtype=torch.float64) keep_prob 0.494\n",
      "Epoch 0/29\n",
      "----------\n",
      "train Loss: 1.9450 Acc: 0.1642 \n",
      "val Loss: 1.9481 Acc: 0.1444 \n",
      "Epoch 1/29\n",
      "----------\n",
      "train Loss: 1.9441 Acc: 0.1791 \n",
      "val Loss: 1.9479 Acc: 0.1444 \n",
      "Epoch 2/29\n",
      "----------\n",
      "train Loss: 1.9428 Acc: 0.2090 \n",
      "val Loss: 1.9476 Acc: 0.1444 \n",
      "Epoch 3/29\n",
      "----------\n",
      "train Loss: 1.9412 Acc: 0.2612 \n",
      "val Loss: 1.9474 Acc: 0.1444 \n",
      "Epoch 4/29\n",
      "----------\n",
      "train Loss: 1.9397 Acc: 0.2761 \n",
      "val Loss: 1.9472 Acc: 0.1556 \n",
      "Epoch 5/29\n",
      "----------\n",
      "train Loss: 1.9380 Acc: 0.3284 \n",
      "val Loss: 1.9469 Acc: 0.1556 \n",
      "Epoch 6/29\n",
      "----------\n",
      "train Loss: 1.9363 Acc: 0.3582 \n",
      "val Loss: 1.9467 Acc: 0.1556 \n",
      "Epoch 7/29\n",
      "----------\n",
      "train Loss: 1.9346 Acc: 0.3955 \n",
      "val Loss: 1.9464 Acc: 0.1778 \n",
      "Epoch 8/29\n",
      "----------\n",
      "train Loss: 1.9329 Acc: 0.4328 \n",
      "val Loss: 1.9461 Acc: 0.1778 \n",
      "Epoch 9/29\n",
      "----------\n",
      "train Loss: 1.9311 Acc: 0.4851 \n",
      "val Loss: 1.9458 Acc: 0.1778 \n",
      "Epoch 10/29\n",
      "----------\n",
      "train Loss: 1.9293 Acc: 0.4925 \n",
      "val Loss: 1.9455 Acc: 0.1889 \n",
      "Epoch 11/29\n",
      "----------\n",
      "train Loss: 1.9274 Acc: 0.5000 \n",
      "val Loss: 1.9453 Acc: 0.2000 \n",
      "Epoch 12/29\n",
      "----------\n",
      "train Loss: 1.9255 Acc: 0.5000 \n",
      "val Loss: 1.9449 Acc: 0.2111 \n",
      "Epoch 13/29\n",
      "----------\n",
      "train Loss: 1.9236 Acc: 0.5224 \n",
      "val Loss: 1.9446 Acc: 0.2222 \n",
      "Epoch 14/29\n",
      "----------\n",
      "train Loss: 1.9215 Acc: 0.5299 \n",
      "val Loss: 1.9443 Acc: 0.2222 \n",
      "Epoch 15/29\n",
      "----------\n",
      "train Loss: 1.9194 Acc: 0.5448 \n",
      "val Loss: 1.9440 Acc: 0.2444 \n",
      "Epoch 16/29\n",
      "----------\n",
      "train Loss: 1.9173 Acc: 0.5373 \n",
      "val Loss: 1.9436 Acc: 0.2444 \n",
      "Epoch 17/29\n",
      "----------\n",
      "train Loss: 1.9150 Acc: 0.5448 \n",
      "val Loss: 1.9432 Acc: 0.2556 \n",
      "Epoch 18/29\n",
      "----------\n",
      "train Loss: 1.9127 Acc: 0.5746 \n",
      "val Loss: 1.9429 Acc: 0.2556 \n",
      "Epoch 19/29\n",
      "----------\n",
      "train Loss: 1.9103 Acc: 0.5970 \n",
      "val Loss: 1.9424 Acc: 0.2667 \n",
      "Epoch 20/29\n",
      "----------\n",
      "train Loss: 1.9078 Acc: 0.6045 \n",
      "val Loss: 1.9420 Acc: 0.2667 \n",
      "Epoch 21/29\n",
      "----------\n",
      "train Loss: 1.9052 Acc: 0.6045 \n",
      "val Loss: 1.9416 Acc: 0.2667 \n",
      "Epoch 22/29\n",
      "----------\n",
      "train Loss: 1.9024 Acc: 0.6119 \n",
      "val Loss: 1.9411 Acc: 0.3000 \n",
      "Epoch 23/29\n",
      "----------\n",
      "train Loss: 1.8996 Acc: 0.6119 \n",
      "val Loss: 1.9406 Acc: 0.3000 \n",
      "Epoch 24/29\n",
      "----------\n",
      "train Loss: 1.8966 Acc: 0.6194 \n",
      "val Loss: 1.9401 Acc: 0.3111 \n",
      "Epoch 25/29\n",
      "----------\n",
      "train Loss: 1.8935 Acc: 0.6269 \n",
      "val Loss: 1.9395 Acc: 0.3222 \n",
      "Epoch 26/29\n",
      "----------\n",
      "train Loss: 1.8902 Acc: 0.6269 \n",
      "val Loss: 1.9389 Acc: 0.3222 \n",
      "Epoch 27/29\n",
      "----------\n",
      "train Loss: 1.8867 Acc: 0.6269 \n",
      "val Loss: 1.9383 Acc: 0.3111 \n",
      "Epoch 28/29\n",
      "----------\n",
      "train Loss: 1.8831 Acc: 0.6269 \n",
      "val Loss: 1.9377 Acc: 0.3111 \n",
      "Epoch 29/29\n",
      "----------\n",
      "train Loss: 1.8793 Acc: 0.6269 \n",
      "val Loss: 1.9370 Acc: 0.3111 \n",
      "Training complete in 0m 5s\n",
      "Best val Acc: 0.322222\n",
      "Epoch generations ( 1 /200) :accuracy(fittest mask) tensor(0.5970, dtype=torch.float64) keep_prob 0.612\n",
      "Epoch generations ( 2 /200) :accuracy(fittest mask) tensor(0.5672, dtype=torch.float64) keep_prob 0.69\n",
      "Epoch generations ( 3 /200) :accuracy(fittest mask) tensor(0.5821, dtype=torch.float64) keep_prob 0.767\n",
      "Epoch generations ( 4 /200) :accuracy(fittest mask) tensor(0.5522, dtype=torch.float64) keep_prob 0.813\n",
      "Epoch generations ( 5 /200) :accuracy(fittest mask) tensor(0.5970, dtype=torch.float64) keep_prob 0.845\n",
      "Epoch generations ( 6 /200) :accuracy(fittest mask) tensor(0.5672, dtype=torch.float64) keep_prob 0.883\n",
      "Epoch generations ( 7 /200) :accuracy(fittest mask) tensor(0.5672, dtype=torch.float64) keep_prob 0.916\n",
      "Epoch generations ( 8 /200) :accuracy(fittest mask) tensor(0.5746, dtype=torch.float64) keep_prob 0.931\n",
      "Epoch generations ( 9 /200) :accuracy(fittest mask) tensor(0.5672, dtype=torch.float64) keep_prob 0.947\n",
      "Epoch generations ( 10 /200) :accuracy(fittest mask) tensor(0.5522, dtype=torch.float64) keep_prob 0.945\n",
      "Epoch 0/29\n",
      "----------\n",
      "train Loss: 1.9192 Acc: 0.5597 \n",
      "val Loss: 1.9390 Acc: 0.3222 \n",
      "Epoch 1/29\n",
      "----------\n",
      "train Loss: 1.9178 Acc: 0.5448 \n",
      "val Loss: 1.9385 Acc: 0.3222 \n",
      "Epoch 2/29\n",
      "----------\n",
      "train Loss: 1.9164 Acc: 0.5672 \n",
      "val Loss: 1.9381 Acc: 0.3222 \n",
      "Epoch 3/29\n",
      "----------\n",
      "train Loss: 1.9150 Acc: 0.5672 \n",
      "val Loss: 1.9377 Acc: 0.3222 \n",
      "Epoch 4/29\n",
      "----------\n",
      "train Loss: 1.9137 Acc: 0.5746 \n",
      "val Loss: 1.9372 Acc: 0.3222 \n",
      "Epoch 5/29\n",
      "----------\n",
      "train Loss: 1.9123 Acc: 0.5746 \n",
      "val Loss: 1.9368 Acc: 0.3222 \n",
      "Epoch 6/29\n",
      "----------\n",
      "train Loss: 1.9108 Acc: 0.5746 \n",
      "val Loss: 1.9364 Acc: 0.3222 \n",
      "Epoch 7/29\n",
      "----------\n",
      "train Loss: 1.9094 Acc: 0.5746 \n",
      "val Loss: 1.9359 Acc: 0.3222 \n",
      "Epoch 8/29\n",
      "----------\n",
      "train Loss: 1.9079 Acc: 0.5746 \n",
      "val Loss: 1.9354 Acc: 0.3333 \n",
      "Epoch 9/29\n",
      "----------\n",
      "train Loss: 1.9064 Acc: 0.5746 \n",
      "val Loss: 1.9349 Acc: 0.3333 \n",
      "Epoch 10/29\n",
      "----------\n",
      "train Loss: 1.9048 Acc: 0.5896 \n",
      "val Loss: 1.9344 Acc: 0.3444 \n",
      "Epoch 11/29\n",
      "----------\n",
      "train Loss: 1.9032 Acc: 0.5896 \n",
      "val Loss: 1.9339 Acc: 0.3444 \n",
      "Epoch 12/29\n",
      "----------\n",
      "train Loss: 1.9016 Acc: 0.5821 \n",
      "val Loss: 1.9334 Acc: 0.3444 \n",
      "Epoch 13/29\n",
      "----------\n",
      "train Loss: 1.8999 Acc: 0.5821 \n",
      "val Loss: 1.9328 Acc: 0.3444 \n",
      "Epoch 14/29\n",
      "----------\n",
      "train Loss: 1.8982 Acc: 0.5821 \n",
      "val Loss: 1.9323 Acc: 0.3444 \n",
      "Epoch 15/29\n",
      "----------\n",
      "train Loss: 1.8965 Acc: 0.5821 \n",
      "val Loss: 1.9317 Acc: 0.3444 \n",
      "Epoch 16/29\n",
      "----------\n",
      "train Loss: 1.8947 Acc: 0.5821 \n",
      "val Loss: 1.9311 Acc: 0.3444 \n",
      "Epoch 17/29\n",
      "----------\n",
      "train Loss: 1.8929 Acc: 0.5821 \n",
      "val Loss: 1.9304 Acc: 0.3333 \n",
      "Epoch 18/29\n",
      "----------\n",
      "train Loss: 1.8910 Acc: 0.5746 \n",
      "val Loss: 1.9298 Acc: 0.3333 \n",
      "Epoch 19/29\n",
      "----------\n",
      "train Loss: 1.8891 Acc: 0.5746 \n",
      "val Loss: 1.9291 Acc: 0.3333 \n",
      "Epoch 20/29\n",
      "----------\n",
      "train Loss: 1.8871 Acc: 0.5746 \n",
      "val Loss: 1.9284 Acc: 0.3333 \n",
      "Epoch 21/29\n",
      "----------\n",
      "train Loss: 1.8851 Acc: 0.5746 \n",
      "val Loss: 1.9277 Acc: 0.3333 \n",
      "Epoch 22/29\n",
      "----------\n",
      "train Loss: 1.8830 Acc: 0.5821 \n",
      "val Loss: 1.9270 Acc: 0.3333 \n",
      "Epoch 23/29\n",
      "----------\n",
      "train Loss: 1.8808 Acc: 0.5821 \n",
      "val Loss: 1.9262 Acc: 0.3333 \n",
      "Epoch 24/29\n",
      "----------\n",
      "train Loss: 1.8786 Acc: 0.5821 \n",
      "val Loss: 1.9254 Acc: 0.3333 \n",
      "Epoch 25/29\n",
      "----------\n",
      "train Loss: 1.8764 Acc: 0.5821 \n",
      "val Loss: 1.9246 Acc: 0.3333 \n",
      "Epoch 26/29\n",
      "----------\n",
      "train Loss: 1.8741 Acc: 0.5821 \n",
      "val Loss: 1.9237 Acc: 0.3333 \n",
      "Epoch 27/29\n",
      "----------\n",
      "train Loss: 1.8717 Acc: 0.5821 \n",
      "val Loss: 1.9228 Acc: 0.3333 \n",
      "Epoch 28/29\n",
      "----------\n",
      "train Loss: 1.8692 Acc: 0.5821 \n",
      "val Loss: 1.9219 Acc: 0.3333 \n",
      "Epoch 29/29\n",
      "----------\n",
      "train Loss: 1.8667 Acc: 0.5896 \n",
      "val Loss: 1.9209 Acc: 0.3333 \n",
      "Training complete in 0m 5s\n",
      "Best val Acc: 0.344444\n",
      "Epoch generations ( 11 /200) :accuracy(fittest mask) tensor(0.5970, dtype=torch.float64) keep_prob 0.962\n",
      "Epoch generations ( 12 /200) :accuracy(fittest mask) tensor(0.5896, dtype=torch.float64) keep_prob 0.968\n",
      "Epoch generations ( 13 /200) :accuracy(fittest mask) tensor(0.5896, dtype=torch.float64) keep_prob 0.97\n",
      "Epoch generations ( 14 /200) :accuracy(fittest mask) tensor(0.5821, dtype=torch.float64) keep_prob 0.979\n",
      "Epoch generations ( 15 /200) :accuracy(fittest mask) tensor(0.5970, dtype=torch.float64) keep_prob 0.983\n",
      "Epoch generations ( 16 /200) :accuracy(fittest mask) tensor(0.5821, dtype=torch.float64) keep_prob 0.987\n",
      "Epoch generations ( 17 /200) :accuracy(fittest mask) tensor(0.5896, dtype=torch.float64) keep_prob 0.992\n",
      "Epoch generations ( 18 /200) :accuracy(fittest mask) tensor(0.5896, dtype=torch.float64) keep_prob 0.994\n",
      "Epoch generations ( 19 /200) :accuracy(fittest mask) tensor(0.5821, dtype=torch.float64) keep_prob 0.988\n",
      "Epoch generations ( 20 /200) :accuracy(fittest mask) tensor(0.5821, dtype=torch.float64) keep_prob 0.991\n",
      "Epoch 0/29\n",
      "----------\n",
      "train Loss: 1.9034 Acc: 0.5821 \n",
      "val Loss: 1.9338 Acc: 0.3556 \n",
      "Epoch 1/29\n",
      "----------\n",
      "train Loss: 1.9017 Acc: 0.5821 \n",
      "val Loss: 1.9332 Acc: 0.3556 \n",
      "Epoch 2/29\n",
      "----------\n",
      "train Loss: 1.9001 Acc: 0.5821 \n",
      "val Loss: 1.9326 Acc: 0.3556 \n",
      "Epoch 3/29\n",
      "----------\n",
      "train Loss: 1.8984 Acc: 0.5746 \n",
      "val Loss: 1.9320 Acc: 0.3556 \n",
      "Epoch 4/29\n",
      "----------\n",
      "train Loss: 1.8967 Acc: 0.5746 \n",
      "val Loss: 1.9315 Acc: 0.3556 \n",
      "Epoch 5/29\n",
      "----------\n",
      "train Loss: 1.8950 Acc: 0.5746 \n",
      "val Loss: 1.9309 Acc: 0.3556 \n",
      "Epoch 6/29\n",
      "----------\n",
      "train Loss: 1.8932 Acc: 0.5672 \n",
      "val Loss: 1.9302 Acc: 0.3333 \n",
      "Epoch 7/29\n",
      "----------\n",
      "train Loss: 1.8914 Acc: 0.5672 \n",
      "val Loss: 1.9296 Acc: 0.3333 \n",
      "Epoch 8/29\n",
      "----------\n",
      "train Loss: 1.8896 Acc: 0.5672 \n",
      "val Loss: 1.9290 Acc: 0.3333 \n",
      "Epoch 9/29\n",
      "----------\n",
      "train Loss: 1.8877 Acc: 0.5672 \n",
      "val Loss: 1.9283 Acc: 0.3333 \n",
      "Epoch 10/29\n",
      "----------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train Loss: 1.8857 Acc: 0.5746 \n",
      "val Loss: 1.9276 Acc: 0.3333 \n",
      "Epoch 11/29\n",
      "----------\n",
      "train Loss: 1.8837 Acc: 0.5746 \n",
      "val Loss: 1.9268 Acc: 0.3444 \n",
      "Epoch 12/29\n",
      "----------\n",
      "train Loss: 1.8817 Acc: 0.5746 \n",
      "val Loss: 1.9261 Acc: 0.3444 \n",
      "Epoch 13/29\n",
      "----------\n",
      "train Loss: 1.8796 Acc: 0.5746 \n",
      "val Loss: 1.9253 Acc: 0.3444 \n",
      "Epoch 14/29\n",
      "----------\n",
      "train Loss: 1.8774 Acc: 0.5746 \n",
      "val Loss: 1.9245 Acc: 0.3444 \n",
      "Epoch 15/29\n",
      "----------\n",
      "train Loss: 1.8752 Acc: 0.5746 \n",
      "val Loss: 1.9237 Acc: 0.3444 \n",
      "Epoch 16/29\n",
      "----------\n",
      "train Loss: 1.8730 Acc: 0.5746 \n",
      "val Loss: 1.9228 Acc: 0.3444 \n",
      "Epoch 17/29\n",
      "----------\n",
      "train Loss: 1.8707 Acc: 0.5746 \n",
      "val Loss: 1.9219 Acc: 0.3444 \n",
      "Epoch 18/29\n",
      "----------\n",
      "train Loss: 1.8683 Acc: 0.5746 \n",
      "val Loss: 1.9209 Acc: 0.3444 \n",
      "Epoch 19/29\n",
      "----------\n",
      "train Loss: 1.8658 Acc: 0.5746 \n",
      "val Loss: 1.9200 Acc: 0.3444 \n",
      "Epoch 20/29\n",
      "----------\n",
      "train Loss: 1.8633 Acc: 0.5821 \n",
      "val Loss: 1.9189 Acc: 0.3444 \n",
      "Epoch 21/29\n",
      "----------\n",
      "train Loss: 1.8608 Acc: 0.5821 \n",
      "val Loss: 1.9179 Acc: 0.3444 \n",
      "Epoch 22/29\n",
      "----------\n",
      "train Loss: 1.8581 Acc: 0.5896 \n",
      "val Loss: 1.9168 Acc: 0.3556 \n",
      "Epoch 23/29\n",
      "----------\n",
      "train Loss: 1.8554 Acc: 0.5896 \n",
      "val Loss: 1.9157 Acc: 0.3667 \n",
      "Epoch 24/29\n",
      "----------\n",
      "train Loss: 1.8526 Acc: 0.5896 \n",
      "val Loss: 1.9145 Acc: 0.3667 \n",
      "Epoch 25/29\n",
      "----------\n",
      "train Loss: 1.8498 Acc: 0.5896 \n",
      "val Loss: 1.9133 Acc: 0.3667 \n",
      "Epoch 26/29\n",
      "----------\n",
      "train Loss: 1.8468 Acc: 0.5896 \n",
      "val Loss: 1.9120 Acc: 0.3667 \n",
      "Epoch 27/29\n",
      "----------\n",
      "train Loss: 1.8438 Acc: 0.5896 \n",
      "val Loss: 1.9107 Acc: 0.3778 \n",
      "Epoch 28/29\n",
      "----------\n",
      "train Loss: 1.8407 Acc: 0.5970 \n",
      "val Loss: 1.9094 Acc: 0.3889 \n",
      "Epoch 29/29\n",
      "----------\n",
      "train Loss: 1.8375 Acc: 0.5970 \n",
      "val Loss: 1.9079 Acc: 0.3889 \n",
      "Training complete in 0m 7s\n",
      "Best val Acc: 0.388889\n",
      "Epoch generations ( 21 /200) :accuracy(fittest mask) tensor(0.6045, dtype=torch.float64) keep_prob 0.993\n",
      "Epoch generations ( 22 /200) :accuracy(fittest mask) tensor(0.6045, dtype=torch.float64) keep_prob 0.994\n",
      "Epoch generations ( 23 /200) :accuracy(fittest mask) tensor(0.5970, dtype=torch.float64) keep_prob 0.996\n",
      "Epoch generations ( 24 /200) :accuracy(fittest mask) tensor(0.5896, dtype=torch.float64) keep_prob 0.995\n",
      "Epoch generations ( 25 /200) :accuracy(fittest mask) tensor(0.5896, dtype=torch.float64) keep_prob 0.997\n",
      "Epoch generations ( 26 /200) :accuracy(fittest mask) tensor(0.5896, dtype=torch.float64) keep_prob 0.997\n",
      "Epoch generations ( 27 /200) :accuracy(fittest mask) tensor(0.5896, dtype=torch.float64) keep_prob 0.996\n",
      "Epoch generations ( 28 /200) :accuracy(fittest mask) tensor(0.5896, dtype=torch.float64) keep_prob 0.998\n",
      "Epoch generations ( 29 /200) :accuracy(fittest mask) tensor(0.5896, dtype=torch.float64) keep_prob 0.998\n",
      "Epoch generations ( 30 /200) :accuracy(fittest mask) tensor(0.5896, dtype=torch.float64) keep_prob 0.998\n",
      "Epoch 0/29\n",
      "----------\n",
      "train Loss: 1.8388 Acc: 0.5970 \n",
      "val Loss: 1.9079 Acc: 0.3889 \n",
      "Epoch 1/29\n",
      "----------\n",
      "train Loss: 1.8356 Acc: 0.5970 \n",
      "val Loss: 1.9065 Acc: 0.3889 \n",
      "Epoch 2/29\n",
      "----------\n",
      "train Loss: 1.8322 Acc: 0.5970 \n",
      "val Loss: 1.9050 Acc: 0.3889 \n",
      "Epoch 3/29\n",
      "----------\n",
      "train Loss: 1.8288 Acc: 0.6045 \n",
      "val Loss: 1.9035 Acc: 0.4000 \n",
      "Epoch 4/29\n",
      "----------\n",
      "train Loss: 1.8253 Acc: 0.6119 \n",
      "val Loss: 1.9019 Acc: 0.4000 \n",
      "Epoch 5/29\n",
      "----------\n",
      "train Loss: 1.8217 Acc: 0.6194 \n",
      "val Loss: 1.9002 Acc: 0.4000 \n",
      "Epoch 6/29\n",
      "----------\n",
      "train Loss: 1.8180 Acc: 0.6343 \n",
      "val Loss: 1.8985 Acc: 0.4000 \n",
      "Epoch 7/29\n",
      "----------\n",
      "train Loss: 1.8142 Acc: 0.6343 \n",
      "val Loss: 1.8967 Acc: 0.4000 \n",
      "Epoch 8/29\n",
      "----------\n",
      "train Loss: 1.8103 Acc: 0.6343 \n",
      "val Loss: 1.8949 Acc: 0.4000 \n",
      "Epoch 9/29\n",
      "----------\n",
      "train Loss: 1.8064 Acc: 0.6343 \n",
      "val Loss: 1.8930 Acc: 0.4222 \n",
      "Epoch 10/29\n",
      "----------\n",
      "train Loss: 1.8023 Acc: 0.6343 \n",
      "val Loss: 1.8911 Acc: 0.4222 \n",
      "Epoch 11/29\n",
      "----------\n",
      "train Loss: 1.7981 Acc: 0.6343 \n",
      "val Loss: 1.8891 Acc: 0.4222 \n",
      "Epoch 12/29\n",
      "----------\n",
      "train Loss: 1.7938 Acc: 0.6418 \n",
      "val Loss: 1.8870 Acc: 0.4222 \n",
      "Epoch 13/29\n",
      "----------\n",
      "train Loss: 1.7895 Acc: 0.6567 \n",
      "val Loss: 1.8849 Acc: 0.4222 \n",
      "Epoch 14/29\n",
      "----------\n",
      "train Loss: 1.7850 Acc: 0.6567 \n",
      "val Loss: 1.8827 Acc: 0.4333 \n",
      "Epoch 15/29\n",
      "----------\n",
      "train Loss: 1.7804 Acc: 0.6567 \n",
      "val Loss: 1.8805 Acc: 0.4333 \n",
      "Epoch 16/29\n",
      "----------\n",
      "train Loss: 1.7758 Acc: 0.6567 \n",
      "val Loss: 1.8782 Acc: 0.4222 \n",
      "Epoch 17/29\n",
      "----------\n",
      "train Loss: 1.7710 Acc: 0.6567 \n",
      "val Loss: 1.8759 Acc: 0.4333 \n",
      "Epoch 18/29\n",
      "----------\n",
      "train Loss: 1.7662 Acc: 0.6567 \n",
      "val Loss: 1.8735 Acc: 0.4333 \n",
      "Epoch 19/29\n",
      "----------\n",
      "train Loss: 1.7613 Acc: 0.6716 \n",
      "val Loss: 1.8711 Acc: 0.4333 \n",
      "Epoch 20/29\n",
      "----------\n",
      "train Loss: 1.7564 Acc: 0.6716 \n",
      "val Loss: 1.8686 Acc: 0.4444 \n",
      "Epoch 21/29\n",
      "----------\n",
      "train Loss: 1.7513 Acc: 0.6716 \n",
      "val Loss: 1.8661 Acc: 0.4444 \n",
      "Epoch 22/29\n",
      "----------\n",
      "train Loss: 1.7463 Acc: 0.6716 \n",
      "val Loss: 1.8635 Acc: 0.4444 \n",
      "Epoch 23/29\n",
      "----------\n",
      "train Loss: 1.7411 Acc: 0.6716 \n",
      "val Loss: 1.8609 Acc: 0.4444 \n",
      "Epoch 24/29\n",
      "----------\n",
      "train Loss: 1.7359 Acc: 0.6716 \n",
      "val Loss: 1.8583 Acc: 0.4444 \n",
      "Epoch 25/29\n",
      "----------\n",
      "train Loss: 1.7307 Acc: 0.6716 \n",
      "val Loss: 1.8557 Acc: 0.4444 \n",
      "Epoch 26/29\n",
      "----------\n",
      "train Loss: 1.7254 Acc: 0.6716 \n",
      "val Loss: 1.8530 Acc: 0.4444 \n",
      "Epoch 27/29\n",
      "----------\n",
      "train Loss: 1.7201 Acc: 0.6791 \n",
      "val Loss: 1.8503 Acc: 0.4444 \n",
      "Epoch 28/29\n",
      "----------\n",
      "train Loss: 1.7148 Acc: 0.6791 \n",
      "val Loss: 1.8476 Acc: 0.4444 \n",
      "Epoch 29/29\n",
      "----------\n",
      "train Loss: 1.7095 Acc: 0.6791 \n",
      "val Loss: 1.8449 Acc: 0.4444 \n",
      "Training complete in 0m 6s\n",
      "Best val Acc: 0.444444\n",
      "Epoch generations ( 31 /200) :accuracy(fittest mask) tensor(0.6716, dtype=torch.float64) keep_prob 0.996\n",
      "Epoch generations ( 32 /200) :accuracy(fittest mask) tensor(0.6716, dtype=torch.float64) keep_prob 0.998\n",
      "Epoch generations ( 33 /200) :accuracy(fittest mask) tensor(0.6716, dtype=torch.float64) keep_prob 0.999\n",
      "Epoch generations ( 34 /200) :accuracy(fittest mask) tensor(0.6716, dtype=torch.float64) keep_prob 0.999\n",
      "Epoch generations ( 35 /200) :accuracy(fittest mask) tensor(0.6716, dtype=torch.float64) keep_prob 0.998\n",
      "Epoch generations ( 36 /200) :accuracy(fittest mask) tensor(0.6716, dtype=torch.float64) keep_prob 0.999\n",
      "Epoch generations ( 37 /200) :accuracy(fittest mask) tensor(0.6716, dtype=torch.float64) keep_prob 0.999\n",
      "Epoch generations ( 38 /200) :accuracy(fittest mask) tensor(0.6716, dtype=torch.float64) keep_prob 0.999\n",
      "Epoch generations ( 39 /200) :accuracy(fittest mask) tensor(0.6716, dtype=torch.float64) keep_prob 0.999\n",
      "Epoch generations ( 40 /200) :accuracy(fittest mask) tensor(0.6716, dtype=torch.float64) keep_prob 0.999\n",
      "Epoch 0/29\n",
      "----------\n",
      "train Loss: 1.7513 Acc: 0.6716 \n",
      "val Loss: 1.8661 Acc: 0.4444 \n",
      "Epoch 1/29\n",
      "----------\n",
      "train Loss: 1.7462 Acc: 0.6716 \n",
      "val Loss: 1.8635 Acc: 0.4444 \n",
      "Epoch 2/29\n",
      "----------\n",
      "train Loss: 1.7410 Acc: 0.6716 \n",
      "val Loss: 1.8609 Acc: 0.4444 \n",
      "Epoch 3/29\n",
      "----------\n",
      "train Loss: 1.7358 Acc: 0.6716 \n",
      "val Loss: 1.8582 Acc: 0.4444 \n",
      "Epoch 4/29\n",
      "----------\n",
      "train Loss: 1.7305 Acc: 0.6716 \n",
      "val Loss: 1.8555 Acc: 0.4444 \n",
      "Epoch 5/29\n",
      "----------\n",
      "train Loss: 1.7253 Acc: 0.6716 \n",
      "val Loss: 1.8528 Acc: 0.4444 \n",
      "Epoch 6/29\n",
      "----------\n",
      "train Loss: 1.7200 Acc: 0.6791 \n",
      "val Loss: 1.8500 Acc: 0.4444 \n",
      "Epoch 7/29\n",
      "----------\n",
      "train Loss: 1.7147 Acc: 0.6791 \n",
      "val Loss: 1.8473 Acc: 0.4444 \n",
      "Epoch 8/29\n",
      "----------\n",
      "train Loss: 1.7093 Acc: 0.6791 \n",
      "val Loss: 1.8446 Acc: 0.4333 \n",
      "Epoch 9/29\n",
      "----------\n",
      "train Loss: 1.7040 Acc: 0.6866 \n",
      "val Loss: 1.8418 Acc: 0.4333 \n",
      "Epoch 10/29\n",
      "----------\n",
      "train Loss: 1.6987 Acc: 0.6866 \n",
      "val Loss: 1.8391 Acc: 0.4444 \n",
      "Epoch 11/29\n",
      "----------\n",
      "train Loss: 1.6934 Acc: 0.6866 \n",
      "val Loss: 1.8364 Acc: 0.4444 \n",
      "Epoch 12/29\n",
      "----------\n",
      "train Loss: 1.6881 Acc: 0.6866 \n",
      "val Loss: 1.8336 Acc: 0.4444 \n",
      "Epoch 13/29\n",
      "----------\n",
      "train Loss: 1.6828 Acc: 0.6866 \n",
      "val Loss: 1.8309 Acc: 0.4444 \n",
      "Epoch 14/29\n",
      "----------\n",
      "train Loss: 1.6775 Acc: 0.6866 \n",
      "val Loss: 1.8282 Acc: 0.4444 \n",
      "Epoch 15/29\n",
      "----------\n",
      "train Loss: 1.6723 Acc: 0.6866 \n",
      "val Loss: 1.8255 Acc: 0.4444 \n",
      "Epoch 16/29\n",
      "----------\n",
      "train Loss: 1.6671 Acc: 0.6866 \n",
      "val Loss: 1.8229 Acc: 0.4444 \n",
      "Epoch 17/29\n",
      "----------\n",
      "train Loss: 1.6619 Acc: 0.6866 \n",
      "val Loss: 1.8202 Acc: 0.4556 \n",
      "Epoch 18/29\n",
      "----------\n",
      "train Loss: 1.6567 Acc: 0.7015 \n",
      "val Loss: 1.8176 Acc: 0.4556 \n",
      "Epoch 19/29\n",
      "----------\n",
      "train Loss: 1.6516 Acc: 0.7015 \n",
      "val Loss: 1.8149 Acc: 0.4556 \n",
      "Epoch 20/29\n",
      "----------\n",
      "train Loss: 1.6466 Acc: 0.7090 \n",
      "val Loss: 1.8123 Acc: 0.4667 \n",
      "Epoch 21/29\n",
      "----------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train Loss: 1.6416 Acc: 0.7090 \n",
      "val Loss: 1.8097 Acc: 0.4667 \n",
      "Epoch 22/29\n",
      "----------\n",
      "train Loss: 1.6366 Acc: 0.7090 \n",
      "val Loss: 1.8072 Acc: 0.4667 \n",
      "Epoch 23/29\n",
      "----------\n",
      "train Loss: 1.6317 Acc: 0.7090 \n",
      "val Loss: 1.8046 Acc: 0.4667 \n",
      "Epoch 24/29\n",
      "----------\n",
      "train Loss: 1.6268 Acc: 0.7090 \n",
      "val Loss: 1.8021 Acc: 0.4667 \n",
      "Epoch 25/29\n",
      "----------\n",
      "train Loss: 1.6219 Acc: 0.7239 \n",
      "val Loss: 1.7996 Acc: 0.4667 \n",
      "Epoch 26/29\n",
      "----------\n",
      "train Loss: 1.6172 Acc: 0.7239 \n",
      "val Loss: 1.7971 Acc: 0.4667 \n",
      "Epoch 27/29\n",
      "----------\n",
      "train Loss: 1.6124 Acc: 0.7239 \n",
      "val Loss: 1.7946 Acc: 0.4667 \n",
      "Epoch 28/29\n",
      "----------\n",
      "train Loss: 1.6078 Acc: 0.7239 \n",
      "val Loss: 1.7921 Acc: 0.4667 \n",
      "Epoch 29/29\n",
      "----------\n",
      "train Loss: 1.6032 Acc: 0.7239 \n",
      "val Loss: 1.7897 Acc: 0.4667 \n",
      "Training complete in 0m 7s\n",
      "Best val Acc: 0.466667\n",
      "Epoch generations ( 41 /200) :accuracy(fittest mask) tensor(0.7090, dtype=torch.float64) keep_prob 1.0\n",
      "Epoch generations ( 42 /200) :accuracy(fittest mask) tensor(0.7090, dtype=torch.float64) keep_prob 1.0\n",
      "Epoch generations ( 43 /200) :accuracy(fittest mask) tensor(0.7090, dtype=torch.float64) keep_prob 1.0\n",
      "Epoch generations ( 44 /200) :accuracy(fittest mask) tensor(0.7090, dtype=torch.float64) keep_prob 1.0\n",
      "Epoch generations ( 45 /200) :accuracy(fittest mask) tensor(0.7090, dtype=torch.float64) keep_prob 1.0\n",
      "Epoch generations ( 46 /200) :accuracy(fittest mask) tensor(0.7090, dtype=torch.float64) keep_prob 1.0\n",
      "Epoch generations ( 47 /200) :accuracy(fittest mask) tensor(0.7090, dtype=torch.float64) keep_prob 1.0\n",
      "Epoch generations ( 48 /200) :accuracy(fittest mask) tensor(0.7090, dtype=torch.float64) keep_prob 1.0\n",
      "Epoch generations ( 49 /200) :accuracy(fittest mask) tensor(0.7090, dtype=torch.float64) keep_prob 1.0\n",
      "Epoch generations ( 50 /200) :accuracy(fittest mask) tensor(0.7090, dtype=torch.float64) keep_prob 1.0\n",
      "Epoch 0/29\n",
      "----------\n",
      "train Loss: 1.6417 Acc: 0.7090 \n",
      "val Loss: 1.8098 Acc: 0.4667 \n",
      "Epoch 1/29\n",
      "----------\n",
      "train Loss: 1.6368 Acc: 0.7090 \n",
      "val Loss: 1.8072 Acc: 0.4667 \n",
      "Epoch 2/29\n",
      "----------\n",
      "train Loss: 1.6319 Acc: 0.7090 \n",
      "val Loss: 1.8046 Acc: 0.4667 \n",
      "Epoch 3/29\n",
      "----------\n",
      "train Loss: 1.6270 Acc: 0.7090 \n",
      "val Loss: 1.8021 Acc: 0.4667 \n",
      "Epoch 4/29\n",
      "----------\n",
      "train Loss: 1.6222 Acc: 0.7239 \n",
      "val Loss: 1.7996 Acc: 0.4667 \n",
      "Epoch 5/29\n",
      "----------\n",
      "train Loss: 1.6174 Acc: 0.7239 \n",
      "val Loss: 1.7971 Acc: 0.4667 \n",
      "Epoch 6/29\n",
      "----------\n",
      "train Loss: 1.6126 Acc: 0.7239 \n",
      "val Loss: 1.7946 Acc: 0.4667 \n",
      "Epoch 7/29\n",
      "----------\n",
      "train Loss: 1.6079 Acc: 0.7239 \n",
      "val Loss: 1.7921 Acc: 0.4667 \n",
      "Epoch 8/29\n",
      "----------\n",
      "train Loss: 1.6033 Acc: 0.7239 \n",
      "val Loss: 1.7897 Acc: 0.4667 \n",
      "Epoch 9/29\n",
      "----------\n",
      "train Loss: 1.5987 Acc: 0.7239 \n",
      "val Loss: 1.7872 Acc: 0.4667 \n",
      "Epoch 10/29\n",
      "----------\n",
      "train Loss: 1.5942 Acc: 0.7239 \n",
      "val Loss: 1.7848 Acc: 0.4667 \n",
      "Epoch 11/29\n",
      "----------\n",
      "train Loss: 1.5897 Acc: 0.7313 \n",
      "val Loss: 1.7824 Acc: 0.4667 \n",
      "Epoch 12/29\n",
      "----------\n",
      "train Loss: 1.5853 Acc: 0.7313 \n",
      "val Loss: 1.7800 Acc: 0.4667 \n",
      "Epoch 13/29\n",
      "----------\n",
      "train Loss: 1.5809 Acc: 0.7388 \n",
      "val Loss: 1.7775 Acc: 0.4667 \n",
      "Epoch 14/29\n",
      "----------\n",
      "train Loss: 1.5766 Acc: 0.7388 \n",
      "val Loss: 1.7751 Acc: 0.4667 \n",
      "Epoch 15/29\n",
      "----------\n",
      "train Loss: 1.5723 Acc: 0.7388 \n",
      "val Loss: 1.7727 Acc: 0.4667 \n",
      "Epoch 16/29\n",
      "----------\n",
      "train Loss: 1.5681 Acc: 0.7463 \n",
      "val Loss: 1.7703 Acc: 0.4667 \n",
      "Epoch 17/29\n",
      "----------\n",
      "train Loss: 1.5639 Acc: 0.7463 \n",
      "val Loss: 1.7679 Acc: 0.4667 \n",
      "Epoch 18/29\n",
      "----------\n",
      "train Loss: 1.5597 Acc: 0.7537 \n",
      "val Loss: 1.7655 Acc: 0.4778 \n",
      "Epoch 19/29\n",
      "----------\n",
      "train Loss: 1.5556 Acc: 0.7612 \n",
      "val Loss: 1.7631 Acc: 0.4778 \n",
      "Epoch 20/29\n",
      "----------\n",
      "train Loss: 1.5515 Acc: 0.7612 \n",
      "val Loss: 1.7606 Acc: 0.4778 \n",
      "Epoch 21/29\n",
      "----------\n",
      "train Loss: 1.5475 Acc: 0.7612 \n",
      "val Loss: 1.7582 Acc: 0.4778 \n",
      "Epoch 22/29\n",
      "----------\n",
      "train Loss: 1.5434 Acc: 0.7687 \n",
      "val Loss: 1.7558 Acc: 0.4778 \n",
      "Epoch 23/29\n",
      "----------\n",
      "train Loss: 1.5394 Acc: 0.7761 \n",
      "val Loss: 1.7533 Acc: 0.4889 \n",
      "Epoch 24/29\n",
      "----------\n",
      "train Loss: 1.5354 Acc: 0.7761 \n",
      "val Loss: 1.7509 Acc: 0.4889 \n",
      "Epoch 25/29\n",
      "----------\n",
      "train Loss: 1.5315 Acc: 0.7836 \n",
      "val Loss: 1.7484 Acc: 0.4889 \n",
      "Epoch 26/29\n",
      "----------\n",
      "train Loss: 1.5275 Acc: 0.7985 \n",
      "val Loss: 1.7460 Acc: 0.4889 \n",
      "Epoch 27/29\n",
      "----------\n",
      "train Loss: 1.5236 Acc: 0.7985 \n",
      "val Loss: 1.7435 Acc: 0.5000 \n",
      "Epoch 28/29\n",
      "----------\n",
      "train Loss: 1.5197 Acc: 0.8060 \n",
      "val Loss: 1.7410 Acc: 0.5000 \n",
      "Epoch 29/29\n",
      "----------\n",
      "train Loss: 1.5159 Acc: 0.8060 \n",
      "val Loss: 1.7385 Acc: 0.5000 \n",
      "Training complete in 0m 7s\n",
      "Best val Acc: 0.500000\n",
      "Epoch generations ( 51 /200) :accuracy(fittest mask) tensor(0.7985, dtype=torch.float64) keep_prob 1.0\n",
      "Epoch generations ( 52 /200) :accuracy(fittest mask) tensor(0.7985, dtype=torch.float64) keep_prob 1.0\n",
      "Epoch generations ( 53 /200) :accuracy(fittest mask) tensor(0.7985, dtype=torch.float64) keep_prob 1.0\n",
      "Epoch generations ( 54 /200) :accuracy(fittest mask) tensor(0.7985, dtype=torch.float64) keep_prob 1.0\n",
      "Epoch generations ( 55 /200) :accuracy(fittest mask) tensor(0.7985, dtype=torch.float64) keep_prob 1.0\n",
      "Epoch generations ( 56 /200) :accuracy(fittest mask) tensor(0.7985, dtype=torch.float64) keep_prob 1.0\n",
      "Epoch generations ( 57 /200) :accuracy(fittest mask) tensor(0.7985, dtype=torch.float64) keep_prob 1.0\n",
      "Epoch generations ( 58 /200) :accuracy(fittest mask) tensor(0.7985, dtype=torch.float64) keep_prob 1.0\n",
      "Epoch generations ( 59 /200) :accuracy(fittest mask) tensor(0.7985, dtype=torch.float64) keep_prob 1.0\n",
      "Epoch generations ( 60 /200) :accuracy(fittest mask) tensor(0.7985, dtype=torch.float64) keep_prob 1.0\n",
      "Epoch 0/29\n",
      "----------\n",
      "train Loss: 1.5197 Acc: 0.8060 \n",
      "val Loss: 1.7410 Acc: 0.5000 \n",
      "Epoch 1/29\n",
      "----------\n",
      "train Loss: 1.5159 Acc: 0.8060 \n",
      "val Loss: 1.7385 Acc: 0.5000 \n",
      "Epoch 2/29\n",
      "----------\n",
      "train Loss: 1.5120 Acc: 0.8060 \n",
      "val Loss: 1.7361 Acc: 0.5111 \n",
      "Epoch 3/29\n",
      "----------\n",
      "train Loss: 1.5082 Acc: 0.8060 \n",
      "val Loss: 1.7336 Acc: 0.5111 \n",
      "Epoch 4/29\n",
      "----------\n",
      "train Loss: 1.5044 Acc: 0.8134 \n",
      "val Loss: 1.7311 Acc: 0.5111 \n",
      "Epoch 5/29\n",
      "----------\n",
      "train Loss: 1.5007 Acc: 0.8134 \n",
      "val Loss: 1.7286 Acc: 0.5222 \n",
      "Epoch 6/29\n",
      "----------\n",
      "train Loss: 1.4969 Acc: 0.8209 \n",
      "val Loss: 1.7262 Acc: 0.5222 \n",
      "Epoch 7/29\n",
      "----------\n",
      "train Loss: 1.4932 Acc: 0.8209 \n",
      "val Loss: 1.7238 Acc: 0.5222 \n",
      "Epoch 8/29\n",
      "----------\n",
      "train Loss: 1.4896 Acc: 0.8209 \n",
      "val Loss: 1.7213 Acc: 0.5222 \n",
      "Epoch 9/29\n",
      "----------\n",
      "train Loss: 1.4859 Acc: 0.8284 \n",
      "val Loss: 1.7189 Acc: 0.5333 \n",
      "Epoch 10/29\n",
      "----------\n",
      "train Loss: 1.4823 Acc: 0.8358 \n",
      "val Loss: 1.7166 Acc: 0.5556 \n",
      "Epoch 11/29\n",
      "----------\n",
      "train Loss: 1.4788 Acc: 0.8507 \n",
      "val Loss: 1.7142 Acc: 0.5556 \n",
      "Epoch 12/29\n",
      "----------\n",
      "train Loss: 1.4753 Acc: 0.8507 \n",
      "val Loss: 1.7118 Acc: 0.5556 \n",
      "Epoch 13/29\n",
      "----------\n",
      "train Loss: 1.4718 Acc: 0.8582 \n",
      "val Loss: 1.7095 Acc: 0.5556 \n",
      "Epoch 14/29\n",
      "----------\n",
      "train Loss: 1.4683 Acc: 0.8657 \n",
      "val Loss: 1.7072 Acc: 0.5556 \n",
      "Epoch 15/29\n",
      "----------\n",
      "train Loss: 1.4649 Acc: 0.8657 \n",
      "val Loss: 1.7050 Acc: 0.5556 \n",
      "Epoch 16/29\n",
      "----------\n",
      "train Loss: 1.4616 Acc: 0.8731 \n",
      "val Loss: 1.7027 Acc: 0.5667 \n",
      "Epoch 17/29\n",
      "----------\n",
      "train Loss: 1.4582 Acc: 0.8731 \n",
      "val Loss: 1.7005 Acc: 0.5667 \n",
      "Epoch 18/29\n",
      "----------\n",
      "train Loss: 1.4549 Acc: 0.8731 \n",
      "val Loss: 1.6982 Acc: 0.5667 \n",
      "Epoch 19/29\n",
      "----------\n",
      "train Loss: 1.4517 Acc: 0.8731 \n",
      "val Loss: 1.6961 Acc: 0.5778 \n",
      "Epoch 20/29\n",
      "----------\n",
      "train Loss: 1.4484 Acc: 0.8731 \n",
      "val Loss: 1.6939 Acc: 0.5778 \n",
      "Epoch 21/29\n",
      "----------\n",
      "train Loss: 1.4452 Acc: 0.8731 \n",
      "val Loss: 1.6917 Acc: 0.5778 \n",
      "Epoch 22/29\n",
      "----------\n",
      "train Loss: 1.4421 Acc: 0.8731 \n",
      "val Loss: 1.6896 Acc: 0.5778 \n",
      "Epoch 23/29\n",
      "----------\n",
      "train Loss: 1.4390 Acc: 0.8731 \n",
      "val Loss: 1.6874 Acc: 0.5778 \n",
      "Epoch 24/29\n",
      "----------\n",
      "train Loss: 1.4359 Acc: 0.8806 \n",
      "val Loss: 1.6853 Acc: 0.5889 \n",
      "Epoch 25/29\n",
      "----------\n",
      "train Loss: 1.4328 Acc: 0.8806 \n",
      "val Loss: 1.6832 Acc: 0.6000 \n",
      "Epoch 26/29\n",
      "----------\n",
      "train Loss: 1.4298 Acc: 0.8806 \n",
      "val Loss: 1.6811 Acc: 0.6000 \n",
      "Epoch 27/29\n",
      "----------\n",
      "train Loss: 1.4268 Acc: 0.8806 \n",
      "val Loss: 1.6791 Acc: 0.6000 \n",
      "Epoch 28/29\n",
      "----------\n",
      "train Loss: 1.4238 Acc: 0.8806 \n",
      "val Loss: 1.6770 Acc: 0.6000 \n",
      "Epoch 29/29\n",
      "----------\n",
      "train Loss: 1.4209 Acc: 0.8806 \n",
      "val Loss: 1.6750 Acc: 0.6000 \n",
      "Training complete in 0m 7s\n",
      "Best val Acc: 0.600000\n",
      "Epoch generations ( 61 /200) :accuracy(fittest mask) tensor(0.8806, dtype=torch.float64) keep_prob 1.0\n",
      "Epoch generations ( 62 /200) :accuracy(fittest mask) tensor(0.8806, dtype=torch.float64) keep_prob 1.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch generations ( 63 /200) :accuracy(fittest mask) tensor(0.8806, dtype=torch.float64) keep_prob 1.0\n",
      "Epoch generations ( 64 /200) :accuracy(fittest mask) tensor(0.8806, dtype=torch.float64) keep_prob 1.0\n",
      "Epoch generations ( 65 /200) :accuracy(fittest mask) tensor(0.8806, dtype=torch.float64) keep_prob 1.0\n",
      "Epoch generations ( 66 /200) :accuracy(fittest mask) tensor(0.8806, dtype=torch.float64) keep_prob 1.0\n",
      "Epoch generations ( 67 /200) :accuracy(fittest mask) tensor(0.8806, dtype=torch.float64) keep_prob 1.0\n",
      "Epoch generations ( 68 /200) :accuracy(fittest mask) tensor(0.8806, dtype=torch.float64) keep_prob 1.0\n",
      "Epoch generations ( 69 /200) :accuracy(fittest mask) tensor(0.8806, dtype=torch.float64) keep_prob 1.0\n",
      "Epoch generations ( 70 /200) :accuracy(fittest mask) tensor(0.8806, dtype=torch.float64) keep_prob 1.0\n",
      "Epoch 0/29\n",
      "----------\n",
      "train Loss: 1.4298 Acc: 0.8806 \n",
      "val Loss: 1.6812 Acc: 0.6000 \n",
      "Epoch 1/29\n",
      "----------\n",
      "train Loss: 1.4268 Acc: 0.8806 \n",
      "val Loss: 1.6791 Acc: 0.6000 \n",
      "Epoch 2/29\n",
      "----------\n",
      "train Loss: 1.4239 Acc: 0.8806 \n",
      "val Loss: 1.6771 Acc: 0.6000 \n",
      "Epoch 3/29\n",
      "----------\n",
      "train Loss: 1.4209 Acc: 0.8806 \n",
      "val Loss: 1.6750 Acc: 0.6000 \n",
      "Epoch 4/29\n",
      "----------\n",
      "train Loss: 1.4180 Acc: 0.8806 \n",
      "val Loss: 1.6730 Acc: 0.6111 \n",
      "Epoch 5/29\n",
      "----------\n",
      "train Loss: 1.4152 Acc: 0.8806 \n",
      "val Loss: 1.6710 Acc: 0.6222 \n",
      "Epoch 6/29\n",
      "----------\n",
      "train Loss: 1.4123 Acc: 0.8806 \n",
      "val Loss: 1.6690 Acc: 0.6333 \n",
      "Epoch 7/29\n",
      "----------\n",
      "train Loss: 1.4095 Acc: 0.8806 \n",
      "val Loss: 1.6670 Acc: 0.6333 \n",
      "Epoch 8/29\n",
      "----------\n",
      "train Loss: 1.4067 Acc: 0.8806 \n",
      "val Loss: 1.6650 Acc: 0.6333 \n",
      "Epoch 9/29\n",
      "----------\n",
      "train Loss: 1.4039 Acc: 0.8806 \n",
      "val Loss: 1.6630 Acc: 0.6444 \n",
      "Epoch 10/29\n",
      "----------\n",
      "train Loss: 1.4012 Acc: 0.8806 \n",
      "val Loss: 1.6611 Acc: 0.6444 \n",
      "Epoch 11/29\n",
      "----------\n",
      "train Loss: 1.3984 Acc: 0.8806 \n",
      "val Loss: 1.6591 Acc: 0.6667 \n",
      "Epoch 12/29\n",
      "----------\n",
      "train Loss: 1.3958 Acc: 0.8806 \n",
      "val Loss: 1.6572 Acc: 0.6667 \n",
      "Epoch 13/29\n",
      "----------\n",
      "train Loss: 1.3931 Acc: 0.8881 \n",
      "val Loss: 1.6553 Acc: 0.6667 \n",
      "Epoch 14/29\n",
      "----------\n",
      "train Loss: 1.3904 Acc: 0.8955 \n",
      "val Loss: 1.6534 Acc: 0.6778 \n",
      "Epoch 15/29\n",
      "----------\n",
      "train Loss: 1.3878 Acc: 0.8955 \n",
      "val Loss: 1.6516 Acc: 0.6778 \n",
      "Epoch 16/29\n",
      "----------\n",
      "train Loss: 1.3852 Acc: 0.9030 \n",
      "val Loss: 1.6497 Acc: 0.6778 \n",
      "Epoch 17/29\n",
      "----------\n",
      "train Loss: 1.3827 Acc: 0.9030 \n",
      "val Loss: 1.6479 Acc: 0.6778 \n",
      "Epoch 18/29\n",
      "----------\n",
      "train Loss: 1.3801 Acc: 0.9030 \n",
      "val Loss: 1.6461 Acc: 0.6778 \n",
      "Epoch 19/29\n",
      "----------\n",
      "train Loss: 1.3776 Acc: 0.9030 \n",
      "val Loss: 1.6443 Acc: 0.6778 \n",
      "Epoch 20/29\n",
      "----------\n",
      "train Loss: 1.3751 Acc: 0.9030 \n",
      "val Loss: 1.6425 Acc: 0.6778 \n",
      "Epoch 21/29\n",
      "----------\n",
      "train Loss: 1.3726 Acc: 0.9030 \n",
      "val Loss: 1.6407 Acc: 0.6778 \n",
      "Epoch 22/29\n",
      "----------\n",
      "train Loss: 1.3702 Acc: 0.9030 \n",
      "val Loss: 1.6390 Acc: 0.6778 \n",
      "Epoch 23/29\n",
      "----------\n",
      "train Loss: 1.3678 Acc: 0.9030 \n",
      "val Loss: 1.6373 Acc: 0.6778 \n",
      "Epoch 24/29\n",
      "----------\n",
      "train Loss: 1.3653 Acc: 0.9030 \n",
      "val Loss: 1.6356 Acc: 0.6778 \n",
      "Epoch 25/29\n",
      "----------\n",
      "train Loss: 1.3630 Acc: 0.9104 \n",
      "val Loss: 1.6339 Acc: 0.6778 \n",
      "Epoch 26/29\n",
      "----------\n",
      "train Loss: 1.3606 Acc: 0.9104 \n",
      "val Loss: 1.6323 Acc: 0.6778 \n",
      "Epoch 27/29\n",
      "----------\n",
      "train Loss: 1.3583 Acc: 0.9104 \n",
      "val Loss: 1.6306 Acc: 0.6778 \n",
      "Epoch 28/29\n",
      "----------\n",
      "train Loss: 1.3560 Acc: 0.9104 \n",
      "val Loss: 1.6290 Acc: 0.6778 \n",
      "Epoch 29/29\n",
      "----------\n",
      "train Loss: 1.3537 Acc: 0.9104 \n",
      "val Loss: 1.6274 Acc: 0.6889 \n",
      "Training complete in 0m 7s\n",
      "Best val Acc: 0.688889\n",
      "Epoch generations ( 71 /200) :accuracy(fittest mask) tensor(0.9104, dtype=torch.float64) keep_prob 1.0\n",
      "Epoch generations ( 72 /200) :accuracy(fittest mask) tensor(0.9104, dtype=torch.float64) keep_prob 1.0\n",
      "Epoch generations ( 73 /200) :accuracy(fittest mask) tensor(0.9104, dtype=torch.float64) keep_prob 1.0\n",
      "Epoch generations ( 74 /200) :accuracy(fittest mask) tensor(0.9104, dtype=torch.float64) keep_prob 1.0\n",
      "Epoch generations ( 75 /200) :accuracy(fittest mask) tensor(0.9104, dtype=torch.float64) keep_prob 1.0\n",
      "Epoch generations ( 76 /200) :accuracy(fittest mask) tensor(0.9104, dtype=torch.float64) keep_prob 1.0\n",
      "Epoch generations ( 77 /200) :accuracy(fittest mask) tensor(0.9104, dtype=torch.float64) keep_prob 1.0\n",
      "Epoch generations ( 78 /200) :accuracy(fittest mask) tensor(0.9104, dtype=torch.float64) keep_prob 1.0\n",
      "Epoch generations ( 79 /200) :accuracy(fittest mask) tensor(0.9104, dtype=torch.float64) keep_prob 1.0\n",
      "Epoch generations ( 80 /200) :accuracy(fittest mask) tensor(0.9104, dtype=torch.float64) keep_prob 1.0\n",
      "Epoch 0/29\n",
      "----------\n",
      "train Loss: 1.3514 Acc: 0.9104 \n",
      "val Loss: 1.6259 Acc: 0.7000 \n",
      "Epoch 1/29\n",
      "----------\n",
      "train Loss: 1.3492 Acc: 0.9104 \n",
      "val Loss: 1.6243 Acc: 0.7000 \n",
      "Epoch 2/29\n",
      "----------\n",
      "train Loss: 1.3470 Acc: 0.9104 \n",
      "val Loss: 1.6228 Acc: 0.7000 \n",
      "Epoch 3/29\n",
      "----------\n",
      "train Loss: 1.3448 Acc: 0.9104 \n",
      "val Loss: 1.6212 Acc: 0.7000 \n",
      "Epoch 4/29\n",
      "----------\n",
      "train Loss: 1.3427 Acc: 0.9104 \n",
      "val Loss: 1.6197 Acc: 0.7000 \n",
      "Epoch 5/29\n",
      "----------\n",
      "train Loss: 1.3405 Acc: 0.9179 \n",
      "val Loss: 1.6183 Acc: 0.7111 \n",
      "Epoch 6/29\n",
      "----------\n",
      "train Loss: 1.3384 Acc: 0.9179 \n",
      "val Loss: 1.6168 Acc: 0.7111 \n",
      "Epoch 7/29\n",
      "----------\n",
      "train Loss: 1.3363 Acc: 0.9179 \n",
      "val Loss: 1.6153 Acc: 0.7111 \n",
      "Epoch 8/29\n",
      "----------\n",
      "train Loss: 1.3342 Acc: 0.9254 \n",
      "val Loss: 1.6139 Acc: 0.7111 \n",
      "Epoch 9/29\n",
      "----------\n",
      "train Loss: 1.3322 Acc: 0.9328 \n",
      "val Loss: 1.6125 Acc: 0.7111 \n",
      "Epoch 10/29\n",
      "----------\n",
      "train Loss: 1.3302 Acc: 0.9328 \n",
      "val Loss: 1.6111 Acc: 0.7000 \n",
      "Epoch 11/29\n",
      "----------\n",
      "train Loss: 1.3281 Acc: 0.9328 \n",
      "val Loss: 1.6097 Acc: 0.7000 \n",
      "Epoch 12/29\n",
      "----------\n",
      "train Loss: 1.3262 Acc: 0.9328 \n",
      "val Loss: 1.6083 Acc: 0.7111 \n",
      "Epoch 13/29\n",
      "----------\n",
      "train Loss: 1.3242 Acc: 0.9328 \n",
      "val Loss: 1.6069 Acc: 0.7222 \n",
      "Epoch 14/29\n",
      "----------\n",
      "train Loss: 1.3222 Acc: 0.9403 \n",
      "val Loss: 1.6056 Acc: 0.7222 \n",
      "Epoch 15/29\n",
      "----------\n",
      "train Loss: 1.3203 Acc: 0.9552 \n",
      "val Loss: 1.6042 Acc: 0.7222 \n",
      "Epoch 16/29\n",
      "----------\n",
      "train Loss: 1.3184 Acc: 0.9627 \n",
      "val Loss: 1.6029 Acc: 0.7111 \n",
      "Epoch 17/29\n",
      "----------\n",
      "train Loss: 1.3165 Acc: 0.9627 \n",
      "val Loss: 1.6016 Acc: 0.7111 \n",
      "Epoch 18/29\n",
      "----------\n",
      "train Loss: 1.3146 Acc: 0.9627 \n",
      "val Loss: 1.6003 Acc: 0.7111 \n",
      "Epoch 19/29\n",
      "----------\n",
      "train Loss: 1.3127 Acc: 0.9627 \n",
      "val Loss: 1.5990 Acc: 0.7111 \n",
      "Epoch 20/29\n",
      "----------\n",
      "train Loss: 1.3109 Acc: 0.9627 \n",
      "val Loss: 1.5977 Acc: 0.7111 \n",
      "Epoch 21/29\n",
      "----------\n",
      "train Loss: 1.3091 Acc: 0.9627 \n",
      "val Loss: 1.5965 Acc: 0.7222 \n",
      "Epoch 22/29\n",
      "----------\n",
      "train Loss: 1.3072 Acc: 0.9627 \n",
      "val Loss: 1.5952 Acc: 0.7222 \n",
      "Epoch 23/29\n",
      "----------\n",
      "train Loss: 1.3054 Acc: 0.9701 \n",
      "val Loss: 1.5940 Acc: 0.7222 \n",
      "Epoch 24/29\n",
      "----------\n",
      "train Loss: 1.3036 Acc: 0.9701 \n",
      "val Loss: 1.5927 Acc: 0.7222 \n",
      "Epoch 25/29\n",
      "----------\n",
      "train Loss: 1.3019 Acc: 0.9701 \n",
      "val Loss: 1.5915 Acc: 0.7333 \n",
      "Epoch 26/29\n",
      "----------\n",
      "train Loss: 1.3001 Acc: 0.9701 \n",
      "val Loss: 1.5903 Acc: 0.7444 \n",
      "Epoch 27/29\n",
      "----------\n",
      "train Loss: 1.2984 Acc: 0.9701 \n",
      "val Loss: 1.5891 Acc: 0.7556 \n",
      "Epoch 28/29\n",
      "----------\n",
      "train Loss: 1.2966 Acc: 0.9701 \n",
      "val Loss: 1.5879 Acc: 0.7556 \n",
      "Epoch 29/29\n",
      "----------\n",
      "train Loss: 1.2949 Acc: 0.9701 \n",
      "val Loss: 1.5867 Acc: 0.7444 \n",
      "Training complete in 0m 7s\n",
      "Best val Acc: 0.755556\n",
      "Epoch generations ( 81 /200) :accuracy(fittest mask) tensor(0.9701, dtype=torch.float64) keep_prob 1.0\n",
      "Epoch generations ( 82 /200) :accuracy(fittest mask) tensor(0.9701, dtype=torch.float64) keep_prob 1.0\n",
      "Epoch generations ( 83 /200) :accuracy(fittest mask) tensor(0.9701, dtype=torch.float64) keep_prob 1.0\n",
      "Epoch generations ( 84 /200) :accuracy(fittest mask) tensor(0.9701, dtype=torch.float64) keep_prob 1.0\n",
      "Epoch generations ( 85 /200) :accuracy(fittest mask) tensor(0.9701, dtype=torch.float64) keep_prob 1.0\n",
      "Epoch generations ( 86 /200) :accuracy(fittest mask) tensor(0.9701, dtype=torch.float64) keep_prob 1.0\n",
      "Epoch generations ( 87 /200) :accuracy(fittest mask) tensor(0.9701, dtype=torch.float64) keep_prob 1.0\n",
      "Epoch generations ( 88 /200) :accuracy(fittest mask) tensor(0.9701, dtype=torch.float64) keep_prob 1.0\n",
      "Epoch generations ( 89 /200) :accuracy(fittest mask) tensor(0.9701, dtype=torch.float64) keep_prob 1.0\n",
      "Epoch generations ( 90 /200) :accuracy(fittest mask) tensor(0.9701, dtype=torch.float64) keep_prob 1.0\n",
      "Epoch 0/29\n",
      "----------\n",
      "train Loss: 1.2966 Acc: 0.9701 \n",
      "val Loss: 1.5879 Acc: 0.7556 \n",
      "Epoch 1/29\n",
      "----------\n",
      "train Loss: 1.2949 Acc: 0.9701 \n",
      "val Loss: 1.5867 Acc: 0.7444 \n",
      "Epoch 2/29\n",
      "----------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train Loss: 1.2932 Acc: 0.9701 \n",
      "val Loss: 1.5855 Acc: 0.7556 \n",
      "Epoch 3/29\n",
      "----------\n",
      "train Loss: 1.2916 Acc: 0.9701 \n",
      "val Loss: 1.5843 Acc: 0.7778 \n",
      "Epoch 4/29\n",
      "----------\n",
      "train Loss: 1.2899 Acc: 0.9701 \n",
      "val Loss: 1.5831 Acc: 0.7667 \n",
      "Epoch 5/29\n",
      "----------\n",
      "train Loss: 1.2882 Acc: 0.9701 \n",
      "val Loss: 1.5819 Acc: 0.7667 \n",
      "Epoch 6/29\n",
      "----------\n",
      "train Loss: 1.2866 Acc: 0.9776 \n",
      "val Loss: 1.5807 Acc: 0.7778 \n",
      "Epoch 7/29\n",
      "----------\n",
      "train Loss: 1.2850 Acc: 0.9851 \n",
      "val Loss: 1.5795 Acc: 0.7778 \n",
      "Epoch 8/29\n",
      "----------\n",
      "train Loss: 1.2834 Acc: 0.9851 \n",
      "val Loss: 1.5784 Acc: 0.7667 \n",
      "Epoch 9/29\n",
      "----------\n",
      "train Loss: 1.2818 Acc: 0.9851 \n",
      "val Loss: 1.5772 Acc: 0.7667 \n",
      "Epoch 10/29\n",
      "----------\n",
      "train Loss: 1.2802 Acc: 0.9851 \n",
      "val Loss: 1.5760 Acc: 0.7556 \n",
      "Epoch 11/29\n",
      "----------\n",
      "train Loss: 1.2787 Acc: 0.9851 \n",
      "val Loss: 1.5749 Acc: 0.7556 \n",
      "Epoch 12/29\n",
      "----------\n",
      "train Loss: 1.2771 Acc: 0.9851 \n",
      "val Loss: 1.5737 Acc: 0.7556 \n",
      "Epoch 13/29\n",
      "----------\n",
      "train Loss: 1.2756 Acc: 0.9851 \n",
      "val Loss: 1.5725 Acc: 0.7556 \n",
      "Epoch 14/29\n",
      "----------\n",
      "train Loss: 1.2741 Acc: 0.9851 \n",
      "val Loss: 1.5714 Acc: 0.7556 \n",
      "Epoch 15/29\n",
      "----------\n",
      "train Loss: 1.2726 Acc: 0.9851 \n",
      "val Loss: 1.5702 Acc: 0.7556 \n",
      "Epoch 16/29\n",
      "----------\n",
      "train Loss: 1.2712 Acc: 0.9925 \n",
      "val Loss: 1.5691 Acc: 0.7556 \n",
      "Epoch 17/29\n",
      "----------\n",
      "train Loss: 1.2697 Acc: 0.9925 \n",
      "val Loss: 1.5679 Acc: 0.7556 \n",
      "Epoch 18/29\n",
      "----------\n",
      "train Loss: 1.2683 Acc: 0.9925 \n",
      "val Loss: 1.5667 Acc: 0.7556 \n",
      "Epoch 19/29\n",
      "----------\n",
      "train Loss: 1.2669 Acc: 0.9925 \n",
      "val Loss: 1.5656 Acc: 0.7556 \n",
      "Epoch 20/29\n",
      "----------\n",
      "train Loss: 1.2655 Acc: 0.9925 \n",
      "val Loss: 1.5644 Acc: 0.7556 \n",
      "Epoch 21/29\n",
      "----------\n",
      "train Loss: 1.2641 Acc: 0.9925 \n",
      "val Loss: 1.5633 Acc: 0.7556 \n",
      "Epoch 22/29\n",
      "----------\n",
      "train Loss: 1.2627 Acc: 0.9925 \n",
      "val Loss: 1.5621 Acc: 0.7667 \n",
      "Epoch 23/29\n",
      "----------\n",
      "train Loss: 1.2614 Acc: 0.9925 \n",
      "val Loss: 1.5610 Acc: 0.7667 \n",
      "Epoch 24/29\n",
      "----------\n",
      "train Loss: 1.2601 Acc: 0.9925 \n",
      "val Loss: 1.5599 Acc: 0.7667 \n",
      "Epoch 25/29\n",
      "----------\n",
      "train Loss: 1.2588 Acc: 0.9925 \n",
      "val Loss: 1.5587 Acc: 0.7778 \n",
      "Epoch 26/29\n",
      "----------\n",
      "train Loss: 1.2575 Acc: 0.9925 \n",
      "val Loss: 1.5576 Acc: 0.7778 \n",
      "Epoch 27/29\n",
      "----------\n",
      "train Loss: 1.2562 Acc: 0.9925 \n",
      "val Loss: 1.5565 Acc: 0.7778 \n",
      "Epoch 28/29\n",
      "----------\n",
      "train Loss: 1.2550 Acc: 0.9925 \n",
      "val Loss: 1.5553 Acc: 0.7778 \n",
      "Epoch 29/29\n",
      "----------\n",
      "train Loss: 1.2538 Acc: 0.9925 \n",
      "val Loss: 1.5542 Acc: 0.7778 \n",
      "Training complete in 0m 7s\n",
      "Best val Acc: 0.777778\n",
      "Epoch generations ( 91 /200) :accuracy(fittest mask) tensor(0.9701, dtype=torch.float64) keep_prob 1.0\n",
      "Epoch generations ( 92 /200) :accuracy(fittest mask) tensor(0.9701, dtype=torch.float64) keep_prob 1.0\n",
      "Epoch generations ( 93 /200) :accuracy(fittest mask) tensor(0.9701, dtype=torch.float64) keep_prob 1.0\n",
      "Epoch generations ( 94 /200) :accuracy(fittest mask) tensor(0.9701, dtype=torch.float64) keep_prob 1.0\n",
      "Epoch generations ( 95 /200) :accuracy(fittest mask) tensor(0.9701, dtype=torch.float64) keep_prob 1.0\n",
      "Epoch generations ( 96 /200) :accuracy(fittest mask) tensor(0.9701, dtype=torch.float64) keep_prob 1.0\n",
      "Epoch generations ( 97 /200) :accuracy(fittest mask) tensor(0.9701, dtype=torch.float64) keep_prob 1.0\n",
      "Epoch generations ( 98 /200) :accuracy(fittest mask) tensor(0.9701, dtype=torch.float64) keep_prob 1.0\n",
      "Epoch generations ( 99 /200) :accuracy(fittest mask) tensor(0.9701, dtype=torch.float64) keep_prob 1.0\n",
      "Epoch generations ( 100 /200) :accuracy(fittest mask) tensor(0.9701, dtype=torch.float64) keep_prob 1.0\n",
      "Epoch 0/29\n",
      "----------\n",
      "train Loss: 1.2900 Acc: 0.9701 \n",
      "val Loss: 1.5831 Acc: 0.7667 \n",
      "Epoch 1/29\n",
      "----------\n",
      "train Loss: 1.2884 Acc: 0.9701 \n",
      "val Loss: 1.5819 Acc: 0.7667 \n",
      "Epoch 2/29\n",
      "----------\n",
      "train Loss: 1.2868 Acc: 0.9776 \n",
      "val Loss: 1.5808 Acc: 0.7778 \n",
      "Epoch 3/29\n",
      "----------\n",
      "train Loss: 1.2852 Acc: 0.9851 \n",
      "val Loss: 1.5796 Acc: 0.7778 \n",
      "Epoch 4/29\n",
      "----------\n",
      "train Loss: 1.2836 Acc: 0.9851 \n",
      "val Loss: 1.5784 Acc: 0.7778 \n",
      "Epoch 5/29\n",
      "----------\n",
      "train Loss: 1.2820 Acc: 0.9851 \n",
      "val Loss: 1.5773 Acc: 0.7778 \n",
      "Epoch 6/29\n",
      "----------\n",
      "train Loss: 1.2804 Acc: 0.9851 \n",
      "val Loss: 1.5761 Acc: 0.7778 \n",
      "Epoch 7/29\n",
      "----------\n",
      "train Loss: 1.2789 Acc: 0.9851 \n",
      "val Loss: 1.5750 Acc: 0.7556 \n",
      "Epoch 8/29\n",
      "----------\n",
      "train Loss: 1.2773 Acc: 0.9851 \n",
      "val Loss: 1.5738 Acc: 0.7556 \n",
      "Epoch 9/29\n",
      "----------\n",
      "train Loss: 1.2758 Acc: 0.9851 \n",
      "val Loss: 1.5727 Acc: 0.7556 \n",
      "Epoch 10/29\n",
      "----------\n",
      "train Loss: 1.2743 Acc: 0.9851 \n",
      "val Loss: 1.5715 Acc: 0.7556 \n",
      "Epoch 11/29\n",
      "----------\n",
      "train Loss: 1.2728 Acc: 0.9851 \n",
      "val Loss: 1.5703 Acc: 0.7556 \n",
      "Epoch 12/29\n",
      "----------\n",
      "train Loss: 1.2714 Acc: 0.9925 \n",
      "val Loss: 1.5692 Acc: 0.7556 \n",
      "Epoch 13/29\n",
      "----------\n",
      "train Loss: 1.2699 Acc: 0.9925 \n",
      "val Loss: 1.5680 Acc: 0.7556 \n",
      "Epoch 14/29\n",
      "----------\n",
      "train Loss: 1.2685 Acc: 0.9925 \n",
      "val Loss: 1.5669 Acc: 0.7556 \n",
      "Epoch 15/29\n",
      "----------\n",
      "train Loss: 1.2670 Acc: 0.9925 \n",
      "val Loss: 1.5657 Acc: 0.7556 \n",
      "Epoch 16/29\n",
      "----------\n",
      "train Loss: 1.2656 Acc: 0.9925 \n",
      "val Loss: 1.5646 Acc: 0.7556 \n",
      "Epoch 17/29\n",
      "----------\n",
      "train Loss: 1.2643 Acc: 0.9925 \n",
      "val Loss: 1.5634 Acc: 0.7556 \n",
      "Epoch 18/29\n",
      "----------\n",
      "train Loss: 1.2629 Acc: 0.9925 \n",
      "val Loss: 1.5623 Acc: 0.7667 \n",
      "Epoch 19/29\n",
      "----------\n",
      "train Loss: 1.2616 Acc: 0.9925 \n",
      "val Loss: 1.5611 Acc: 0.7667 \n",
      "Epoch 20/29\n",
      "----------\n",
      "train Loss: 1.2602 Acc: 0.9925 \n",
      "val Loss: 1.5600 Acc: 0.7667 \n",
      "Epoch 21/29\n",
      "----------\n",
      "train Loss: 1.2589 Acc: 0.9925 \n",
      "val Loss: 1.5588 Acc: 0.7778 \n",
      "Epoch 22/29\n",
      "----------\n",
      "train Loss: 1.2577 Acc: 0.9925 \n",
      "val Loss: 1.5577 Acc: 0.7778 \n",
      "Epoch 23/29\n",
      "----------\n",
      "train Loss: 1.2564 Acc: 0.9925 \n",
      "val Loss: 1.5566 Acc: 0.7778 \n",
      "Epoch 24/29\n",
      "----------\n",
      "train Loss: 1.2551 Acc: 0.9925 \n",
      "val Loss: 1.5554 Acc: 0.7778 \n",
      "Epoch 25/29\n",
      "----------\n",
      "train Loss: 1.2539 Acc: 0.9925 \n",
      "val Loss: 1.5543 Acc: 0.7778 \n",
      "Epoch 26/29\n",
      "----------\n",
      "train Loss: 1.2527 Acc: 0.9925 \n",
      "val Loss: 1.5532 Acc: 0.7778 \n",
      "Epoch 27/29\n",
      "----------\n",
      "train Loss: 1.2515 Acc: 0.9925 \n",
      "val Loss: 1.5521 Acc: 0.7778 \n",
      "Epoch 28/29\n",
      "----------\n",
      "train Loss: 1.2503 Acc: 0.9925 \n",
      "val Loss: 1.5510 Acc: 0.7778 \n",
      "Epoch 29/29\n",
      "----------\n",
      "train Loss: 1.2492 Acc: 0.9925 \n",
      "val Loss: 1.5499 Acc: 0.7778 \n",
      "Training complete in 0m 8s\n",
      "Best val Acc: 0.777778\n",
      "Epoch generations ( 101 /200) :accuracy(fittest mask) tensor(0.9851, dtype=torch.float64) keep_prob 1.0\n",
      "Epoch generations ( 102 /200) :accuracy(fittest mask) tensor(0.9851, dtype=torch.float64) keep_prob 1.0\n",
      "Epoch generations ( 103 /200) :accuracy(fittest mask) tensor(0.9851, dtype=torch.float64) keep_prob 1.0\n",
      "Epoch generations ( 104 /200) :accuracy(fittest mask) tensor(0.9851, dtype=torch.float64) keep_prob 1.0\n",
      "Epoch generations ( 105 /200) :accuracy(fittest mask) tensor(0.9851, dtype=torch.float64) keep_prob 1.0\n",
      "Epoch generations ( 106 /200) :accuracy(fittest mask) tensor(0.9851, dtype=torch.float64) keep_prob 1.0\n",
      "Epoch generations ( 107 /200) :accuracy(fittest mask) tensor(0.9851, dtype=torch.float64) keep_prob 1.0\n",
      "Epoch generations ( 108 /200) :accuracy(fittest mask) tensor(0.9851, dtype=torch.float64) keep_prob 1.0\n",
      "Epoch generations ( 109 /200) :accuracy(fittest mask) tensor(0.9851, dtype=torch.float64) keep_prob 1.0\n",
      "Epoch generations ( 110 /200) :accuracy(fittest mask) tensor(0.9851, dtype=torch.float64) keep_prob 1.0\n",
      "Epoch 0/29\n",
      "----------\n",
      "train Loss: 1.2853 Acc: 0.9851 \n",
      "val Loss: 1.5796 Acc: 0.7778 \n",
      "Epoch 1/29\n",
      "----------\n",
      "train Loss: 1.2838 Acc: 0.9851 \n",
      "val Loss: 1.5785 Acc: 0.7778 \n",
      "Epoch 2/29\n",
      "----------\n",
      "train Loss: 1.2823 Acc: 0.9851 \n",
      "val Loss: 1.5773 Acc: 0.7778 \n",
      "Epoch 3/29\n",
      "----------\n",
      "train Loss: 1.2807 Acc: 0.9851 \n",
      "val Loss: 1.5762 Acc: 0.7778 \n",
      "Epoch 4/29\n",
      "----------\n",
      "train Loss: 1.2791 Acc: 0.9851 \n",
      "val Loss: 1.5751 Acc: 0.7778 \n",
      "Epoch 5/29\n",
      "----------\n",
      "train Loss: 1.2776 Acc: 0.9851 \n",
      "val Loss: 1.5739 Acc: 0.7778 \n",
      "Epoch 6/29\n",
      "----------\n",
      "train Loss: 1.2761 Acc: 0.9851 \n",
      "val Loss: 1.5728 Acc: 0.7667 \n",
      "Epoch 7/29\n",
      "----------\n",
      "train Loss: 1.2745 Acc: 0.9851 \n",
      "val Loss: 1.5716 Acc: 0.7556 \n",
      "Epoch 8/29\n",
      "----------\n",
      "train Loss: 1.2730 Acc: 0.9851 \n",
      "val Loss: 1.5705 Acc: 0.7556 \n",
      "Epoch 9/29\n",
      "----------\n",
      "train Loss: 1.2716 Acc: 0.9925 \n",
      "val Loss: 1.5693 Acc: 0.7556 \n",
      "Epoch 10/29\n",
      "----------\n",
      "train Loss: 1.2701 Acc: 0.9925 \n",
      "val Loss: 1.5682 Acc: 0.7556 \n",
      "Epoch 11/29\n",
      "----------\n",
      "train Loss: 1.2687 Acc: 0.9925 \n",
      "val Loss: 1.5670 Acc: 0.7556 \n",
      "Epoch 12/29\n",
      "----------\n",
      "train Loss: 1.2672 Acc: 0.9925 \n",
      "val Loss: 1.5659 Acc: 0.7556 \n",
      "Epoch 13/29\n",
      "----------\n",
      "train Loss: 1.2658 Acc: 0.9925 \n",
      "val Loss: 1.5647 Acc: 0.7556 \n",
      "Epoch 14/29\n",
      "----------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train Loss: 1.2645 Acc: 0.9925 \n",
      "val Loss: 1.5636 Acc: 0.7556 \n",
      "Epoch 15/29\n",
      "----------\n",
      "train Loss: 1.2631 Acc: 0.9925 \n",
      "val Loss: 1.5624 Acc: 0.7667 \n",
      "Epoch 16/29\n",
      "----------\n",
      "train Loss: 1.2617 Acc: 0.9925 \n",
      "val Loss: 1.5613 Acc: 0.7667 \n",
      "Epoch 17/29\n",
      "----------\n",
      "train Loss: 1.2604 Acc: 0.9925 \n",
      "val Loss: 1.5601 Acc: 0.7667 \n",
      "Epoch 18/29\n",
      "----------\n",
      "train Loss: 1.2591 Acc: 0.9925 \n",
      "val Loss: 1.5590 Acc: 0.7778 \n",
      "Epoch 19/29\n",
      "----------\n",
      "train Loss: 1.2578 Acc: 0.9925 \n",
      "val Loss: 1.5578 Acc: 0.7778 \n",
      "Epoch 20/29\n",
      "----------\n",
      "train Loss: 1.2565 Acc: 0.9925 \n",
      "val Loss: 1.5567 Acc: 0.7778 \n",
      "Epoch 21/29\n",
      "----------\n",
      "train Loss: 1.2553 Acc: 0.9925 \n",
      "val Loss: 1.5556 Acc: 0.7778 \n",
      "Epoch 22/29\n",
      "----------\n",
      "train Loss: 1.2541 Acc: 0.9925 \n",
      "val Loss: 1.5544 Acc: 0.7778 \n",
      "Epoch 23/29\n",
      "----------\n",
      "train Loss: 1.2528 Acc: 0.9925 \n",
      "val Loss: 1.5533 Acc: 0.7778 \n",
      "Epoch 24/29\n",
      "----------\n",
      "train Loss: 1.2517 Acc: 0.9925 \n",
      "val Loss: 1.5522 Acc: 0.7778 \n",
      "Epoch 25/29\n",
      "----------\n",
      "train Loss: 1.2505 Acc: 0.9925 \n",
      "val Loss: 1.5511 Acc: 0.7778 \n",
      "Epoch 26/29\n",
      "----------\n",
      "train Loss: 1.2493 Acc: 0.9925 \n",
      "val Loss: 1.5500 Acc: 0.7778 \n",
      "Epoch 27/29\n",
      "----------\n",
      "train Loss: 1.2482 Acc: 0.9925 \n",
      "val Loss: 1.5489 Acc: 0.7778 \n",
      "Epoch 28/29\n",
      "----------\n",
      "train Loss: 1.2471 Acc: 0.9925 \n",
      "val Loss: 1.5478 Acc: 0.7889 \n",
      "Epoch 29/29\n",
      "----------\n",
      "train Loss: 1.2460 Acc: 0.9925 \n",
      "val Loss: 1.5467 Acc: 0.7889 \n",
      "Training complete in 0m 7s\n",
      "Best val Acc: 0.788889\n",
      "Epoch generations ( 111 /200) :accuracy(fittest mask) tensor(0.9925, dtype=torch.float64) keep_prob 1.0\n",
      "Epoch generations ( 112 /200) :accuracy(fittest mask) tensor(0.9925, dtype=torch.float64) keep_prob 1.0\n",
      "Epoch generations ( 113 /200) :accuracy(fittest mask) tensor(0.9925, dtype=torch.float64) keep_prob 1.0\n",
      "Epoch generations ( 114 /200) :accuracy(fittest mask) tensor(0.9925, dtype=torch.float64) keep_prob 1.0\n",
      "Epoch generations ( 115 /200) :accuracy(fittest mask) tensor(0.9925, dtype=torch.float64) keep_prob 1.0\n",
      "Epoch generations ( 116 /200) :accuracy(fittest mask) tensor(0.9925, dtype=torch.float64) keep_prob 1.0\n",
      "Epoch generations ( 117 /200) :accuracy(fittest mask) tensor(0.9925, dtype=torch.float64) keep_prob 1.0\n",
      "Epoch generations ( 118 /200) :accuracy(fittest mask) tensor(0.9925, dtype=torch.float64) keep_prob 1.0\n",
      "Epoch generations ( 119 /200) :accuracy(fittest mask) tensor(0.9925, dtype=torch.float64) keep_prob 1.0\n",
      "Epoch generations ( 120 /200) :accuracy(fittest mask) tensor(0.9925, dtype=torch.float64) keep_prob 1.0\n",
      "Epoch 0/29\n",
      "----------\n",
      "train Loss: 1.2460 Acc: 0.9925 \n",
      "val Loss: 1.5467 Acc: 0.7889 \n",
      "Epoch 1/29\n",
      "----------\n",
      "train Loss: 1.2449 Acc: 0.9925 \n",
      "val Loss: 1.5456 Acc: 0.7889 \n",
      "Epoch 2/29\n",
      "----------\n",
      "train Loss: 1.2438 Acc: 0.9925 \n",
      "val Loss: 1.5445 Acc: 0.7889 \n",
      "Epoch 3/29\n",
      "----------\n",
      "train Loss: 1.2427 Acc: 0.9925 \n",
      "val Loss: 1.5434 Acc: 0.7889 \n",
      "Epoch 4/29\n",
      "----------\n",
      "train Loss: 1.2417 Acc: 0.9925 \n",
      "val Loss: 1.5424 Acc: 0.7889 \n",
      "Epoch 5/29\n",
      "----------\n",
      "train Loss: 1.2407 Acc: 0.9925 \n",
      "val Loss: 1.5413 Acc: 0.7889 \n",
      "Epoch 6/29\n",
      "----------\n",
      "train Loss: 1.2397 Acc: 0.9925 \n",
      "val Loss: 1.5402 Acc: 0.7889 \n",
      "Epoch 7/29\n",
      "----------\n",
      "train Loss: 1.2387 Acc: 0.9925 \n",
      "val Loss: 1.5392 Acc: 0.7889 \n",
      "Epoch 8/29\n",
      "----------\n",
      "train Loss: 1.2377 Acc: 0.9925 \n",
      "val Loss: 1.5381 Acc: 0.8000 \n",
      "Epoch 9/29\n",
      "----------\n",
      "train Loss: 1.2368 Acc: 0.9925 \n",
      "val Loss: 1.5371 Acc: 0.8000 \n",
      "Epoch 10/29\n",
      "----------\n",
      "train Loss: 1.2358 Acc: 0.9925 \n",
      "val Loss: 1.5361 Acc: 0.8111 \n",
      "Epoch 11/29\n",
      "----------\n",
      "train Loss: 1.2349 Acc: 0.9925 \n",
      "val Loss: 1.5350 Acc: 0.8111 \n",
      "Epoch 12/29\n",
      "----------\n",
      "train Loss: 1.2340 Acc: 0.9925 \n",
      "val Loss: 1.5340 Acc: 0.8111 \n",
      "Epoch 13/29\n",
      "----------\n",
      "train Loss: 1.2331 Acc: 0.9925 \n",
      "val Loss: 1.5330 Acc: 0.8111 \n",
      "Epoch 14/29\n",
      "----------\n",
      "train Loss: 1.2322 Acc: 0.9925 \n",
      "val Loss: 1.5320 Acc: 0.8111 \n",
      "Epoch 15/29\n",
      "----------\n",
      "train Loss: 1.2314 Acc: 0.9925 \n",
      "val Loss: 1.5310 Acc: 0.8111 \n",
      "Epoch 16/29\n",
      "----------\n",
      "train Loss: 1.2305 Acc: 0.9925 \n",
      "val Loss: 1.5300 Acc: 0.8111 \n",
      "Epoch 17/29\n",
      "----------\n",
      "train Loss: 1.2297 Acc: 0.9925 \n",
      "val Loss: 1.5290 Acc: 0.8111 \n",
      "Epoch 18/29\n",
      "----------\n",
      "train Loss: 1.2289 Acc: 0.9925 \n",
      "val Loss: 1.5280 Acc: 0.8111 \n",
      "Epoch 19/29\n",
      "----------\n",
      "train Loss: 1.2280 Acc: 0.9925 \n",
      "val Loss: 1.5271 Acc: 0.8111 \n",
      "Epoch 20/29\n",
      "----------\n",
      "train Loss: 1.2273 Acc: 0.9925 \n",
      "val Loss: 1.5261 Acc: 0.8222 \n",
      "Epoch 21/29\n",
      "----------\n",
      "train Loss: 1.2265 Acc: 0.9925 \n",
      "val Loss: 1.5251 Acc: 0.8222 \n",
      "Epoch 22/29\n",
      "----------\n",
      "train Loss: 1.2257 Acc: 0.9925 \n",
      "val Loss: 1.5242 Acc: 0.8222 \n",
      "Epoch 23/29\n",
      "----------\n",
      "train Loss: 1.2249 Acc: 0.9925 \n",
      "val Loss: 1.5232 Acc: 0.8222 \n",
      "Epoch 24/29\n",
      "----------\n",
      "train Loss: 1.2242 Acc: 0.9925 \n",
      "val Loss: 1.5223 Acc: 0.8222 \n",
      "Epoch 25/29\n",
      "----------\n",
      "train Loss: 1.2235 Acc: 1.0000 \n",
      "val Loss: 1.5214 Acc: 0.8222 \n",
      "Epoch 26/29\n",
      "----------\n",
      "train Loss: 1.2227 Acc: 1.0000 \n",
      "val Loss: 1.5205 Acc: 0.8222 \n",
      "Epoch 27/29\n",
      "----------\n",
      "train Loss: 1.2220 Acc: 1.0000 \n",
      "val Loss: 1.5195 Acc: 0.8222 \n",
      "Epoch 28/29\n",
      "----------\n",
      "train Loss: 1.2213 Acc: 1.0000 \n",
      "val Loss: 1.5186 Acc: 0.8222 \n",
      "Epoch 29/29\n",
      "----------\n",
      "train Loss: 1.2206 Acc: 1.0000 \n",
      "val Loss: 1.5177 Acc: 0.8222 \n",
      "Training complete in 0m 6s\n",
      "Best val Acc: 0.822222\n",
      "Epoch generations ( 121 /200) :accuracy(fittest mask) tensor(0.9925, dtype=torch.float64) keep_prob 1.0\n",
      "Epoch generations ( 122 /200) :accuracy(fittest mask) tensor(0.9925, dtype=torch.float64) keep_prob 1.0\n",
      "Epoch generations ( 123 /200) :accuracy(fittest mask) tensor(0.9925, dtype=torch.float64) keep_prob 1.0\n",
      "Epoch generations ( 124 /200) :accuracy(fittest mask) tensor(0.9925, dtype=torch.float64) keep_prob 1.0\n",
      "Epoch generations ( 125 /200) :accuracy(fittest mask) tensor(0.9925, dtype=torch.float64) keep_prob 1.0\n",
      "Epoch generations ( 126 /200) :accuracy(fittest mask) tensor(0.9925, dtype=torch.float64) keep_prob 1.0\n",
      "Epoch generations ( 127 /200) :accuracy(fittest mask) tensor(0.9925, dtype=torch.float64) keep_prob 1.0\n",
      "Epoch generations ( 128 /200) :accuracy(fittest mask) tensor(0.9925, dtype=torch.float64) keep_prob 1.0\n",
      "Epoch generations ( 129 /200) :accuracy(fittest mask) tensor(0.9925, dtype=torch.float64) keep_prob 1.0\n",
      "Epoch generations ( 130 /200) :accuracy(fittest mask) tensor(0.9925, dtype=torch.float64) keep_prob 1.0\n",
      "Epoch 0/29\n",
      "----------\n",
      "train Loss: 1.2265 Acc: 0.9925 \n",
      "val Loss: 1.5252 Acc: 0.8222 \n",
      "Epoch 1/29\n",
      "----------\n",
      "train Loss: 1.2257 Acc: 0.9925 \n",
      "val Loss: 1.5242 Acc: 0.8222 \n",
      "Epoch 2/29\n",
      "----------\n",
      "train Loss: 1.2250 Acc: 0.9925 \n",
      "val Loss: 1.5233 Acc: 0.8222 \n",
      "Epoch 3/29\n",
      "----------\n",
      "train Loss: 1.2242 Acc: 0.9925 \n",
      "val Loss: 1.5224 Acc: 0.8222 \n",
      "Epoch 4/29\n",
      "----------\n",
      "train Loss: 1.2235 Acc: 1.0000 \n",
      "val Loss: 1.5214 Acc: 0.8222 \n",
      "Epoch 5/29\n",
      "----------\n",
      "train Loss: 1.2228 Acc: 1.0000 \n",
      "val Loss: 1.5205 Acc: 0.8222 \n",
      "Epoch 6/29\n",
      "----------\n",
      "train Loss: 1.2221 Acc: 1.0000 \n",
      "val Loss: 1.5196 Acc: 0.8222 \n",
      "Epoch 7/29\n",
      "----------\n",
      "train Loss: 1.2214 Acc: 1.0000 \n",
      "val Loss: 1.5187 Acc: 0.8222 \n",
      "Epoch 8/29\n",
      "----------\n",
      "train Loss: 1.2207 Acc: 1.0000 \n",
      "val Loss: 1.5178 Acc: 0.8222 \n",
      "Epoch 9/29\n",
      "----------\n",
      "train Loss: 1.2200 Acc: 1.0000 \n",
      "val Loss: 1.5169 Acc: 0.8222 \n",
      "Epoch 10/29\n",
      "----------\n",
      "train Loss: 1.2193 Acc: 1.0000 \n",
      "val Loss: 1.5160 Acc: 0.8222 \n",
      "Epoch 11/29\n",
      "----------\n",
      "train Loss: 1.2187 Acc: 1.0000 \n",
      "val Loss: 1.5151 Acc: 0.8222 \n",
      "Epoch 12/29\n",
      "----------\n",
      "train Loss: 1.2180 Acc: 1.0000 \n",
      "val Loss: 1.5143 Acc: 0.8222 \n",
      "Epoch 13/29\n",
      "----------\n",
      "train Loss: 1.2174 Acc: 1.0000 \n",
      "val Loss: 1.5134 Acc: 0.8222 \n",
      "Epoch 14/29\n",
      "----------\n",
      "train Loss: 1.2168 Acc: 1.0000 \n",
      "val Loss: 1.5126 Acc: 0.8222 \n",
      "Epoch 15/29\n",
      "----------\n",
      "train Loss: 1.2161 Acc: 1.0000 \n",
      "val Loss: 1.5117 Acc: 0.8222 \n",
      "Epoch 16/29\n",
      "----------\n",
      "train Loss: 1.2155 Acc: 1.0000 \n",
      "val Loss: 1.5109 Acc: 0.8222 \n",
      "Epoch 17/29\n",
      "----------\n",
      "train Loss: 1.2149 Acc: 1.0000 \n",
      "val Loss: 1.5100 Acc: 0.8222 \n",
      "Epoch 18/29\n",
      "----------\n",
      "train Loss: 1.2144 Acc: 1.0000 \n",
      "val Loss: 1.5092 Acc: 0.8222 \n",
      "Epoch 19/29\n",
      "----------\n",
      "train Loss: 1.2138 Acc: 1.0000 \n",
      "val Loss: 1.5084 Acc: 0.8333 \n",
      "Epoch 20/29\n",
      "----------\n",
      "train Loss: 1.2132 Acc: 1.0000 \n",
      "val Loss: 1.5076 Acc: 0.8333 \n",
      "Epoch 21/29\n",
      "----------\n",
      "train Loss: 1.2126 Acc: 1.0000 \n",
      "val Loss: 1.5068 Acc: 0.8333 \n",
      "Epoch 22/29\n",
      "----------\n",
      "train Loss: 1.2121 Acc: 1.0000 \n",
      "val Loss: 1.5060 Acc: 0.8333 \n",
      "Epoch 23/29\n",
      "----------\n",
      "train Loss: 1.2116 Acc: 1.0000 \n",
      "val Loss: 1.5052 Acc: 0.8333 \n",
      "Epoch 24/29\n",
      "----------\n",
      "train Loss: 1.2110 Acc: 1.0000 \n",
      "val Loss: 1.5044 Acc: 0.8333 \n",
      "Epoch 25/29\n",
      "----------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train Loss: 1.2105 Acc: 1.0000 \n",
      "val Loss: 1.5036 Acc: 0.8333 \n",
      "Epoch 26/29\n",
      "----------\n",
      "train Loss: 1.2100 Acc: 1.0000 \n",
      "val Loss: 1.5029 Acc: 0.8333 \n",
      "Epoch 27/29\n",
      "----------\n",
      "train Loss: 1.2095 Acc: 1.0000 \n",
      "val Loss: 1.5021 Acc: 0.8333 \n",
      "Epoch 28/29\n",
      "----------\n",
      "train Loss: 1.2090 Acc: 1.0000 \n",
      "val Loss: 1.5014 Acc: 0.8333 \n",
      "Epoch 29/29\n",
      "----------\n",
      "train Loss: 1.2085 Acc: 1.0000 \n",
      "val Loss: 1.5006 Acc: 0.8333 \n",
      "Training complete in 0m 7s\n",
      "Best val Acc: 0.833333\n",
      "Epoch generations ( 131 /200) :accuracy(fittest mask) tensor(1., dtype=torch.float64) keep_prob 1.0\n",
      "Epoch generations ( 132 /200) :accuracy(fittest mask) tensor(1., dtype=torch.float64) keep_prob 1.0\n",
      "Epoch generations ( 133 /200) :accuracy(fittest mask) tensor(1., dtype=torch.float64) keep_prob 1.0\n",
      "Epoch generations ( 134 /200) :accuracy(fittest mask) tensor(1., dtype=torch.float64) keep_prob 1.0\n",
      "Epoch generations ( 135 /200) :accuracy(fittest mask) tensor(1., dtype=torch.float64) keep_prob 1.0\n",
      "Epoch generations ( 136 /200) :accuracy(fittest mask) tensor(1., dtype=torch.float64) keep_prob 1.0\n",
      "Epoch generations ( 137 /200) :accuracy(fittest mask) tensor(1., dtype=torch.float64) keep_prob 1.0\n",
      "Epoch generations ( 138 /200) :accuracy(fittest mask) tensor(1., dtype=torch.float64) keep_prob 1.0\n",
      "Epoch generations ( 139 /200) :accuracy(fittest mask) tensor(1., dtype=torch.float64) keep_prob 1.0\n",
      "Epoch generations ( 140 /200) :accuracy(fittest mask) tensor(1., dtype=torch.float64) keep_prob 1.0\n",
      "Epoch 0/29\n",
      "----------\n",
      "train Loss: 1.2132 Acc: 1.0000 \n",
      "val Loss: 1.5076 Acc: 0.8333 \n",
      "Epoch 1/29\n",
      "----------\n",
      "train Loss: 1.2127 Acc: 1.0000 \n",
      "val Loss: 1.5068 Acc: 0.8333 \n",
      "Epoch 2/29\n",
      "----------\n",
      "train Loss: 1.2121 Acc: 1.0000 \n",
      "val Loss: 1.5060 Acc: 0.8333 \n",
      "Epoch 3/29\n",
      "----------\n",
      "train Loss: 1.2116 Acc: 1.0000 \n",
      "val Loss: 1.5053 Acc: 0.8333 \n",
      "Epoch 4/29\n",
      "----------\n",
      "train Loss: 1.2111 Acc: 1.0000 \n",
      "val Loss: 1.5045 Acc: 0.8333 \n",
      "Epoch 5/29\n",
      "----------\n",
      "train Loss: 1.2105 Acc: 1.0000 \n",
      "val Loss: 1.5037 Acc: 0.8333 \n",
      "Epoch 6/29\n",
      "----------\n",
      "train Loss: 1.2100 Acc: 1.0000 \n",
      "val Loss: 1.5029 Acc: 0.8333 \n",
      "Epoch 7/29\n",
      "----------\n",
      "train Loss: 1.2095 Acc: 1.0000 \n",
      "val Loss: 1.5022 Acc: 0.8333 \n",
      "Epoch 8/29\n",
      "----------\n",
      "train Loss: 1.2090 Acc: 1.0000 \n",
      "val Loss: 1.5014 Acc: 0.8333 \n",
      "Epoch 9/29\n",
      "----------\n",
      "train Loss: 1.2085 Acc: 1.0000 \n",
      "val Loss: 1.5007 Acc: 0.8333 \n",
      "Epoch 10/29\n",
      "----------\n",
      "train Loss: 1.2080 Acc: 1.0000 \n",
      "val Loss: 1.4999 Acc: 0.8333 \n",
      "Epoch 11/29\n",
      "----------\n",
      "train Loss: 1.2075 Acc: 1.0000 \n",
      "val Loss: 1.4992 Acc: 0.8333 \n",
      "Epoch 12/29\n",
      "----------\n",
      "train Loss: 1.2071 Acc: 1.0000 \n",
      "val Loss: 1.4985 Acc: 0.8333 \n",
      "Epoch 13/29\n",
      "----------\n",
      "train Loss: 1.2066 Acc: 1.0000 \n",
      "val Loss: 1.4978 Acc: 0.8333 \n",
      "Epoch 14/29\n",
      "----------\n",
      "train Loss: 1.2062 Acc: 1.0000 \n",
      "val Loss: 1.4970 Acc: 0.8333 \n",
      "Epoch 15/29\n",
      "----------\n",
      "train Loss: 1.2057 Acc: 1.0000 \n",
      "val Loss: 1.4963 Acc: 0.8333 \n",
      "Epoch 16/29\n",
      "----------\n",
      "train Loss: 1.2053 Acc: 1.0000 \n",
      "val Loss: 1.4956 Acc: 0.8333 \n",
      "Epoch 17/29\n",
      "----------\n",
      "train Loss: 1.2048 Acc: 1.0000 \n",
      "val Loss: 1.4949 Acc: 0.8333 \n",
      "Epoch 18/29\n",
      "----------\n",
      "train Loss: 1.2044 Acc: 1.0000 \n",
      "val Loss: 1.4943 Acc: 0.8333 \n",
      "Epoch 19/29\n",
      "----------\n",
      "train Loss: 1.2040 Acc: 1.0000 \n",
      "val Loss: 1.4936 Acc: 0.8333 \n",
      "Epoch 20/29\n",
      "----------\n",
      "train Loss: 1.2036 Acc: 1.0000 \n",
      "val Loss: 1.4929 Acc: 0.8333 \n",
      "Epoch 21/29\n",
      "----------\n",
      "train Loss: 1.2032 Acc: 1.0000 \n",
      "val Loss: 1.4922 Acc: 0.8333 \n",
      "Epoch 22/29\n",
      "----------\n",
      "train Loss: 1.2028 Acc: 1.0000 \n",
      "val Loss: 1.4916 Acc: 0.8333 \n",
      "Epoch 23/29\n",
      "----------\n",
      "train Loss: 1.2024 Acc: 1.0000 \n",
      "val Loss: 1.4909 Acc: 0.8333 \n",
      "Epoch 24/29\n",
      "----------\n",
      "train Loss: 1.2020 Acc: 1.0000 \n",
      "val Loss: 1.4903 Acc: 0.8333 \n",
      "Epoch 25/29\n",
      "----------\n",
      "train Loss: 1.2016 Acc: 1.0000 \n",
      "val Loss: 1.4896 Acc: 0.8333 \n",
      "Epoch 26/29\n",
      "----------\n",
      "train Loss: 1.2012 Acc: 1.0000 \n",
      "val Loss: 1.4890 Acc: 0.8333 \n",
      "Epoch 27/29\n",
      "----------\n",
      "train Loss: 1.2008 Acc: 1.0000 \n",
      "val Loss: 1.4883 Acc: 0.8333 \n",
      "Epoch 28/29\n",
      "----------\n",
      "train Loss: 1.2005 Acc: 1.0000 \n",
      "val Loss: 1.4877 Acc: 0.8333 \n",
      "Epoch 29/29\n",
      "----------\n",
      "train Loss: 1.2001 Acc: 1.0000 \n",
      "val Loss: 1.4871 Acc: 0.8333 \n",
      "Training complete in 0m 6s\n",
      "Best val Acc: 0.833333\n",
      "Epoch generations ( 141 /200) :accuracy(fittest mask) tensor(1., dtype=torch.float64) keep_prob 1.0\n",
      "Epoch generations ( 142 /200) :accuracy(fittest mask) tensor(1., dtype=torch.float64) keep_prob 1.0\n",
      "Epoch generations ( 143 /200) :accuracy(fittest mask) tensor(1., dtype=torch.float64) keep_prob 1.0\n",
      "Epoch generations ( 144 /200) :accuracy(fittest mask) tensor(1., dtype=torch.float64) keep_prob 1.0\n",
      "Epoch generations ( 145 /200) :accuracy(fittest mask) tensor(1., dtype=torch.float64) keep_prob 1.0\n",
      "Epoch generations ( 146 /200) :accuracy(fittest mask) tensor(1., dtype=torch.float64) keep_prob 1.0\n",
      "Epoch generations ( 147 /200) :accuracy(fittest mask) tensor(1., dtype=torch.float64) keep_prob 1.0\n",
      "Epoch generations ( 148 /200) :accuracy(fittest mask) tensor(1., dtype=torch.float64) keep_prob 1.0\n",
      "Epoch generations ( 149 /200) :accuracy(fittest mask) tensor(1., dtype=torch.float64) keep_prob 1.0\n",
      "Epoch generations ( 150 /200) :accuracy(fittest mask) tensor(1., dtype=torch.float64) keep_prob 1.0\n",
      "Epoch 0/29\n",
      "----------\n",
      "train Loss: 1.2127 Acc: 1.0000 \n",
      "val Loss: 1.5069 Acc: 0.8333 \n",
      "Epoch 1/29\n",
      "----------\n",
      "train Loss: 1.2122 Acc: 1.0000 \n",
      "val Loss: 1.5061 Acc: 0.8333 \n",
      "Epoch 2/29\n",
      "----------\n",
      "train Loss: 1.2117 Acc: 1.0000 \n",
      "val Loss: 1.5054 Acc: 0.8333 \n",
      "Epoch 3/29\n",
      "----------\n",
      "train Loss: 1.2111 Acc: 1.0000 \n",
      "val Loss: 1.5046 Acc: 0.8333 \n",
      "Epoch 4/29\n",
      "----------\n",
      "train Loss: 1.2106 Acc: 1.0000 \n",
      "val Loss: 1.5038 Acc: 0.8333 \n",
      "Epoch 5/29\n",
      "----------\n",
      "train Loss: 1.2101 Acc: 1.0000 \n",
      "val Loss: 1.5031 Acc: 0.8333 \n",
      "Epoch 6/29\n",
      "----------\n",
      "train Loss: 1.2096 Acc: 1.0000 \n",
      "val Loss: 1.5023 Acc: 0.8333 \n",
      "Epoch 7/29\n",
      "----------\n",
      "train Loss: 1.2091 Acc: 1.0000 \n",
      "val Loss: 1.5016 Acc: 0.8333 \n",
      "Epoch 8/29\n",
      "----------\n",
      "train Loss: 1.2086 Acc: 1.0000 \n",
      "val Loss: 1.5008 Acc: 0.8333 \n",
      "Epoch 9/29\n",
      "----------\n",
      "train Loss: 1.2081 Acc: 1.0000 \n",
      "val Loss: 1.5001 Acc: 0.8333 \n",
      "Epoch 10/29\n",
      "----------\n",
      "train Loss: 1.2076 Acc: 1.0000 \n",
      "val Loss: 1.4993 Acc: 0.8333 \n",
      "Epoch 11/29\n",
      "----------\n",
      "train Loss: 1.2072 Acc: 1.0000 \n",
      "val Loss: 1.4986 Acc: 0.8333 \n",
      "Epoch 12/29\n",
      "----------\n",
      "train Loss: 1.2067 Acc: 1.0000 \n",
      "val Loss: 1.4979 Acc: 0.8333 \n",
      "Epoch 13/29\n",
      "----------\n",
      "train Loss: 1.2062 Acc: 1.0000 \n",
      "val Loss: 1.4972 Acc: 0.8333 \n",
      "Epoch 14/29\n",
      "----------\n",
      "train Loss: 1.2058 Acc: 1.0000 \n",
      "val Loss: 1.4965 Acc: 0.8333 \n",
      "Epoch 15/29\n",
      "----------\n",
      "train Loss: 1.2053 Acc: 1.0000 \n",
      "val Loss: 1.4958 Acc: 0.8333 \n",
      "Epoch 16/29\n",
      "----------\n",
      "train Loss: 1.2049 Acc: 1.0000 \n",
      "val Loss: 1.4951 Acc: 0.8333 \n",
      "Epoch 17/29\n",
      "----------\n",
      "train Loss: 1.2045 Acc: 1.0000 \n",
      "val Loss: 1.4944 Acc: 0.8333 \n",
      "Epoch 18/29\n",
      "----------\n",
      "train Loss: 1.2040 Acc: 1.0000 \n",
      "val Loss: 1.4937 Acc: 0.8333 \n",
      "Epoch 19/29\n",
      "----------\n",
      "train Loss: 1.2036 Acc: 1.0000 \n",
      "val Loss: 1.4930 Acc: 0.8333 \n",
      "Epoch 20/29\n",
      "----------\n",
      "train Loss: 1.2032 Acc: 1.0000 \n",
      "val Loss: 1.4923 Acc: 0.8333 \n",
      "Epoch 21/29\n",
      "----------\n",
      "train Loss: 1.2028 Acc: 1.0000 \n",
      "val Loss: 1.4917 Acc: 0.8333 \n",
      "Epoch 22/29\n",
      "----------\n",
      "train Loss: 1.2024 Acc: 1.0000 \n",
      "val Loss: 1.4910 Acc: 0.8333 \n",
      "Epoch 23/29\n",
      "----------\n",
      "train Loss: 1.2020 Acc: 1.0000 \n",
      "val Loss: 1.4904 Acc: 0.8333 \n",
      "Epoch 24/29\n",
      "----------\n",
      "train Loss: 1.2016 Acc: 1.0000 \n",
      "val Loss: 1.4897 Acc: 0.8333 \n",
      "Epoch 25/29\n",
      "----------\n",
      "train Loss: 1.2013 Acc: 1.0000 \n",
      "val Loss: 1.4891 Acc: 0.8333 \n",
      "Epoch 26/29\n",
      "----------\n",
      "train Loss: 1.2009 Acc: 1.0000 \n",
      "val Loss: 1.4885 Acc: 0.8333 \n",
      "Epoch 27/29\n",
      "----------\n",
      "train Loss: 1.2005 Acc: 1.0000 \n",
      "val Loss: 1.4878 Acc: 0.8333 \n",
      "Epoch 28/29\n",
      "----------\n",
      "train Loss: 1.2002 Acc: 1.0000 \n",
      "val Loss: 1.4872 Acc: 0.8333 \n",
      "Epoch 29/29\n",
      "----------\n",
      "train Loss: 1.1998 Acc: 1.0000 \n",
      "val Loss: 1.4866 Acc: 0.8333 \n",
      "Training complete in 0m 6s\n",
      "Best val Acc: 0.833333\n",
      "Epoch generations ( 151 /200) :accuracy(fittest mask) tensor(1., dtype=torch.float64) keep_prob 1.0\n",
      "Epoch generations ( 152 /200) :accuracy(fittest mask) tensor(1., dtype=torch.float64) keep_prob 1.0\n",
      "Epoch generations ( 153 /200) :accuracy(fittest mask) tensor(1., dtype=torch.float64) keep_prob 1.0\n",
      "Epoch generations ( 154 /200) :accuracy(fittest mask) tensor(1., dtype=torch.float64) keep_prob 1.0\n",
      "Epoch generations ( 155 /200) :accuracy(fittest mask) tensor(1., dtype=torch.float64) keep_prob 1.0\n",
      "Epoch generations ( 156 /200) :accuracy(fittest mask) tensor(1., dtype=torch.float64) keep_prob 1.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch generations ( 157 /200) :accuracy(fittest mask) tensor(1., dtype=torch.float64) keep_prob 1.0\n",
      "Epoch generations ( 158 /200) :accuracy(fittest mask) tensor(1., dtype=torch.float64) keep_prob 1.0\n",
      "Epoch generations ( 159 /200) :accuracy(fittest mask) tensor(1., dtype=torch.float64) keep_prob 1.0\n",
      "Epoch generations ( 160 /200) :accuracy(fittest mask) tensor(1., dtype=torch.float64) keep_prob 1.0\n",
      "Epoch 0/29\n",
      "----------\n",
      "train Loss: 1.2122 Acc: 1.0000 \n",
      "val Loss: 1.5062 Acc: 0.8333 \n",
      "Epoch 1/29\n",
      "----------\n",
      "train Loss: 1.2117 Acc: 1.0000 \n",
      "val Loss: 1.5054 Acc: 0.8333 \n",
      "Epoch 2/29\n",
      "----------\n",
      "train Loss: 1.2112 Acc: 1.0000 \n",
      "val Loss: 1.5047 Acc: 0.8333 \n",
      "Epoch 3/29\n",
      "----------\n",
      "train Loss: 1.2107 Acc: 1.0000 \n",
      "val Loss: 1.5039 Acc: 0.8333 \n",
      "Epoch 4/29\n",
      "----------\n",
      "train Loss: 1.2102 Acc: 1.0000 \n",
      "val Loss: 1.5032 Acc: 0.8333 \n",
      "Epoch 5/29\n",
      "----------\n",
      "train Loss: 1.2097 Acc: 1.0000 \n",
      "val Loss: 1.5024 Acc: 0.8333 \n",
      "Epoch 6/29\n",
      "----------\n",
      "train Loss: 1.2091 Acc: 1.0000 \n",
      "val Loss: 1.5016 Acc: 0.8333 \n",
      "Epoch 7/29\n",
      "----------\n",
      "train Loss: 1.2087 Acc: 1.0000 \n",
      "val Loss: 1.5009 Acc: 0.8333 \n",
      "Epoch 8/29\n",
      "----------\n",
      "train Loss: 1.2082 Acc: 1.0000 \n",
      "val Loss: 1.5002 Acc: 0.8333 \n",
      "Epoch 9/29\n",
      "----------\n",
      "train Loss: 1.2077 Acc: 1.0000 \n",
      "val Loss: 1.4994 Acc: 0.8333 \n",
      "Epoch 10/29\n",
      "----------\n",
      "train Loss: 1.2072 Acc: 1.0000 \n",
      "val Loss: 1.4987 Acc: 0.8333 \n",
      "Epoch 11/29\n",
      "----------\n",
      "train Loss: 1.2067 Acc: 1.0000 \n",
      "val Loss: 1.4980 Acc: 0.8333 \n",
      "Epoch 12/29\n",
      "----------\n",
      "train Loss: 1.2063 Acc: 1.0000 \n",
      "val Loss: 1.4973 Acc: 0.8333 \n",
      "Epoch 13/29\n",
      "----------\n",
      "train Loss: 1.2058 Acc: 1.0000 \n",
      "val Loss: 1.4966 Acc: 0.8333 \n",
      "Epoch 14/29\n",
      "----------\n",
      "train Loss: 1.2054 Acc: 1.0000 \n",
      "val Loss: 1.4958 Acc: 0.8333 \n",
      "Epoch 15/29\n",
      "----------\n",
      "train Loss: 1.2050 Acc: 1.0000 \n",
      "val Loss: 1.4952 Acc: 0.8333 \n",
      "Epoch 16/29\n",
      "----------\n",
      "train Loss: 1.2045 Acc: 1.0000 \n",
      "val Loss: 1.4945 Acc: 0.8333 \n",
      "Epoch 17/29\n",
      "----------\n",
      "train Loss: 1.2041 Acc: 1.0000 \n",
      "val Loss: 1.4938 Acc: 0.8333 \n",
      "Epoch 18/29\n",
      "----------\n",
      "train Loss: 1.2037 Acc: 1.0000 \n",
      "val Loss: 1.4931 Acc: 0.8333 \n",
      "Epoch 19/29\n",
      "----------\n",
      "train Loss: 1.2033 Acc: 1.0000 \n",
      "val Loss: 1.4924 Acc: 0.8333 \n",
      "Epoch 20/29\n",
      "----------\n",
      "train Loss: 1.2029 Acc: 1.0000 \n",
      "val Loss: 1.4918 Acc: 0.8333 \n",
      "Epoch 21/29\n",
      "----------\n",
      "train Loss: 1.2025 Acc: 1.0000 \n",
      "val Loss: 1.4911 Acc: 0.8333 \n",
      "Epoch 22/29\n",
      "----------\n",
      "train Loss: 1.2021 Acc: 1.0000 \n",
      "val Loss: 1.4905 Acc: 0.8333 \n",
      "Epoch 23/29\n",
      "----------\n",
      "train Loss: 1.2017 Acc: 1.0000 \n",
      "val Loss: 1.4898 Acc: 0.8333 \n",
      "Epoch 24/29\n",
      "----------\n",
      "train Loss: 1.2013 Acc: 1.0000 \n",
      "val Loss: 1.4892 Acc: 0.8333 \n",
      "Epoch 25/29\n",
      "----------\n",
      "train Loss: 1.2009 Acc: 1.0000 \n",
      "val Loss: 1.4885 Acc: 0.8333 \n",
      "Epoch 26/29\n",
      "----------\n",
      "train Loss: 1.2006 Acc: 1.0000 \n",
      "val Loss: 1.4879 Acc: 0.8333 \n",
      "Epoch 27/29\n",
      "----------\n",
      "train Loss: 1.2002 Acc: 1.0000 \n",
      "val Loss: 1.4873 Acc: 0.8333 \n",
      "Epoch 28/29\n",
      "----------\n",
      "train Loss: 1.1999 Acc: 1.0000 \n",
      "val Loss: 1.4867 Acc: 0.8333 \n",
      "Epoch 29/29\n",
      "----------\n",
      "train Loss: 1.1995 Acc: 1.0000 \n",
      "val Loss: 1.4861 Acc: 0.8333 \n",
      "Training complete in 0m 6s\n",
      "Best val Acc: 0.833333\n",
      "Epoch generations ( 161 /200) :accuracy(fittest mask) tensor(1., dtype=torch.float64) keep_prob 1.0\n",
      "Epoch generations ( 162 /200) :accuracy(fittest mask) tensor(1., dtype=torch.float64) keep_prob 1.0\n",
      "Epoch generations ( 163 /200) :accuracy(fittest mask) tensor(1., dtype=torch.float64) keep_prob 1.0\n",
      "Epoch generations ( 164 /200) :accuracy(fittest mask) tensor(1., dtype=torch.float64) keep_prob 1.0\n",
      "Epoch generations ( 165 /200) :accuracy(fittest mask) tensor(1., dtype=torch.float64) keep_prob 1.0\n",
      "Epoch generations ( 166 /200) :accuracy(fittest mask) tensor(1., dtype=torch.float64) keep_prob 1.0\n",
      "Epoch generations ( 167 /200) :accuracy(fittest mask) tensor(1., dtype=torch.float64) keep_prob 1.0\n",
      "Epoch generations ( 168 /200) :accuracy(fittest mask) tensor(1., dtype=torch.float64) keep_prob 1.0\n",
      "Epoch generations ( 169 /200) :accuracy(fittest mask) tensor(1., dtype=torch.float64) keep_prob 1.0\n",
      "Epoch generations ( 170 /200) :accuracy(fittest mask) tensor(1., dtype=torch.float64) keep_prob 1.0\n",
      "Epoch 0/29\n",
      "----------\n",
      "train Loss: 1.2118 Acc: 1.0000 \n",
      "val Loss: 1.5055 Acc: 0.8333 \n",
      "Epoch 1/29\n",
      "----------\n",
      "train Loss: 1.2113 Acc: 1.0000 \n",
      "val Loss: 1.5048 Acc: 0.8333 \n",
      "Epoch 2/29\n",
      "----------\n",
      "train Loss: 1.2107 Acc: 1.0000 \n",
      "val Loss: 1.5040 Acc: 0.8333 \n",
      "Epoch 3/29\n",
      "----------\n",
      "train Loss: 1.2102 Acc: 1.0000 \n",
      "val Loss: 1.5032 Acc: 0.8333 \n",
      "Epoch 4/29\n",
      "----------\n",
      "train Loss: 1.2097 Acc: 1.0000 \n",
      "val Loss: 1.5025 Acc: 0.8333 \n",
      "Epoch 5/29\n",
      "----------\n",
      "train Loss: 1.2092 Acc: 1.0000 \n",
      "val Loss: 1.5017 Acc: 0.8333 \n",
      "Epoch 6/29\n",
      "----------\n",
      "train Loss: 1.2087 Acc: 1.0000 \n",
      "val Loss: 1.5010 Acc: 0.8333 \n",
      "Epoch 7/29\n",
      "----------\n",
      "train Loss: 1.2082 Acc: 1.0000 \n",
      "val Loss: 1.5003 Acc: 0.8333 \n",
      "Epoch 8/29\n",
      "----------\n",
      "train Loss: 1.2077 Acc: 1.0000 \n",
      "val Loss: 1.4995 Acc: 0.8333 \n",
      "Epoch 9/29\n",
      "----------\n",
      "train Loss: 1.2073 Acc: 1.0000 \n",
      "val Loss: 1.4988 Acc: 0.8333 \n",
      "Epoch 10/29\n",
      "----------\n",
      "train Loss: 1.2068 Acc: 1.0000 \n",
      "val Loss: 1.4981 Acc: 0.8333 \n",
      "Epoch 11/29\n",
      "----------\n",
      "train Loss: 1.2063 Acc: 1.0000 \n",
      "val Loss: 1.4974 Acc: 0.8333 \n",
      "Epoch 12/29\n",
      "----------\n",
      "train Loss: 1.2059 Acc: 1.0000 \n",
      "val Loss: 1.4966 Acc: 0.8333 \n",
      "Epoch 13/29\n",
      "----------\n",
      "train Loss: 1.2054 Acc: 1.0000 \n",
      "val Loss: 1.4959 Acc: 0.8333 \n",
      "Epoch 14/29\n",
      "----------\n",
      "train Loss: 1.2050 Acc: 1.0000 \n",
      "val Loss: 1.4952 Acc: 0.8333 \n",
      "Epoch 15/29\n",
      "----------\n",
      "train Loss: 1.2046 Acc: 1.0000 \n",
      "val Loss: 1.4945 Acc: 0.8333 \n",
      "Epoch 16/29\n",
      "----------\n",
      "train Loss: 1.2041 Acc: 1.0000 \n",
      "val Loss: 1.4939 Acc: 0.8333 \n",
      "Epoch 17/29\n",
      "----------\n",
      "train Loss: 1.2037 Acc: 1.0000 \n",
      "val Loss: 1.4932 Acc: 0.8333 \n",
      "Epoch 18/29\n",
      "----------\n",
      "train Loss: 1.2033 Acc: 1.0000 \n",
      "val Loss: 1.4925 Acc: 0.8333 \n",
      "Epoch 19/29\n",
      "----------\n",
      "train Loss: 1.2029 Acc: 1.0000 \n",
      "val Loss: 1.4919 Acc: 0.8333 \n",
      "Epoch 20/29\n",
      "----------\n",
      "train Loss: 1.2025 Acc: 1.0000 \n",
      "val Loss: 1.4912 Acc: 0.8333 \n",
      "Epoch 21/29\n",
      "----------\n",
      "train Loss: 1.2021 Acc: 1.0000 \n",
      "val Loss: 1.4905 Acc: 0.8333 \n",
      "Epoch 22/29\n",
      "----------\n",
      "train Loss: 1.2017 Acc: 1.0000 \n",
      "val Loss: 1.4899 Acc: 0.8333 \n",
      "Epoch 23/29\n",
      "----------\n",
      "train Loss: 1.2014 Acc: 1.0000 \n",
      "val Loss: 1.4893 Acc: 0.8333 \n",
      "Epoch 24/29\n",
      "----------\n",
      "train Loss: 1.2010 Acc: 1.0000 \n",
      "val Loss: 1.4886 Acc: 0.8333 \n",
      "Epoch 25/29\n",
      "----------\n",
      "train Loss: 1.2006 Acc: 1.0000 \n",
      "val Loss: 1.4880 Acc: 0.8333 \n",
      "Epoch 26/29\n",
      "----------\n",
      "train Loss: 1.2003 Acc: 1.0000 \n",
      "val Loss: 1.4874 Acc: 0.8333 \n",
      "Epoch 27/29\n",
      "----------\n",
      "train Loss: 1.1999 Acc: 1.0000 \n",
      "val Loss: 1.4868 Acc: 0.8333 \n",
      "Epoch 28/29\n",
      "----------\n",
      "train Loss: 1.1996 Acc: 1.0000 \n",
      "val Loss: 1.4861 Acc: 0.8333 \n",
      "Epoch 29/29\n",
      "----------\n",
      "train Loss: 1.1992 Acc: 1.0000 \n",
      "val Loss: 1.4855 Acc: 0.8333 \n",
      "Training complete in 0m 6s\n",
      "Best val Acc: 0.833333\n",
      "Epoch generations ( 171 /200) :accuracy(fittest mask) tensor(1., dtype=torch.float64) keep_prob 1.0\n",
      "Epoch generations ( 172 /200) :accuracy(fittest mask) tensor(1., dtype=torch.float64) keep_prob 1.0\n",
      "Epoch generations ( 173 /200) :accuracy(fittest mask) tensor(1., dtype=torch.float64) keep_prob 1.0\n",
      "Epoch generations ( 174 /200) :accuracy(fittest mask) tensor(1., dtype=torch.float64) keep_prob 1.0\n",
      "Epoch generations ( 175 /200) :accuracy(fittest mask) tensor(1., dtype=torch.float64) keep_prob 1.0\n",
      "Epoch generations ( 176 /200) :accuracy(fittest mask) tensor(1., dtype=torch.float64) keep_prob 1.0\n",
      "Epoch generations ( 177 /200) :accuracy(fittest mask) tensor(1., dtype=torch.float64) keep_prob 1.0\n",
      "Epoch generations ( 178 /200) :accuracy(fittest mask) tensor(1., dtype=torch.float64) keep_prob 1.0\n",
      "Epoch generations ( 179 /200) :accuracy(fittest mask) tensor(1., dtype=torch.float64) keep_prob 1.0\n",
      "Epoch generations ( 180 /200) :accuracy(fittest mask) tensor(1., dtype=torch.float64) keep_prob 1.0\n",
      "Epoch 0/29\n",
      "----------\n",
      "train Loss: 1.2113 Acc: 1.0000 \n",
      "val Loss: 1.5048 Acc: 0.8333 \n",
      "Epoch 1/29\n",
      "----------\n",
      "train Loss: 1.2108 Acc: 1.0000 \n",
      "val Loss: 1.5041 Acc: 0.8333 \n",
      "Epoch 2/29\n",
      "----------\n",
      "train Loss: 1.2103 Acc: 1.0000 \n",
      "val Loss: 1.5033 Acc: 0.8333 \n",
      "Epoch 3/29\n",
      "----------\n",
      "train Loss: 1.2098 Acc: 1.0000 \n",
      "val Loss: 1.5026 Acc: 0.8333 \n",
      "Epoch 4/29\n",
      "----------\n",
      "train Loss: 1.2093 Acc: 1.0000 \n",
      "val Loss: 1.5018 Acc: 0.8333 \n",
      "Epoch 5/29\n",
      "----------\n",
      "train Loss: 1.2088 Acc: 1.0000 \n",
      "val Loss: 1.5011 Acc: 0.8333 \n",
      "Epoch 6/29\n",
      "----------\n",
      "train Loss: 1.2083 Acc: 1.0000 \n",
      "val Loss: 1.5003 Acc: 0.8333 \n",
      "Epoch 7/29\n",
      "----------\n",
      "train Loss: 1.2078 Acc: 1.0000 \n",
      "val Loss: 1.4996 Acc: 0.8333 \n",
      "Epoch 8/29\n",
      "----------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train Loss: 1.2073 Acc: 1.0000 \n",
      "val Loss: 1.4989 Acc: 0.8333 \n",
      "Epoch 9/29\n",
      "----------\n",
      "train Loss: 1.2069 Acc: 1.0000 \n",
      "val Loss: 1.4982 Acc: 0.8333 \n",
      "Epoch 10/29\n",
      "----------\n",
      "train Loss: 1.2064 Acc: 1.0000 \n",
      "val Loss: 1.4974 Acc: 0.8333 \n",
      "Epoch 11/29\n",
      "----------\n",
      "train Loss: 1.2059 Acc: 1.0000 \n",
      "val Loss: 1.4967 Acc: 0.8333 \n",
      "Epoch 12/29\n",
      "----------\n",
      "train Loss: 1.2055 Acc: 1.0000 \n",
      "val Loss: 1.4960 Acc: 0.8333 \n",
      "Epoch 13/29\n",
      "----------\n",
      "train Loss: 1.2051 Acc: 1.0000 \n",
      "val Loss: 1.4953 Acc: 0.8333 \n",
      "Epoch 14/29\n",
      "----------\n",
      "train Loss: 1.2046 Acc: 1.0000 \n",
      "val Loss: 1.4946 Acc: 0.8333 \n",
      "Epoch 15/29\n",
      "----------\n",
      "train Loss: 1.2042 Acc: 1.0000 \n",
      "val Loss: 1.4940 Acc: 0.8333 \n",
      "Epoch 16/29\n",
      "----------\n",
      "train Loss: 1.2038 Acc: 1.0000 \n",
      "val Loss: 1.4933 Acc: 0.8333 \n",
      "Epoch 17/29\n",
      "----------\n",
      "train Loss: 1.2034 Acc: 1.0000 \n",
      "val Loss: 1.4926 Acc: 0.8333 \n",
      "Epoch 18/29\n",
      "----------\n",
      "train Loss: 1.2030 Acc: 1.0000 \n",
      "val Loss: 1.4919 Acc: 0.8333 \n",
      "Epoch 19/29\n",
      "----------\n",
      "train Loss: 1.2026 Acc: 1.0000 \n",
      "val Loss: 1.4913 Acc: 0.8333 \n",
      "Epoch 20/29\n",
      "----------\n",
      "train Loss: 1.2022 Acc: 1.0000 \n",
      "val Loss: 1.4906 Acc: 0.8333 \n",
      "Epoch 21/29\n",
      "----------\n",
      "train Loss: 1.2018 Acc: 1.0000 \n",
      "val Loss: 1.4900 Acc: 0.8333 \n",
      "Epoch 22/29\n",
      "----------\n",
      "train Loss: 1.2014 Acc: 1.0000 \n",
      "val Loss: 1.4893 Acc: 0.8333 \n",
      "Epoch 23/29\n",
      "----------\n",
      "train Loss: 1.2010 Acc: 1.0000 \n",
      "val Loss: 1.4887 Acc: 0.8333 \n",
      "Epoch 24/29\n",
      "----------\n",
      "train Loss: 1.2007 Acc: 1.0000 \n",
      "val Loss: 1.4881 Acc: 0.8333 \n",
      "Epoch 25/29\n",
      "----------\n",
      "train Loss: 1.2003 Acc: 1.0000 \n",
      "val Loss: 1.4874 Acc: 0.8333 \n",
      "Epoch 26/29\n",
      "----------\n",
      "train Loss: 1.1999 Acc: 1.0000 \n",
      "val Loss: 1.4868 Acc: 0.8333 \n",
      "Epoch 27/29\n",
      "----------\n",
      "train Loss: 1.1996 Acc: 1.0000 \n",
      "val Loss: 1.4862 Acc: 0.8333 \n",
      "Epoch 28/29\n",
      "----------\n",
      "train Loss: 1.1992 Acc: 1.0000 \n",
      "val Loss: 1.4856 Acc: 0.8333 \n",
      "Epoch 29/29\n",
      "----------\n",
      "train Loss: 1.1989 Acc: 1.0000 \n",
      "val Loss: 1.4850 Acc: 0.8333 \n",
      "Training complete in 0m 6s\n",
      "Best val Acc: 0.833333\n",
      "Epoch generations ( 181 /200) :accuracy(fittest mask) tensor(1., dtype=torch.float64) keep_prob 1.0\n",
      "Epoch generations ( 182 /200) :accuracy(fittest mask) tensor(1., dtype=torch.float64) keep_prob 1.0\n",
      "Epoch generations ( 183 /200) :accuracy(fittest mask) tensor(1., dtype=torch.float64) keep_prob 1.0\n",
      "Epoch generations ( 184 /200) :accuracy(fittest mask) tensor(1., dtype=torch.float64) keep_prob 1.0\n",
      "Epoch generations ( 185 /200) :accuracy(fittest mask) tensor(1., dtype=torch.float64) keep_prob 1.0\n",
      "Epoch generations ( 186 /200) :accuracy(fittest mask) tensor(1., dtype=torch.float64) keep_prob 1.0\n",
      "Epoch generations ( 187 /200) :accuracy(fittest mask) tensor(1., dtype=torch.float64) keep_prob 1.0\n",
      "Epoch generations ( 188 /200) :accuracy(fittest mask) tensor(1., dtype=torch.float64) keep_prob 1.0\n",
      "Epoch generations ( 189 /200) :accuracy(fittest mask) tensor(1., dtype=torch.float64) keep_prob 1.0\n",
      "Epoch generations ( 190 /200) :accuracy(fittest mask) tensor(1., dtype=torch.float64) keep_prob 1.0\n",
      "Epoch 0/29\n",
      "----------\n",
      "train Loss: 1.2108 Acc: 1.0000 \n",
      "val Loss: 1.5041 Acc: 0.8333 \n",
      "Epoch 1/29\n",
      "----------\n",
      "train Loss: 1.2103 Acc: 1.0000 \n",
      "val Loss: 1.5034 Acc: 0.8333 \n",
      "Epoch 2/29\n",
      "----------\n",
      "train Loss: 1.2098 Acc: 1.0000 \n",
      "val Loss: 1.5027 Acc: 0.8333 \n",
      "Epoch 3/29\n",
      "----------\n",
      "train Loss: 1.2093 Acc: 1.0000 \n",
      "val Loss: 1.5019 Acc: 0.8333 \n",
      "Epoch 4/29\n",
      "----------\n",
      "train Loss: 1.2088 Acc: 1.0000 \n",
      "val Loss: 1.5012 Acc: 0.8333 \n",
      "Epoch 5/29\n",
      "----------\n",
      "train Loss: 1.2083 Acc: 1.0000 \n",
      "val Loss: 1.5004 Acc: 0.8333 \n",
      "Epoch 6/29\n",
      "----------\n",
      "train Loss: 1.2079 Acc: 1.0000 \n",
      "val Loss: 1.4997 Acc: 0.8333 \n",
      "Epoch 7/29\n",
      "----------\n",
      "train Loss: 1.2074 Acc: 1.0000 \n",
      "val Loss: 1.4990 Acc: 0.8333 \n",
      "Epoch 8/29\n",
      "----------\n",
      "train Loss: 1.2069 Acc: 1.0000 \n",
      "val Loss: 1.4983 Acc: 0.8333 \n",
      "Epoch 9/29\n",
      "----------\n",
      "train Loss: 1.2065 Acc: 1.0000 \n",
      "val Loss: 1.4975 Acc: 0.8333 \n",
      "Epoch 10/29\n",
      "----------\n",
      "train Loss: 1.2060 Acc: 1.0000 \n",
      "val Loss: 1.4968 Acc: 0.8333 \n",
      "Epoch 11/29\n",
      "----------\n",
      "train Loss: 1.2056 Acc: 1.0000 \n",
      "val Loss: 1.4961 Acc: 0.8333 \n",
      "Epoch 12/29\n",
      "----------\n",
      "train Loss: 1.2051 Acc: 1.0000 \n",
      "val Loss: 1.4954 Acc: 0.8333 \n",
      "Epoch 13/29\n",
      "----------\n",
      "train Loss: 1.2047 Acc: 1.0000 \n",
      "val Loss: 1.4947 Acc: 0.8333 \n",
      "Epoch 14/29\n",
      "----------\n",
      "train Loss: 1.2043 Acc: 1.0000 \n",
      "val Loss: 1.4940 Acc: 0.8333 \n",
      "Epoch 15/29\n",
      "----------\n",
      "train Loss: 1.2038 Acc: 1.0000 \n",
      "val Loss: 1.4934 Acc: 0.8333 \n",
      "Epoch 16/29\n",
      "----------\n",
      "train Loss: 1.2034 Acc: 1.0000 \n",
      "val Loss: 1.4927 Acc: 0.8333 \n",
      "Epoch 17/29\n",
      "----------\n",
      "train Loss: 1.2030 Acc: 1.0000 \n",
      "val Loss: 1.4920 Acc: 0.8333 \n",
      "Epoch 18/29\n",
      "----------\n",
      "train Loss: 1.2026 Acc: 1.0000 \n",
      "val Loss: 1.4914 Acc: 0.8333 \n",
      "Epoch 19/29\n",
      "----------\n",
      "train Loss: 1.2022 Acc: 1.0000 \n",
      "val Loss: 1.4907 Acc: 0.8333 \n",
      "Epoch 20/29\n",
      "----------\n",
      "train Loss: 1.2018 Acc: 1.0000 \n",
      "val Loss: 1.4901 Acc: 0.8333 \n",
      "Epoch 21/29\n",
      "----------\n",
      "train Loss: 1.2015 Acc: 1.0000 \n",
      "val Loss: 1.4894 Acc: 0.8333 \n",
      "Epoch 22/29\n",
      "----------\n",
      "train Loss: 1.2011 Acc: 1.0000 \n",
      "val Loss: 1.4888 Acc: 0.8333 \n",
      "Epoch 23/29\n",
      "----------\n",
      "train Loss: 1.2007 Acc: 1.0000 \n",
      "val Loss: 1.4882 Acc: 0.8333 \n",
      "Epoch 24/29\n",
      "----------\n",
      "train Loss: 1.2003 Acc: 1.0000 \n",
      "val Loss: 1.4875 Acc: 0.8333 \n",
      "Epoch 25/29\n",
      "----------\n",
      "train Loss: 1.2000 Acc: 1.0000 \n",
      "val Loss: 1.4869 Acc: 0.8333 \n",
      "Epoch 26/29\n",
      "----------\n",
      "train Loss: 1.1996 Acc: 1.0000 \n",
      "val Loss: 1.4863 Acc: 0.8333 \n",
      "Epoch 27/29\n",
      "----------\n",
      "train Loss: 1.1993 Acc: 1.0000 \n",
      "val Loss: 1.4857 Acc: 0.8333 \n",
      "Epoch 28/29\n",
      "----------\n",
      "train Loss: 1.1990 Acc: 1.0000 \n",
      "val Loss: 1.4851 Acc: 0.8333 \n",
      "Epoch 29/29\n",
      "----------\n",
      "train Loss: 1.1986 Acc: 1.0000 \n",
      "val Loss: 1.4845 Acc: 0.8333 \n",
      "Training complete in 0m 6s\n",
      "Best val Acc: 0.833333\n",
      "Epoch generations ( 191 /200) :accuracy(fittest mask) tensor(1., dtype=torch.float64) keep_prob 1.0\n",
      "Epoch generations ( 192 /200) :accuracy(fittest mask) tensor(1., dtype=torch.float64) keep_prob 1.0\n",
      "Epoch generations ( 193 /200) :accuracy(fittest mask) tensor(1., dtype=torch.float64) keep_prob 1.0\n",
      "Epoch generations ( 194 /200) :accuracy(fittest mask) tensor(1., dtype=torch.float64) keep_prob 1.0\n",
      "Epoch generations ( 195 /200) :accuracy(fittest mask) tensor(1., dtype=torch.float64) keep_prob 1.0\n",
      "Epoch generations ( 196 /200) :accuracy(fittest mask) tensor(1., dtype=torch.float64) keep_prob 1.0\n",
      "Epoch generations ( 197 /200) :accuracy(fittest mask) tensor(1., dtype=torch.float64) keep_prob 1.0\n",
      "Epoch generations ( 198 /200) :accuracy(fittest mask) tensor(1., dtype=torch.float64) keep_prob 1.0\n",
      "Epoch generations ( 199 /200) :accuracy(fittest mask) tensor(1., dtype=torch.float64) keep_prob 1.0\n",
      "Epoch generations ( 200 /200) :accuracy(fittest mask) tensor(1., dtype=torch.float64) keep_prob 1.0\n",
      "Epoch 0/29\n",
      "----------\n",
      "train Loss: 1.2104 Acc: 1.0000 \n",
      "val Loss: 1.5034 Acc: 0.8333 \n",
      "Epoch 1/29\n",
      "----------\n",
      "train Loss: 1.2099 Acc: 1.0000 \n",
      "val Loss: 1.5027 Acc: 0.8333 \n",
      "Epoch 2/29\n",
      "----------\n",
      "train Loss: 1.2094 Acc: 1.0000 \n",
      "val Loss: 1.5020 Acc: 0.8333 \n",
      "Epoch 3/29\n",
      "----------\n",
      "train Loss: 1.2089 Acc: 1.0000 \n",
      "val Loss: 1.5013 Acc: 0.8333 \n",
      "Epoch 4/29\n",
      "----------\n",
      "train Loss: 1.2084 Acc: 1.0000 \n",
      "val Loss: 1.5005 Acc: 0.8333 \n",
      "Epoch 5/29\n",
      "----------\n",
      "train Loss: 1.2079 Acc: 1.0000 \n",
      "val Loss: 1.4998 Acc: 0.8333 \n",
      "Epoch 6/29\n",
      "----------\n",
      "train Loss: 1.2074 Acc: 1.0000 \n",
      "val Loss: 1.4991 Acc: 0.8333 \n",
      "Epoch 7/29\n",
      "----------\n",
      "train Loss: 1.2070 Acc: 1.0000 \n",
      "val Loss: 1.4983 Acc: 0.8333 \n",
      "Epoch 8/29\n",
      "----------\n",
      "train Loss: 1.2065 Acc: 1.0000 \n",
      "val Loss: 1.4976 Acc: 0.8333 \n",
      "Epoch 9/29\n",
      "----------\n",
      "train Loss: 1.2061 Acc: 1.0000 \n",
      "val Loss: 1.4969 Acc: 0.8333 \n",
      "Epoch 10/29\n",
      "----------\n",
      "train Loss: 1.2056 Acc: 1.0000 \n",
      "val Loss: 1.4962 Acc: 0.8333 \n",
      "Epoch 11/29\n",
      "----------\n",
      "train Loss: 1.2052 Acc: 1.0000 \n",
      "val Loss: 1.4955 Acc: 0.8333 \n",
      "Epoch 12/29\n",
      "----------\n",
      "train Loss: 1.2047 Acc: 1.0000 \n",
      "val Loss: 1.4948 Acc: 0.8333 \n",
      "Epoch 13/29\n",
      "----------\n",
      "train Loss: 1.2043 Acc: 1.0000 \n",
      "val Loss: 1.4941 Acc: 0.8333 \n",
      "Epoch 14/29\n",
      "----------\n",
      "train Loss: 1.2039 Acc: 1.0000 \n",
      "val Loss: 1.4935 Acc: 0.8333 \n",
      "Epoch 15/29\n",
      "----------\n",
      "train Loss: 1.2035 Acc: 1.0000 \n",
      "val Loss: 1.4928 Acc: 0.8333 \n",
      "Epoch 16/29\n",
      "----------\n",
      "train Loss: 1.2031 Acc: 1.0000 \n",
      "val Loss: 1.4921 Acc: 0.8333 \n",
      "Epoch 17/29\n",
      "----------\n",
      "train Loss: 1.2027 Acc: 1.0000 \n",
      "val Loss: 1.4914 Acc: 0.8333 \n",
      "Epoch 18/29\n",
      "----------\n",
      "train Loss: 1.2023 Acc: 1.0000 \n",
      "val Loss: 1.4908 Acc: 0.8333 \n",
      "Epoch 19/29\n",
      "----------\n",
      "train Loss: 1.2019 Acc: 1.0000 \n",
      "val Loss: 1.4901 Acc: 0.8333 \n",
      "Epoch 20/29\n",
      "----------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train Loss: 1.2015 Acc: 1.0000 \n",
      "val Loss: 1.4895 Acc: 0.8333 \n",
      "Epoch 21/29\n",
      "----------\n",
      "train Loss: 1.2011 Acc: 1.0000 \n",
      "val Loss: 1.4889 Acc: 0.8333 \n",
      "Epoch 22/29\n",
      "----------\n",
      "train Loss: 1.2008 Acc: 1.0000 \n",
      "val Loss: 1.4882 Acc: 0.8333 \n",
      "Epoch 23/29\n",
      "----------\n",
      "train Loss: 1.2004 Acc: 1.0000 \n",
      "val Loss: 1.4876 Acc: 0.8333 \n",
      "Epoch 24/29\n",
      "----------\n",
      "train Loss: 1.2000 Acc: 1.0000 \n",
      "val Loss: 1.4870 Acc: 0.8333 \n",
      "Epoch 25/29\n",
      "----------\n",
      "train Loss: 1.1997 Acc: 1.0000 \n",
      "val Loss: 1.4864 Acc: 0.8333 \n",
      "Epoch 26/29\n",
      "----------\n",
      "train Loss: 1.1993 Acc: 1.0000 \n",
      "val Loss: 1.4858 Acc: 0.8333 \n",
      "Epoch 27/29\n",
      "----------\n",
      "train Loss: 1.1990 Acc: 1.0000 \n",
      "val Loss: 1.4852 Acc: 0.8333 \n",
      "Epoch 28/29\n",
      "----------\n",
      "train Loss: 1.1987 Acc: 1.0000 \n",
      "val Loss: 1.4846 Acc: 0.8333 \n",
      "Epoch 29/29\n",
      "----------\n",
      "train Loss: 1.1983 Acc: 1.0000 \n",
      "val Loss: 1.4840 Acc: 0.8333 \n",
      "Training complete in 0m 7s\n",
      "Best val Acc: 0.833333\n"
     ]
    }
   ],
   "source": [
    "#control block controls the epochs and the generations of mask\n",
    "#step 1 an object of the population class randomly generating the first population \n",
    "#step2 :calculate fitness of each entitiy of the population \n",
    "#step3: creates a mating pool of the population based on the worst two performing parent \n",
    "#step 4 :fittest mask of the generating along with keep_prob found \n",
    "#step 5: if 0th ,10th ,20th, the epochs starts training on the worst performing mask /other wise new generation is created \n",
    "\n",
    "epochgens=0\n",
    "population =Population(mutation_rate,max_population,maskLength)\n",
    "total_acc=[]\n",
    "while (epochgens<=200):\n",
    "    print ('Epoch generations (',epochgens,'/200)',end=' :')\n",
    "    population .calcFitness(model)\n",
    "    population.naturalSelection()\n",
    "    fittestmask,p=population .fittest()\n",
    "    accuracy=fittestmask.fitness(model)\n",
    "    print (\"accuracy(fittest mask)\",accuracy,\"keep_prob\",p,end='\\n')\n",
    "    if (epochgens%10==0):\n",
    "        model,losses,accuracies=train_model(model,criterion,optimizer,fittestmask.gene,p,30)\n",
    "        total_acc=total_acc+accuracies\n",
    "    population.generate()\n",
    "    epochgens+=1\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYIAAAEWCAYAAABrDZDcAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4xLjEsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy8QZhcZAAAgAElEQVR4nO3deXxdVbn/8c+TuWnTdEgHOrcQSstUaigg81QLckEUlToiCA5MKleFn4o4XOUKV/z5u4gXvKg4MKgoFdAyz1MDLQiFzk1bWmiadEozJ8/vj70TTpKT5rTNzsnJ/r5fr/PKns+z09PzZK2111rm7oiISHxlpTsAERFJLyUCEZGYUyIQEYk5JQIRkZhTIhARiTklAhGRmFMiEMkgZjbFzNzMctIdiwwcSgQiIjGnRCAS0l/ZEldKBDLgmdlaM7vGzJaa2VYz+7WZFZjZSWa2wcy+aWbvAL8Oj7/YzFaaWbWZLTCzcQnXcjO7wsxWm9kWM7vBzLLCfVlm9m0zqzCzzWZ2h5kVh/sKzOz3ZlZlZtvMbJGZjQn3FZvZ/5rZJjN728x+aGbZ4b5sM7sxfK/VwAf7+vcnA58SgcTFJ4EPAPsDBwLfDrePBUYAk4FLzOwU4MfAx4D9gArgrk7XOhcoA2YD5wAXhtsvCF8nA9OAIcB/h/s+CxQDE4GRwBeBunDfb4Fm4ADgCGAu8Plw38XAWeH2MuC8vbx/ke65u156DegXsBb4YsL6mcAq4CSgEShI2Pe/wE8S1ocATcCUcN2BeQn7vww8Gi4/Cnw5Yd/08NwcgmTxHHBYp9jGAA3AoIRt84HHw+XHOsU+N4whJ92/V70Gzkt1ohIX6xOWK4C26p5Kd69P2DcOeKVtxd1rzKwKGE+QUHZ3rXHheuK+HIIv+98RlAbuMrNhwO+BbxGURHKBTWbWdl5WwnuMS/J+Ir1KiUDiYmLC8iRgY7jcefjdjQRfzgCY2WCCqpy3O13rjSTX6nBuuK8ZeNfdm4HvAd8zsynAg8Cy8GcDUBIe09mmJLGL9Cq1EUhcXGpmE8xsBPB/gLu7Oe6PwOfMbJaZ5QM/Al5097UJx3zdzIab2UTgyoRr3Ql81cymmtmQ8Ny73b3ZzE42s0PDRuAdBFVGLe6+CXgI+C8zGxo2OO9vZieG17wHuCKMfThwdW/9QkTaKBFIXPyR4At3dfj6YbKD3P1R4DvAXwj+Gt8fOL/TYfcBLwNLgAcI2hUAbieoAnoKWAPUA5eH+8YCfyZIAm8CTxJUDwF8BsgDlgJbw+P2C/fdBiwEXiWosrp3D+9bpEfmrolpZGAzs7XA5939kV64lgOl7r5ynwMT6SdUIhARiTklAhGRmFPVkIhIzKlEICIScxnXj6CkpMSnTJmS7jBERDLKyy+/vMXdRyXbl3GJYMqUKZSXl6c7DBGRjGJm3fZKV9WQiEjMKRGIiMScEoGISMwpEYiIxJwSgYhIzEWWCMzs9nC6vte72W9m9vNwSsDXzGx2VLGIiEj3oiwR/AaYt5v9ZwCl4esS4JYIYxERkW5E1o/A3Z8KJ+DozjnAHR6McfGCmQ0zs/3C8dlFZC+0tjq3P7uGHXVN5Odmc9FxU1m0tppFa6rTHZr0glNnjOHwicN6/brp7FA2no5T8G0It3VJBGZ2CUGpgUmTNEGTSHeeX13FDx94s319dFE+P1m4jMqdDbw3E6ZkqtFDCwZcIkj2sUw6Ap673wrcClBWVqZR8iQyzS2tfOPPr7Fpe33S/eceMZ6PHTmxw7bytdX87JEVtLR2/9EszMvmJ+cdxsgh+QC8sXE71//jLZpbevfjvGl7HbnZxuJr53LSDU/wn/9cxpaaBm447zA+Wjax5wtILKXzqaENdJyLdQLvzf0qkhZL1m/j3sVvs62uiZZW7/BaumkHdy1a1+WcO56v4JV1W7sc3/ZqaG7h0bc288833mk/586X1vHimupuz9nb1+iiAr504v4Myc/h8lMOYFrJYE6ePorTZ47py1+jZJhIh6EO2wjud/dDkuz7IHAZcCZwFPBzd5/T0zXLyspcYw1Jb3lk6btcducr7X/Nt/1c/J25FBfmdjj2yrsWs3jdNg4cU8STyze3b29qcc49Yjw3fXxW0vdwd469/jE27agnJ8vazznloNHcfsGRUdyWSBdm9rK7lyXbF1nVkJndCZwElJjZBuC7QC6Au/8SeJAgCawEaoHPRRWLSHcWvLqRQbnZzJ/zXtvTgWOKuiQBgJIh+ayrrmX91lqOO6CEQ8cXA5BlxkfeN6Hb9zAzfnLe4Ty3akvCNvi3w8f14p2I7L0onxqa38N+By6N6v1FetLa6jyzcgsnTx/NN+Yd1OPxJWH9vjtcNXc6s/ag0e640hKOKy3Z61hFopRxw1CL7KnWVuee8vXUNDR32F69q5HqXY0cf2BqX9CjivLbl9tKAyIDgRKBDHhPr9zC1ff+K+m+ovwcji9NOldHF9PHFJFl8KFZ48nO0rOYMnAoEciA9NzKLfz2+bW4w5otu8jLyeK5q08hP6fjg3J5OVnk52SndM1DJxSz9PvzulxDJNMpEciA9IsnVrF43VYmjigkO8u48Nip7XX8+6IgN7WkIZJJlAhkwKlvauGltdV8+ujJfOesmekOR6TfUxlXBpx11bU0Nrdy2AQ16IqkQolABpyKqloAJo8cnOZIRDKDEoEMOBVVuwCYPKIwzZGIZAYlAhlw3npnJ8MLcxmWpHewiHSlxmIZMH755CpWbq7h4aXvctwBJZjGXRZJiRKBDAjvbK/n+n+8xfDCXIYOyuG8su7H/hGRjpQIZEB4ekUlAH+8+Ghm7Dc0zdGIZBa1EUjGq29q4bG3NlMyJJ+DxhalOxyRjKMSgWQ0d+fMnz/N6spdfHj2eLULiOwFJQLJKOura/nX29vb16t2NbK6chfz50zkylMPTGNkIplLiUAyymV/fIVXN2zvsC0ny7j8lFLGFhekKSqRzKZEIP1Wa6tzy5Or2FLTAAQTwrz29nYuPHYqH0+YQH7ooBz2Kx6UrjBFMp4SgfRbr27Yxg0Ll1GYl90+/n/JkHzmz5lI6Rg1Cov0FiUC6ZeaWlr5/G/LAXjmm6cwYnBemiMSGbj0+Kj0Sy9XbKVqVyNzZ45REhCJmBKB9EtPr6gkO8v4r48dnu5QRAY8VQ1Jn6vc2cCyd3bu9phHlm5m9qRhFBVo4DiRqCkRSJ/70u9fprxia4/HfWPe9D6IRkSUCCRyy97ZyQP/2gQEPYFfWbeV+XMm8eHZ47s9J8uMQ8drhjGRvqBEIJH70YNv8uTyyvb1gtwsPn30ZGaO0+BwIv2BEoFE6pGl7/Lk8koueP8Urjv74HSHIyJJ6KkhidSvnlkNwMfKJvZwpIikixKBRGZXQzMvV2zlCydOUzWQSD+mRCCRuXfx2zS1OCeUjkp3KCKyG0oEEom6xha+87fXAXjf5OFpjkZEdkeJQCJRUb0LgCtOLaUgNzvN0YjI7igRSCQqqmoBOG3G6DRHIiI9USKQSKwLE8HkEYPTHImI9ESJQCKxZP029isuoLhQYwWJ9HdKBNLrWlqdZ1Zu4fjSknSHIiIpUCKQXuXuPP7WZrbXNXGcHhsVyQhKBNKr7ni+gs/fUU52lnHs/iPTHY6IpECJQHpNTUMzNz2ynMkjC7nrkqMZOSQ/3SGJSAoiTQRmNs/MlpnZSjO7Osn+SWb2uJktNrPXzOzMKOORaP36mTVsq23irMP248gpI9IdjoikKLLRR80sG7gZOB3YACwyswXuvjThsG8D97j7LWY2E3gQmBJVTNL7ttQ0cNPDy2lsbuW5VVVMHlnIVadrQhmRTBLlMNRzgJXuvhrAzO4CzgESE4EDbaORFQMbI4xHInDvKxv4w4vrGFdcgJnxpRP3JyvL0h2WiOyBKBPBeGB9wvoG4KhOx1wHPGRmlwODgdMijEd6WUXVLn704FtMH1PEwq+ekO5wRGQvRdlGkOzPQu+0Ph/4jbtPAM4EfmdmXWIys0vMrNzMyisrKzvvljR5YXUVAJ86elKaIxGRfRFlItgAJM5GMoGuVT8XAfcAuPvzQAHQpReSu9/q7mXuXjZqlJ5N7y8qqmrJyTLmz1EiEMlkUSaCRUCpmU01szzgfGBBp2PWAacCmNkMgkSgP/kzREV1LROGDyInW08hi2SyyP4Hu3szcBmwEHiT4OmgN8zs+2Z2dnjYVcDFZvYqcCdwgbt3rj6Sfmr5OzuZWqJB5UQyXaST17v7gwSPhCZuuzZheSlwbJQxSDTuW/I2KzbX8NGyCekORUT2kcr0sld++vByAD5w8Ng0RyIi+0qJQPZYRdUuKqpque7fZjJ5pKqGRDKdEoHsEXfnyeVBe/4JB+oJLpGBINI2Ahl4vrvgDe54voLxwwapoVhkgFAikJS1tjr3v7aJ2ZOGcc2ZMzDTUBIiA4GqhiRlSzftoHpXI585ZopGFxUZQJQIJCXuzrX3vQ7AsQdoCkqRgUSJQFLycsVWXlm3jQPHDGFUkSacERlI1EYgPXrsrXf5jwfeJMvgT194f7rDEZFephKB9OjWp1ZTubOBi0+YRnFhbrrDEZFepkQgu/W759fywupq5s+ZxDVnzEh3OCISAVUNxZy7s2JzDfVNLRQPymXyyMFs3lnPO9vrAbi7PJhb6HPHTk1nmCISISWCmHt+dRWfuO1FAMzgmW+ewrk3P8vmnQ3tx3z1tAMZW1yQrhBFJGJKBDH31qadQPBlf9Mjy3lpTRWbdzbwyaMmccpBo8nKMo6ZNjLNUYpIlNRGEHPrqmsZkp/D+XOCyeSeXr4FgJOnj+bUGWM4efpoCnKz0xmiiERMJYKYuv+1jdz/6iaWrN/GpBGFjC7KpyA3i4eXvgvA5JGFaY5QRPqKEkFM3bBwGdtqmxg7tIAPzx6PmXH+kZN4flUVY4oLmKIB5URiQ4kghv7roWVUVNXyvbMP5rPvn9K+/bqzD05fUCKSNmojiKGnVwTtAOfMGpfmSESkP1AiiKF11bXMnzOJYYV56Q5FRPoBVQ3FwDMrtrBxex0AzS1O9a5GNQaLSDslggHu3R31fPr2F3HvuP2QccXpCUhE+h0lggHkT+XrWbS2mo/MnsBR00ay/N2d/McDb+IOv7/oKKaUBKWAvJwsRhepp7CIBJQIBojG5la+u+ANahtbqKiq5e4vHMNtT63m2ZVbOL60hGMPGKmpJUUkKSWCDPY/T67i7kXBoHCNLa3UNraw/6jBLFpbzSk3PsGGbXXMPXgMv/jk+9IcqYj0Z0oEGWzBqxupa2qhLJw/+MQDR3HB+6dwyxOraGp1Dp1QzIUaNVREepBSIjCzvwC3A/9w99ZoQ5JUuDsVVbV8ZPZ4vnfOIR32/fTjs9IUlYhkolRLBLcAnwN+bmZ/An7j7m9FF5Z0p3pXIy+tqaKuqYWahmYmjdRQECKyb1JKBO7+CPCImRUD84GHzWw9cBvwe3dvijBGSXD9P97knvIN7eszxhalMRoRGQhSbiMws5HAp4BPA4uBPwDHAZ8FTooiOOlq5eYaZk0cxo8/fCgFudlMUccwEdlHqbYR3AscBPwO+Dd33xTuutvMyqMKTrpaV13LaTPGMGO/oekORUQGiFRLBP/t7o8l2+HuZb0Yj+xGTUMzW2oamaRSgIj0olQHnZthZsPaVsxsuJl9OaKYpJPWVqexuZWX1lQBcLCGhxCRXpRqieBid7+5bcXdt5rZxcAvoglL2rS0Oifd+Djrq4NB4/JyspgT9hsQEekNqSaCLDMz92DoMjPLBgb0GMYtrc4r67bS1NzKqKJ8Ssek5+mcTdvrWF9dx5mHjuXgccUcNLaIQXmaQ1hEek+qiWAhcI+Z/RJw4IvAPyOLqh+495UNfP3PrwGQk2Us+e5chuT3fUfsdVW1AHzqqMm8/4CSPn9/ERn4Um0j+CbwGPAl4FLgUeAbUQXVHzy+bDNjhxZwyQnTaG51auqb0xJHRXWQCNRALCJRSbVDWStB7+Jbog2nf2hpdZ5ZsYV5h4xlelgl1NicnpE13ti4ncF52exXPCgt7y8iA1+q/QhKgR8DM4H2gezdfVpEcaXVD+5fyo76Zo4vHUXbyM0NzS1pieXpFVs4Zv+RZGdpCGkRiUaqVUO/JigNNAMnA3cQdC7bLTObZ2bLzGylmV3dzTEfM7OlZvaGmf0x1cCj0tjcyj3l68nLzuKk6aPIzwkaZhvSUCKoqNpFRVUtx5eO6vP3FpH4SDURDHL3RwFz9wp3vw44ZXcnhE8W3QycQVCSmG9mMzsdUwpcAxzr7gcDX9nD+HvdF35XTm1jC//vE0dQVJBLfk7wK0pHieDpFVsAOL5UjcQiEp1UH4OpN7MsYIWZXQa8DYzu4Zw5wEp3Xw1gZncB5wBLE465GLjZ3bcCuPvmPQm+t9U1tvDsqiomjSjk5OnB7eW1JYKmvisRVNU0sHFbPQvfeIfxwwYxtUQjjIpIdFJNBF8BCoErgB8QVA99todzxgPrE9Y3AEd1OuZAADN7FsgGrnP3Lo+lmtklwCUAkyZNSjHkPbdk/TYam1v53tkHtyeA90oEfZMI3J0P/eLZ9g5knzxqkqaYFJFI9ZgIwiqej7n714EagnkJUpHs28uTvH8pweilE4CnzewQd9/W4ST3W4FbAcrKyjpfo9e8syP48k18VLO7NoK/v7qRUUX5HD1tZLfXW7S2mnHDBjF+WM9P/Ly2YRvla7eyo76J9dV1XHz8VI6eNpIjp6oXsYhEq8dE4O4tZva+xJ7FKdoATExYnwBsTHLMC+F8BmvMbBlBYli0B+/Ta7bsbARgVFF++7b83K5tBOura7n8zsUArPiPM8jN7trUUt/Uwkd/+TwlQ/Io//bpPb73FXcuZm3YeWxQbjYXHTeNscUFPZwlIrLvUq0aWgzcF85Otqtto7vfu5tzFgGlZjaVoE3hfOATnY75G8FEN78xsxKCqqLVKcbU67bUNJCXk0VRQg/ivOyuVUNrtrT/Crjot+UMzsvm5OmjOfmg0fzowTepb2phZ9gBbUtNIz+8fynf+uCMbqt4bn9mDWuravk/Zx7Ex8smkZ+bRUGuhpEQkb6RaiIYAVTR8UkhB7pNBO7eHDYsLySo/7/d3d8ws+8D5e6+INw318yWAi3A1929ai/uo1dU7mxg1JD8Dl/Y75UI3ksEbb19p4ws5J3tdWypaeSF1VVsr2vir4vfpnT0EMyChubhhbn86pk1nDt7fNJRQ92dW58Kct85s8ZTXJgb5S2KiHSRas/iVNsFOp/3IPBgp23XJiw78LXwlXaVNQ2UDOk4ll5bG0FdYzPn3fIca6tqqW1sJj8ni8euOomsLONvi9/mK3cv4YaHljFt1GAe/tqJ7ee/u6Oeo370KJ+47UWu//ChbK1t4qZHlvNeJZuzpaaR6z98KGOGqipIRPpeqj2Lf03Xhl7c/cJejyiNttU2JUkEQYng8bcqKa/YyikHjWZscQGHjS8mK+ztO/fgMVx8/FR2NbZw+swxHc4fM7SAa8+ayS+eWMkfX1pHVU0j+TlZnHDge53ECnKy+eBh+0V8dyIiyaVaNXR/wnIBcC5dG34z3ra6RvYf1fGZ/bZE8PzqoMbqxo8ezojBHZNFYV4O3/pgh75yHVx43FQqqnbx2+crALjq9AO5/NTS3gxdRGSvpVo19JfEdTO7E3gkkojSaHttE8MKO37JJ7YXnHHI2C5JIFVfPf1AZk8ejplx6kE99cUTEek7ezvAfikQXc+uNGhpdXbUNzN0UPeNtR84eOxeX39YYR7nzBq/1+eLiEQl1TaCnXRsI3iHYI6CAWNnfRMAw3aTCEqG5He7T0QkU6VaNZSeeRr70Pa6MBEkeXzTDNw7djQTERkoUhp91MzONbPihPVhZvah6MLqW9tqGznr588AUJykRDBheDBExMghA3qaZhGJqVSHof6uu29vWwnHAvpuNCH1vadWbGFnQ9ATuHR018LPHRcexdc/MJ2Re9lQLCLSn6XaWJwsYfT9TO4Rqa5paF9ONjfw1JLBXHryAX0ZkohIn0m1RFBuZj81s/3NbJqZ3QS8HGVgfaltyIh/XHl8miMREel7qSaCy4FG4G7gHqAOuDSqoPra8nd3cvC4oczYb2i6QxER6XOpPjW0C0g653Cmq2tsYdHarXzm6MnpDkVEJC1SfWroYTMblrA+3MwWRhdW31m5uYbG5lbKpgxPdygiImmRatVQSeKsYeEcwwNinITKmnoARmvkTxGJqVQTQauZtQ8pYWZTSDIaaSZqn5VMvYZFJKZSfQT0W8AzZvZkuH4C4WTyma4yfHRUvYZFJK5SbSz+p5mVEXz5LwHuI3hyKOM9t2oLg/OyNTWkiMRWqoPOfR64kmAC+iXA0cDzdJy6MuOsqqzh2ZVVTBrRtROZiEhcpNpGcCVwJFDh7icDRwCVkUXVR55YFtzCz86fleZIRETSJ9VEUO/u9QBmlu/ubwHTowurbzy1vJJpowYze5IeHRWR+Eq1sXhD2I/gb8DDZraVDJ+qsqG5hRfXVHH+kQNqfh0RkT2WamPxueHidWb2OFAM/DOyqPrA5h0N1De1MlPDSohIzO3xCKLu/mTPR/V/9U0tAAzK09NCIhJvqbYRDDh1bYlAj42KSMzFNxE0qkQgIgJxTgRhiUAdyUQk7mKbCOpVNSQiAsQ4EdSpsVhEBIhzImhsBVQiEBGJbyJQ1ZCICBDjRNDWRlCQF9tfgYgIEONEUNfYQpZBXnZsfwUiIkCcE0FTC4NyszGzdIciIpJWsU0EtY0temJIRIQYJ4KahmaKCnLTHYaISNrFNxHUNzEkf4/H3BMRGXDimwgampUIRESIcSLYWd/MkAIlAhGRSBOBmc0zs2VmttLMrt7NceeZmZtZWZTxJKppaKZIJQIRkegSgZllAzcDZwAzgflmNjPJcUXAFcCLUcWSTE2DSgQiIhBtiWAOsNLdV7t7I3AXcE6S434A/ASojzCWDtydmnq1EYiIQLSJYDywPmF9Q7itnZkdAUx09/t3dyEzu8TMys2svLKycp8Da2hupbnVVSIQESHaRJCsy6637zTLAm4CrurpQu5+q7uXuXvZqFGj9jmwnfXNAGojEBEh2kSwAZiYsD4B2JiwXgQcAjxhZmuBo4EFfdFgXNMQJAKVCEREok0Ei4BSM5tqZnnA+cCCtp3uvt3dS9x9irtPAV4Aznb38ghjAqAmLBEMyVfPYhGRyBKBuzcDlwELgTeBe9z9DTP7vpmdHdX7pmJnQxOAGotFRIBIvwnd/UHgwU7bru3m2JOijCVRW4mgSFVDIiLx7Fnc3kagEoGISMwTgUoEIiLxTAQ761UiEBFpE8tEUNPQTG62kZ8Ty9sXEekglt+EO+qaKCrI1TSVIiLENBFsr2ti2CD1IRARgRgnguJCJQIREYhxIlCJQEQkEMtEsK22iWIlAhERILaJoJFhhXnpDkNEpF+IXSJoaXV2NjQzVCUCEREghomgtrEZd81FICLSJnaJoK6pBYBBedlpjkREpH+IXSKob2wFYFCuEoGICMQwEahEICLSUXwTgUoEIiJAHBNBY5AICpQIRESAGCaCelUNiYh0ELtE0FY1VKhEICICxDAR1DaqjUBEJFHsEkFbiUBtBCIigdglgvpGtRGIiCSKXSJoLxFomkoRESCGiWBnfRP5OVnkZMfu1kVEkordt+H66jomDB+U7jBERPqN2CWCtVW7mDxycLrDEBHpN2KVCNyd9dW1TBpRmO5QRET6jVglgsaWVnY1tjCqKD/doYiI9BuxSgQaglpEpKtYJQINQS0i0lU8E4FKBCIi7eKVCDQEtYhIF/FKBKoaEhHpIlaJoF5VQyIiXcQqEdRpCGoRkS7ilQjaq4ZiddsiIrsVq29ENRaLiHQVr0SgNgIRkS4iTQRmNs/MlpnZSjO7Osn+r5nZUjN7zcweNbPJUcajp4ZERLqKLBGYWTZwM3AGMBOYb2YzOx22GChz98OAPwM/iSoegNqGZsygIEeJQESkTZQlgjnASndf7e6NwF3AOYkHuPvj7l4brr4ATIgwHrbXNVGUn0NWlkX5NiIiGSXKRDAeWJ+wviHc1p2LgH8k22Fml5hZuZmVV1ZW7nVA2+qaGFaYt9fni4gMRFEmgmR/dnvSA80+BZQBNyTb7+63unuZu5eNGjVqrwPaXtdE8aDcvT5fRGQgyonw2huAiQnrE4CNnQ8ys9OAbwEnuntDhPGwrbaJYYVKBCIiiaIsESwCSs1sqpnlAecDCxIPMLMjgP8Bznb3zRHGAsAOlQhERLqILBG4ezNwGbAQeBO4x93fMLPvm9nZ4WE3AEOAP5nZEjNb0M3lesU2JQIRkS6irBrC3R8EHuy07dqE5dOifP9ETS2tbK9rYsRgNRaLiCSKTc/ijdvqaGl1Jg7XxPUiIolikwgqqoLuCpNHKhGIiCSKTyKobksEg9MciYhI/xKbRDCmKJ/TZ45hdFF+ukMREelXIm0s7k/mHjyWuQePTXcYIiL9TmxKBCIikpwSgYhIzCkRiIjEnBKBiEjMKRGIiMScEoGISMwpEYiIxJwSgYhIzJl70knD+i0zqwQq9vL0EmBLL4aTDpl+D4o//TL9HjI9fkjPPUx296RTPGZcItgXZlbu7mXpjmNfZPo9KP70y/R7yPT4of/dg6qGRERiTolARCTm4pYIbk13AL0g0+9B8adfpt9DpscP/eweYtVGICIiXcWtRCAiIp0oEYiIxFxsEoGZzTOzZWa20syuTnc8yZjZ7Wa22cxeT9g2wsweNrMV4c/h4XYzs5+H9/Oamc1OX+TtsU40s8fN7E0ze8PMrgy3Z9I9FJjZS2b2angP3wu3TzWzF8N7uNvM8sLt+eH6ynD/lHTG38bMss1ssZndH65nWvxrzexfZrbEzMrDbZn0ORpmZn82s7fC/w/H9Of4Y5EIzCwbuBk4A5gJzDezmemNKqnfAPM6bbsaeNTdS4FHw3UI7qU0fF0C3NJHMe5OM3CVu88AjgYuDX/PmXQPDcAp7n44MAuYZ2ZHA/8J3BTew1bgovD4i4Ct7n4AcFN4XH9wJfBmwnqmxQ9wsrvPSnjePpM+R/8X+Ke7HwQcTvBv0X/jd/cB/wKOARYmrHREJRwAAAZ0SURBVF8DXJPuuLqJdQrwesL6MmC/cHk/YFm4/D/A/GTH9ZcXcB9weqbeA1AIvAIcRdALNKfz5wlYCBwTLueEx1ma455A8EVzCnA/YJkUfxjLWqCk07aM+BwBQ4E1nX+P/Tn+WJQIgPHA+oT1DeG2TDDG3TcBhD9Hh9v79T2FVQxHAC+SYfcQVqssATYDDwOrgG3u3hwekhhn+z2E+7cDI/s24i5+BnwDaA3XR5JZ8QM48JCZvWxml4TbMuVzNA2oBH4dVs/9yswG04/jj0sisCTbMv252X57T2Y2BPgL8BV337G7Q5NsS/s9uHuLu88i+Mt6DjAj2WHhz351D2Z2FrDZ3V9O3Jzk0H4Zf4Jj3X02QbXJpWZ2wm6O7W/3kAPMBm5x9yOAXbxXDZRM2uOPSyLYAExMWJ8AbExTLHvqXTPbDyD8uTnc3i/vycxyCZLAH9z93nBzRt1DG3ffBjxB0N4xzMxywl2JcbbfQ7i/GKju20g7OBY428zWAncRVA/9jMyJHwB33xj+3Az8lSAhZ8rnaAOwwd1fDNf/TJAY+m38cUkEi4DS8MmJPOB8YEGaY0rVAuCz4fJnCerd27Z/Jnzi4Ghge1uxM13MzID/Bd50958m7MqkexhlZsPC5UHAaQQNfY8D54WHdb6Htns7D3jMw4redHD3a9x9grtPIficP+bunyRD4gcws8FmVtS2DMwFXidDPkfu/g6w3symh5tOBZbSn+NPV4NKX7+AM4HlBPW930p3PN3EeCewCWgi+CvhIoL62keBFeHPEeGxRvAk1CrgX0BZP4j/OIIi7WvAkvB1Zobdw2HA4vAeXgeuDbdPA14CVgJ/AvLD7QXh+spw/7R030PCvZwE3J9p8Yexvhq+3mj7/5phn6NZQHn4OfobMLw/x68hJkREYi4uVUMiItINJQIRkZhTIhARiTklAhGRmFMiEBGJOSUCiR0ze8LMIp843MyuCEee/EPU79Xpfa8zs3/vy/eUzJbT8yEi0sbMcvy9MXt68mXgDHdfE2VMIvtKJQLpl8xsSvjX9G0WzAvwUNjTt8Nf9GZWEg6ngJldYGZ/M7O/m9kaM7vMzL4WDvz1gpmNSHiLT5nZc2b2upnNCc8fbMGcEIvCc85JuO6fzOzvwENJYv1aeJ3Xzewr4bZfEnSMWmBmX+10fLaZ3RC+z2tm9oVw+0lm9pSZ/dXMlprZL80sK9w334Lx+V83s/9MuNY8M3vFgvkTHk14m5nh72m1mV2RcH8PhMe+bmYf35d/IxlA0t0DTy+9kr0IhuNuBmaF6/cAnwqXnyDsfQmUAGvD5QsIesgWAaMIRtL8YrjvJoJB8NrOvy1cPoFw2G/gRwnvMYygJ/rg8LobCHuCdorzfQS9QQcDQwh6wh4R7ltLp6GUw+2XAN8Ol/MJeqBOJegJXE+QQLIJRj49DxgHrAvvKQd4DPhQuL4emBpeq62n6nXAc+G1S4AqIBf4SNt9h8cVp/vfWa/+8VLVkPRna9x9Sbj8MkFy6Mnj7r4T2Glm24G/h9v/RTB8RJs7Adz9KTMbGo4vNJdgwLa2+vUCYFK4/LC7JxuM7Tjgr+6+C8DM7gWOJximojtzgcPMrG3sn2KCSUkagZfcfXV4rTvD6zcBT7h7Zbj9DwQJrAV4ysOqp07xPeDuDUCDmW0GxoS/gxvDEsX97v70bmKUGFEikP6sIWG5BRgULjfzXrVmwW7OaU1Yb6Xj573z2CpOMObLR9x9WeIOMzuKYCjhZJINIdwTAy5394Wd3uek3cTV3XW6GyOm8+8ux92Xm9n7CMZ/+rGZPeTu39/T4GXgURuBZKK1BFUy8N6Imnvq4wBmdhzBaI/bCWbrujwcRRUzOyKF6zwFfMjMCsORMs8FevpLeyHwJQuG7MbMDgzPBZgTjpKbFcb4DMHkPieG7SHZwHzgSeD5cPvU8DojOr9RIjMbB9S6+++BGwmGRhZRiUAy0o3APWb2aYL68r2x1cyeI5hW8MJw2w8Ixu5/LUwGa4GzdncRd3/FzH5DMHInwK/cfXfVQgC/IqjmeiV8n0qCOn8IvtyvBw4lSDJ/dfdWM7uGYChpAx509/sALJi9694wcWwmmBq0O4cCN5hZK0F105d6iFNiQqOPivQTYdXQv7v7bpOPSG9T1ZCISMypRCAiEnMqEYiIxJwSgYhIzCkRiIjEnBKBiEjMKRGIiMTc/weiugtfpIKRYgAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt \n",
    "plt.plot(range(630),total_acc)\n",
    "plt.xlabel('number of epochs')\n",
    "plt.ylabel('accuracy')\n",
    "plt.title('proposed')\n",
    "plt.savefig('48_1440...1000/ga_drop1.jpg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
