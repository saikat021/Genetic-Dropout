{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "id": "pdWtHV8WcuiV",
    "outputId": "d00f3832-17bc-4679-b5d7-1ddb119ed3c7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
     ]
    }
   ],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 256
    },
    "colab_type": "code",
    "id": "mmQC8wswc4f1",
    "outputId": "cdf2a6dc-61aa-4f7f-dda0-b9fc1e8caf21"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>att_1</th>\n",
       "      <th>att_2</th>\n",
       "      <th>att_3</th>\n",
       "      <th>att_4</th>\n",
       "      <th>att_5</th>\n",
       "      <th>att_6</th>\n",
       "      <th>att_7</th>\n",
       "      <th>att_8</th>\n",
       "      <th>att_9</th>\n",
       "      <th>att_10</th>\n",
       "      <th>att_11</th>\n",
       "      <th>att_12</th>\n",
       "      <th>att_13</th>\n",
       "      <th>att_14</th>\n",
       "      <th>att_15</th>\n",
       "      <th>att_16</th>\n",
       "      <th>att_17</th>\n",
       "      <th>att_18</th>\n",
       "      <th>att_19</th>\n",
       "      <th>att_20</th>\n",
       "      <th>att_21</th>\n",
       "      <th>att_22</th>\n",
       "      <th>att_23</th>\n",
       "      <th>att_24</th>\n",
       "      <th>att_25</th>\n",
       "      <th>att_26</th>\n",
       "      <th>att_27</th>\n",
       "      <th>att_28</th>\n",
       "      <th>att_29</th>\n",
       "      <th>att_30</th>\n",
       "      <th>att_31</th>\n",
       "      <th>att_32</th>\n",
       "      <th>att_33</th>\n",
       "      <th>att_34</th>\n",
       "      <th>att_35</th>\n",
       "      <th>att_36</th>\n",
       "      <th>att_37</th>\n",
       "      <th>att_38</th>\n",
       "      <th>att_39</th>\n",
       "      <th>att_40</th>\n",
       "      <th>...</th>\n",
       "      <th>att_142</th>\n",
       "      <th>att_143</th>\n",
       "      <th>att_144</th>\n",
       "      <th>att_145</th>\n",
       "      <th>att_146</th>\n",
       "      <th>att_147</th>\n",
       "      <th>att_148</th>\n",
       "      <th>att_149</th>\n",
       "      <th>att_150</th>\n",
       "      <th>att_151</th>\n",
       "      <th>att_152</th>\n",
       "      <th>att_153</th>\n",
       "      <th>att_154</th>\n",
       "      <th>att_155</th>\n",
       "      <th>att_156</th>\n",
       "      <th>att_157</th>\n",
       "      <th>att_158</th>\n",
       "      <th>att_159</th>\n",
       "      <th>att_160</th>\n",
       "      <th>att_161</th>\n",
       "      <th>att_162</th>\n",
       "      <th>att_163</th>\n",
       "      <th>att_164</th>\n",
       "      <th>att_165</th>\n",
       "      <th>att_166</th>\n",
       "      <th>att_167</th>\n",
       "      <th>att_168</th>\n",
       "      <th>att_169</th>\n",
       "      <th>att_170</th>\n",
       "      <th>att_171</th>\n",
       "      <th>att_172</th>\n",
       "      <th>att_173</th>\n",
       "      <th>att_174</th>\n",
       "      <th>att_175</th>\n",
       "      <th>att_176</th>\n",
       "      <th>att_177</th>\n",
       "      <th>att_178</th>\n",
       "      <th>att_179</th>\n",
       "      <th>att_180</th>\n",
       "      <th>class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>12.184764</td>\n",
       "      <td>18.534243</td>\n",
       "      <td>28.945810</td>\n",
       "      <td>22.241393</td>\n",
       "      <td>7.353465</td>\n",
       "      <td>17.095399</td>\n",
       "      <td>26.996429</td>\n",
       "      <td>41.427220</td>\n",
       "      <td>24.394928</td>\n",
       "      <td>3.888953</td>\n",
       "      <td>16.692057</td>\n",
       "      <td>26.119638</td>\n",
       "      <td>39.722458</td>\n",
       "      <td>23.561138</td>\n",
       "      <td>4.422741</td>\n",
       "      <td>11.639512</td>\n",
       "      <td>20.120012</td>\n",
       "      <td>29.659610</td>\n",
       "      <td>20.367249</td>\n",
       "      <td>6.726386</td>\n",
       "      <td>14.140901</td>\n",
       "      <td>24.451595</td>\n",
       "      <td>33.493874</td>\n",
       "      <td>21.272691</td>\n",
       "      <td>8.236757</td>\n",
       "      <td>16.860680</td>\n",
       "      <td>31.564917</td>\n",
       "      <td>40.518904</td>\n",
       "      <td>23.521893</td>\n",
       "      <td>9.398887</td>\n",
       "      <td>15.659533</td>\n",
       "      <td>30.509884</td>\n",
       "      <td>36.151632</td>\n",
       "      <td>24.099052</td>\n",
       "      <td>9.867436</td>\n",
       "      <td>14.418768</td>\n",
       "      <td>29.246418</td>\n",
       "      <td>34.769063</td>\n",
       "      <td>22.928747</td>\n",
       "      <td>17.098696</td>\n",
       "      <td>...</td>\n",
       "      <td>0.011635</td>\n",
       "      <td>0.014132</td>\n",
       "      <td>0.015190</td>\n",
       "      <td>0.018073</td>\n",
       "      <td>0.007951</td>\n",
       "      <td>0.011116</td>\n",
       "      <td>0.012599</td>\n",
       "      <td>0.011804</td>\n",
       "      <td>0.009471</td>\n",
       "      <td>0.007398</td>\n",
       "      <td>0.011174</td>\n",
       "      <td>0.013660</td>\n",
       "      <td>0.012094</td>\n",
       "      <td>0.007816</td>\n",
       "      <td>0.006827</td>\n",
       "      <td>0.011113</td>\n",
       "      <td>0.014481</td>\n",
       "      <td>0.013526</td>\n",
       "      <td>0.008810</td>\n",
       "      <td>0.007137</td>\n",
       "      <td>0.010558</td>\n",
       "      <td>0.015235</td>\n",
       "      <td>0.014294</td>\n",
       "      <td>0.007546</td>\n",
       "      <td>0.007302</td>\n",
       "      <td>0.010320</td>\n",
       "      <td>0.014466</td>\n",
       "      <td>0.014560</td>\n",
       "      <td>0.008232</td>\n",
       "      <td>0.007450</td>\n",
       "      <td>0.010558</td>\n",
       "      <td>0.014658</td>\n",
       "      <td>0.016326</td>\n",
       "      <td>0.009556</td>\n",
       "      <td>0.007948</td>\n",
       "      <td>0.011146</td>\n",
       "      <td>0.016568</td>\n",
       "      <td>0.017170</td>\n",
       "      <td>0.008511</td>\n",
       "      <td>A</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>25.639720</td>\n",
       "      <td>46.945876</td>\n",
       "      <td>63.464762</td>\n",
       "      <td>44.700204</td>\n",
       "      <td>23.165613</td>\n",
       "      <td>18.646121</td>\n",
       "      <td>28.515218</td>\n",
       "      <td>41.926007</td>\n",
       "      <td>29.490375</td>\n",
       "      <td>8.800943</td>\n",
       "      <td>13.905585</td>\n",
       "      <td>20.948323</td>\n",
       "      <td>29.197588</td>\n",
       "      <td>24.777035</td>\n",
       "      <td>11.693705</td>\n",
       "      <td>12.900319</td>\n",
       "      <td>23.106127</td>\n",
       "      <td>28.827459</td>\n",
       "      <td>20.651541</td>\n",
       "      <td>12.113402</td>\n",
       "      <td>16.001618</td>\n",
       "      <td>28.730252</td>\n",
       "      <td>33.722841</td>\n",
       "      <td>22.691698</td>\n",
       "      <td>14.884630</td>\n",
       "      <td>12.886395</td>\n",
       "      <td>22.841184</td>\n",
       "      <td>29.999213</td>\n",
       "      <td>21.743363</td>\n",
       "      <td>14.644041</td>\n",
       "      <td>10.872357</td>\n",
       "      <td>17.554802</td>\n",
       "      <td>26.507216</td>\n",
       "      <td>25.717512</td>\n",
       "      <td>12.146767</td>\n",
       "      <td>12.578959</td>\n",
       "      <td>20.806199</td>\n",
       "      <td>30.296498</td>\n",
       "      <td>26.063166</td>\n",
       "      <td>11.027926</td>\n",
       "      <td>...</td>\n",
       "      <td>0.018399</td>\n",
       "      <td>0.024574</td>\n",
       "      <td>0.023404</td>\n",
       "      <td>0.019674</td>\n",
       "      <td>0.010260</td>\n",
       "      <td>0.016181</td>\n",
       "      <td>0.021939</td>\n",
       "      <td>0.023295</td>\n",
       "      <td>0.019952</td>\n",
       "      <td>0.010074</td>\n",
       "      <td>0.015923</td>\n",
       "      <td>0.022831</td>\n",
       "      <td>0.022867</td>\n",
       "      <td>0.014010</td>\n",
       "      <td>0.009507</td>\n",
       "      <td>0.014709</td>\n",
       "      <td>0.020469</td>\n",
       "      <td>0.019867</td>\n",
       "      <td>0.012323</td>\n",
       "      <td>0.009276</td>\n",
       "      <td>0.014710</td>\n",
       "      <td>0.020125</td>\n",
       "      <td>0.019822</td>\n",
       "      <td>0.009504</td>\n",
       "      <td>0.009147</td>\n",
       "      <td>0.014281</td>\n",
       "      <td>0.020338</td>\n",
       "      <td>0.019589</td>\n",
       "      <td>0.010720</td>\n",
       "      <td>0.009711</td>\n",
       "      <td>0.014752</td>\n",
       "      <td>0.022414</td>\n",
       "      <td>0.024294</td>\n",
       "      <td>0.014430</td>\n",
       "      <td>0.010937</td>\n",
       "      <td>0.016972</td>\n",
       "      <td>0.025050</td>\n",
       "      <td>0.023918</td>\n",
       "      <td>0.014216</td>\n",
       "      <td>A</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>14.381637</td>\n",
       "      <td>24.706305</td>\n",
       "      <td>35.822925</td>\n",
       "      <td>29.553382</td>\n",
       "      <td>22.935236</td>\n",
       "      <td>13.837493</td>\n",
       "      <td>22.689212</td>\n",
       "      <td>36.892506</td>\n",
       "      <td>33.376486</td>\n",
       "      <td>9.703304</td>\n",
       "      <td>11.278967</td>\n",
       "      <td>19.369300</td>\n",
       "      <td>29.284697</td>\n",
       "      <td>28.393769</td>\n",
       "      <td>8.605397</td>\n",
       "      <td>10.737174</td>\n",
       "      <td>18.724029</td>\n",
       "      <td>22.954104</td>\n",
       "      <td>22.718452</td>\n",
       "      <td>12.324788</td>\n",
       "      <td>13.249558</td>\n",
       "      <td>26.019134</td>\n",
       "      <td>26.479294</td>\n",
       "      <td>21.735270</td>\n",
       "      <td>13.723333</td>\n",
       "      <td>14.321044</td>\n",
       "      <td>38.548935</td>\n",
       "      <td>47.383356</td>\n",
       "      <td>28.481238</td>\n",
       "      <td>20.100181</td>\n",
       "      <td>13.700423</td>\n",
       "      <td>34.864279</td>\n",
       "      <td>46.987831</td>\n",
       "      <td>27.429569</td>\n",
       "      <td>25.531466</td>\n",
       "      <td>14.113249</td>\n",
       "      <td>32.314696</td>\n",
       "      <td>43.158068</td>\n",
       "      <td>27.469746</td>\n",
       "      <td>19.794746</td>\n",
       "      <td>...</td>\n",
       "      <td>0.014387</td>\n",
       "      <td>0.020853</td>\n",
       "      <td>0.022117</td>\n",
       "      <td>0.020079</td>\n",
       "      <td>0.009182</td>\n",
       "      <td>0.013964</td>\n",
       "      <td>0.018120</td>\n",
       "      <td>0.020490</td>\n",
       "      <td>0.016575</td>\n",
       "      <td>0.009079</td>\n",
       "      <td>0.013714</td>\n",
       "      <td>0.017907</td>\n",
       "      <td>0.018449</td>\n",
       "      <td>0.016712</td>\n",
       "      <td>0.008954</td>\n",
       "      <td>0.014038</td>\n",
       "      <td>0.018255</td>\n",
       "      <td>0.017512</td>\n",
       "      <td>0.014393</td>\n",
       "      <td>0.008906</td>\n",
       "      <td>0.014206</td>\n",
       "      <td>0.017487</td>\n",
       "      <td>0.016339</td>\n",
       "      <td>0.014050</td>\n",
       "      <td>0.008712</td>\n",
       "      <td>0.013781</td>\n",
       "      <td>0.017666</td>\n",
       "      <td>0.019439</td>\n",
       "      <td>0.016598</td>\n",
       "      <td>0.008739</td>\n",
       "      <td>0.013782</td>\n",
       "      <td>0.019098</td>\n",
       "      <td>0.024171</td>\n",
       "      <td>0.020194</td>\n",
       "      <td>0.009770</td>\n",
       "      <td>0.014811</td>\n",
       "      <td>0.021661</td>\n",
       "      <td>0.021909</td>\n",
       "      <td>0.015823</td>\n",
       "      <td>A</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>20.123721</td>\n",
       "      <td>39.021980</td>\n",
       "      <td>59.427789</td>\n",
       "      <td>33.768032</td>\n",
       "      <td>16.884216</td>\n",
       "      <td>14.235512</td>\n",
       "      <td>24.346061</td>\n",
       "      <td>33.150834</td>\n",
       "      <td>17.558946</td>\n",
       "      <td>5.433499</td>\n",
       "      <td>11.368576</td>\n",
       "      <td>16.641580</td>\n",
       "      <td>16.312035</td>\n",
       "      <td>12.704722</td>\n",
       "      <td>7.584464</td>\n",
       "      <td>11.794634</td>\n",
       "      <td>17.600989</td>\n",
       "      <td>15.405322</td>\n",
       "      <td>12.884437</td>\n",
       "      <td>18.272439</td>\n",
       "      <td>12.702698</td>\n",
       "      <td>21.817848</td>\n",
       "      <td>21.906229</td>\n",
       "      <td>15.560347</td>\n",
       "      <td>13.799468</td>\n",
       "      <td>11.923086</td>\n",
       "      <td>23.846825</td>\n",
       "      <td>27.034937</td>\n",
       "      <td>21.597422</td>\n",
       "      <td>14.734907</td>\n",
       "      <td>10.231710</td>\n",
       "      <td>20.986869</td>\n",
       "      <td>27.847546</td>\n",
       "      <td>21.334325</td>\n",
       "      <td>9.338959</td>\n",
       "      <td>11.167240</td>\n",
       "      <td>20.317803</td>\n",
       "      <td>29.955248</td>\n",
       "      <td>24.897731</td>\n",
       "      <td>9.992332</td>\n",
       "      <td>...</td>\n",
       "      <td>0.015341</td>\n",
       "      <td>0.020819</td>\n",
       "      <td>0.020321</td>\n",
       "      <td>0.021944</td>\n",
       "      <td>0.009273</td>\n",
       "      <td>0.013473</td>\n",
       "      <td>0.017387</td>\n",
       "      <td>0.016286</td>\n",
       "      <td>0.012100</td>\n",
       "      <td>0.009066</td>\n",
       "      <td>0.013612</td>\n",
       "      <td>0.019132</td>\n",
       "      <td>0.019185</td>\n",
       "      <td>0.010803</td>\n",
       "      <td>0.008737</td>\n",
       "      <td>0.013403</td>\n",
       "      <td>0.018748</td>\n",
       "      <td>0.017529</td>\n",
       "      <td>0.009745</td>\n",
       "      <td>0.008372</td>\n",
       "      <td>0.013575</td>\n",
       "      <td>0.018175</td>\n",
       "      <td>0.015580</td>\n",
       "      <td>0.012225</td>\n",
       "      <td>0.008549</td>\n",
       "      <td>0.013577</td>\n",
       "      <td>0.020318</td>\n",
       "      <td>0.018893</td>\n",
       "      <td>0.015797</td>\n",
       "      <td>0.009185</td>\n",
       "      <td>0.014727</td>\n",
       "      <td>0.022686</td>\n",
       "      <td>0.022876</td>\n",
       "      <td>0.013031</td>\n",
       "      <td>0.010639</td>\n",
       "      <td>0.016706</td>\n",
       "      <td>0.026014</td>\n",
       "      <td>0.023591</td>\n",
       "      <td>0.012991</td>\n",
       "      <td>A</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>20.855774</td>\n",
       "      <td>31.241891</td>\n",
       "      <td>46.056155</td>\n",
       "      <td>36.617243</td>\n",
       "      <td>20.152911</td>\n",
       "      <td>15.612698</td>\n",
       "      <td>27.206975</td>\n",
       "      <td>42.754484</td>\n",
       "      <td>29.983009</td>\n",
       "      <td>7.457517</td>\n",
       "      <td>11.662058</td>\n",
       "      <td>18.905805</td>\n",
       "      <td>31.409227</td>\n",
       "      <td>24.829528</td>\n",
       "      <td>4.523870</td>\n",
       "      <td>12.044599</td>\n",
       "      <td>25.853903</td>\n",
       "      <td>34.339422</td>\n",
       "      <td>17.169122</td>\n",
       "      <td>6.675385</td>\n",
       "      <td>11.463279</td>\n",
       "      <td>29.172892</td>\n",
       "      <td>33.184479</td>\n",
       "      <td>14.121998</td>\n",
       "      <td>8.962364</td>\n",
       "      <td>11.977819</td>\n",
       "      <td>31.257173</td>\n",
       "      <td>36.656117</td>\n",
       "      <td>19.558887</td>\n",
       "      <td>13.133167</td>\n",
       "      <td>13.351911</td>\n",
       "      <td>32.172718</td>\n",
       "      <td>41.949929</td>\n",
       "      <td>30.115787</td>\n",
       "      <td>14.969102</td>\n",
       "      <td>14.343485</td>\n",
       "      <td>35.341644</td>\n",
       "      <td>44.988423</td>\n",
       "      <td>27.579509</td>\n",
       "      <td>9.464650</td>\n",
       "      <td>...</td>\n",
       "      <td>0.015889</td>\n",
       "      <td>0.022721</td>\n",
       "      <td>0.022730</td>\n",
       "      <td>0.022472</td>\n",
       "      <td>0.010109</td>\n",
       "      <td>0.015527</td>\n",
       "      <td>0.022441</td>\n",
       "      <td>0.023648</td>\n",
       "      <td>0.013539</td>\n",
       "      <td>0.010414</td>\n",
       "      <td>0.015549</td>\n",
       "      <td>0.021899</td>\n",
       "      <td>0.022117</td>\n",
       "      <td>0.011333</td>\n",
       "      <td>0.010250</td>\n",
       "      <td>0.015448</td>\n",
       "      <td>0.021103</td>\n",
       "      <td>0.018298</td>\n",
       "      <td>0.010251</td>\n",
       "      <td>0.010060</td>\n",
       "      <td>0.014660</td>\n",
       "      <td>0.020015</td>\n",
       "      <td>0.017280</td>\n",
       "      <td>0.009596</td>\n",
       "      <td>0.009790</td>\n",
       "      <td>0.013656</td>\n",
       "      <td>0.017717</td>\n",
       "      <td>0.016867</td>\n",
       "      <td>0.011999</td>\n",
       "      <td>0.010235</td>\n",
       "      <td>0.014497</td>\n",
       "      <td>0.019523</td>\n",
       "      <td>0.023203</td>\n",
       "      <td>0.016096</td>\n",
       "      <td>0.011517</td>\n",
       "      <td>0.016587</td>\n",
       "      <td>0.024086</td>\n",
       "      <td>0.022221</td>\n",
       "      <td>0.015074</td>\n",
       "      <td>A</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 181 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       att_1      att_2      att_3  ...   att_179   att_180  class\n",
       "0  12.184764  18.534243  28.945810  ...  0.017170  0.008511      A\n",
       "1  25.639720  46.945876  63.464762  ...  0.023918  0.014216      A\n",
       "2  14.381637  24.706305  35.822925  ...  0.021909  0.015823      A\n",
       "3  20.123721  39.021980  59.427789  ...  0.023591  0.012991      A\n",
       "4  20.855774  31.241891  46.056155  ...  0.022221  0.015074      A\n",
       "\n",
       "[5 rows x 181 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "path ='Riktim_Testing/Word-level-Features/MLG_180_Features.csv'\n",
    "df = pd.read_csv(path) #header=None\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "id": "JvdIw7L-c6jj",
    "outputId": "5877d884-9f75-4988-eed5-3ccbebe32172"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(12000, 181)\n"
     ]
    }
   ],
   "source": [
    "print(df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "wi_1eAi-c80l"
   },
   "outputs": [],
   "source": [
    "# Import modules\n",
    "import numpy as np\n",
    "# import matplotlib.pyplot as plt\n",
    "# from sklearn.datasets import load_iris,load_digits,load_breast_cancer,load_wine\n",
    "# from sklearn import preprocessing\n",
    "\n",
    "# Some more magic so that the notebook will reload external python modules;\n",
    "# see http://stackoverflow.com/questions/1907993/autoreload-of-modules-in-ipython\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "XWti03uamkMV"
   },
   "outputs": [],
   "source": [
    "# # Load the iris dataset\n",
    "# data = load_digits()\n",
    "\n",
    "# # Store the features as X and the labels as y\n",
    "# X = data.data\n",
    "# y = data.target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 127
    },
    "colab_type": "code",
    "id": "CTevYo6Tc-jM",
    "outputId": "4d19021d-0704-42f7-e6dd-2594f5283364"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['att_1', 'att_2', 'att_3', 'att_4', 'att_5', 'att_6', 'att_7', 'att_8',\n",
       "       'att_9', 'att_10',\n",
       "       ...\n",
       "       'att_172', 'att_173', 'att_174', 'att_175', 'att_176', 'att_177',\n",
       "       'att_178', 'att_179', 'att_180', 'class'],\n",
       "      dtype='object', length=181)"
      ]
     },
     "execution_count": 7,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 127
    },
    "colab_type": "code",
    "id": "RyCqrQcEdBMu",
    "outputId": "2994132f-52d1-4930-956d-b3d8bf45c9b9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(12000, 181)\n",
      "<class 'numpy.ndarray'>\n",
      "(12000, 180)\n",
      "\n",
      "['A' 'B' 'C' 'D' 'E' 'F' 'G' 'H' 'I' 'J' 'K' 'L']\n",
      "[0 0 0 0 0 0 0 0 0 0]\n"
     ]
    }
   ],
   "source": [
    "print(df.shape)\n",
    "#X=df.loc[:,df.columns != 'Classifier'].values\n",
    "X=df.loc[:,df.columns != df.columns[-1]].values\n",
    "print(type(X))\n",
    "print(X.shape)\n",
    "#y_label=df['Classifier'].values \n",
    "y_label =df.iloc[:,-1].values\n",
    "#print(type(y_label))\n",
    "#print(X[0])\n",
    "print()\n",
    "from sklearn import preprocessing\n",
    "le = preprocessing.LabelEncoder()\n",
    "le.fit(y_label)\n",
    "print(le.classes_)\n",
    "y=le.transform(y_label)\n",
    "print(y[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 54
    },
    "colab_type": "code",
    "id": "pI7UW-CrdH6W",
    "outputId": "f658c4c5-a824-475f-f474-234fcdb6f94d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11} {0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11}\n",
      "(12000, 180) (12000,) (8400, 180) (3600, 180) (8400,) (3600,)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler #for normalization\n",
    "sc = StandardScaler()\n",
    "X = sc.fit_transform(X)\n",
    "# from sklearn.preprocessing import MinMaxScaler\n",
    "# scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "# X = scaler.fit_transform(X)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42, shuffle=True)\n",
    "# print(y_train)\n",
    "# print(y_test)\n",
    "#print(len(X_train), len(y_train), len(X_test), len(y_test))\n",
    "# # X=X_train\n",
    "# # y=y_train\n",
    "print(set(y_train), set(y_test))\n",
    "print(X.shape, y.shape,X_train.shape,X_test.shape,y_train.shape,y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "K3sGsQIWdKTq"
   },
   "outputs": [],
   "source": [
    "n_inputs = X_train.shape[1]\n",
    "n_hidden = X_train.shape[1]\n",
    "n_classes = len(le.classes_)\n",
    "#N = X_train.shape[0] # Number of samples\n",
    "\n",
    "#print(N)\n",
    "#hyperparameters\n",
    "lr = 0.001\n",
    "num_epoch = 310"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "VdBHK_mIdU7x"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "#torch.manual_seed(1234)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "PWfQFapWjDPM"
   },
   "outputs": [],
   "source": [
    "#build model\n",
    "class Net(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        self.fc1 = nn.Linear(n_inputs, n_hidden)\n",
    "        self.drop = nn.Dropout (p=0.4)\n",
    "        self.fc2 = nn.Linear(n_hidden,n_classes )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.fc1(x))\n",
    "        #x=self.drop(x)\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "        \n",
    "net = Net()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "JS0_RD2EjHQt"
   },
   "outputs": [],
   "source": [
    "#choose optimizer and loss function\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "#optimizer = torch.optim.SGD(net.parameters(), lr=0.001)\n",
    "optimizer = torch.optim.Adam(net.parameters(), lr=lr)\n",
    "\n",
    "#swap test and train\n",
    "# X_train,X_test=X_test,X_train\n",
    "# y_train,y_test =y_test,y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "azXqn4uCAupH"
   },
   "outputs": [],
   "source": [
    "# #print(type(X_train[0]),type(X_test[0]),type(y_train[0]),type(y_test[0]))\n",
    "# from sklearn.model_selection import KFold # import KFold\n",
    "# kf = KFold(n_splits=5,shuffle=True, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "USZ9caBSjIDA"
   },
   "outputs": [],
   "source": [
    "# #train\n",
    "# train_scores=[]\n",
    "# test_scores=[]\n",
    "# acc=0\n",
    "# for k, (train, test) in enumerate(kf.split(X, y)):\n",
    "#   X_train, X_test = X[train], X[test]\n",
    "#   y_train, y_test = y[train], y[test]\n",
    "#  # X_train = torch.tensor(X_train, dtype=torch.float)\n",
    "#  # y_train = torch.tensor(y_train, dtype=torch.long)\n",
    "#  # X_test = torch.tensor(X_test, dtype=torch.float)\n",
    "#  # y_test = torch.tensor(y_test, dtype=torch.long)\n",
    "#   net.train()\n",
    "#   #print(X_train.shape, y_train.shape)\n",
    "#   #train.sort()\n",
    "#   #test.sort()\n",
    "#   #print(train== test)\n",
    "#   #print(train[:10])\n",
    "#   #print(test[:10])\n",
    "#   print(set(list(y_test)))\n",
    "#   print(set(list(y_train)))\n",
    "#   continue\n",
    "#   for epoch in range(num_epoch):\n",
    "#       #feedforward - backprop\n",
    "#       optimizer.zero_grad()\n",
    "#       out = net(X_train)\n",
    "#       loss = criterion(out, y_train)\n",
    "#       loss.backward()\n",
    "#       optimizer.step()\n",
    "#       acc = 100 * torch.sum(y_train==torch.max(out.data, 1)[1]).double() / len(y_train)\n",
    "#       print ('Epoch [%d/%d] Loss: %.4f   Acc: %.4f' \n",
    "#                     %(epoch+1, num_epoch, loss.item(), acc.item()))\n",
    "#   train_scores.append(acc)\n",
    "#   print(X_test.shape, y_test.shape)\n",
    "#   net.eval()\n",
    "#   out = net(X_test)\n",
    "#   _, predicted = torch.max(out.data, 1) \n",
    "#   test_scores.append((100 * torch.sum(y_test==predicted).double() / len(y_test)));\n",
    "#   del net\n",
    "#   net = Net()\n",
    "#   criterion = nn.CrossEntropyLoss()\n",
    "#   #optimizer = torch.optim.SGD(net.parameters(), lr=0.001)\n",
    "#   optimizer = torch.optim.Adam(net.parameters(), lr=lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "70hRvef3fnEK"
   },
   "outputs": [],
   "source": [
    "# out = net(X_train)\n",
    "# _, predicted = torch.max(out.data, 1) \n",
    "# print((100 * torch.sum(y_train==predicted).double() / len(y_train)));"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "GJ_nuJcRcXvy"
   },
   "outputs": [],
   "source": [
    "# print(train_scores)\n",
    "# print(test_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "i7ZhHlDOiadV",
    "outputId": "5a948084-df8e-4ba0-b8e2-e6aa33fb9ce2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/310] Loss: 2.5480   Acc: 5.1071\n",
      "Epoch [2/310] Loss: 2.4014   Acc: 16.2857\n",
      "Epoch [3/310] Loss: 2.2665   Acc: 32.7262\n",
      "Epoch [4/310] Loss: 2.1433   Acc: 42.2262\n",
      "Epoch [5/310] Loss: 2.0313   Acc: 47.3214\n",
      "Epoch [6/310] Loss: 1.9297   Acc: 50.3095\n",
      "Epoch [7/310] Loss: 1.8372   Acc: 52.7024\n",
      "Epoch [8/310] Loss: 1.7527   Acc: 54.6071\n",
      "Epoch [9/310] Loss: 1.6753   Acc: 56.5595\n",
      "Epoch [10/310] Loss: 1.6040   Acc: 58.4405\n",
      "Epoch [11/310] Loss: 1.5377   Acc: 59.8333\n",
      "Epoch [12/310] Loss: 1.4756   Acc: 61.5119\n",
      "Epoch [13/310] Loss: 1.4170   Acc: 62.9524\n",
      "Epoch [14/310] Loss: 1.3612   Acc: 64.4167\n",
      "Epoch [15/310] Loss: 1.3082   Acc: 66.3452\n",
      "Epoch [16/310] Loss: 1.2578   Acc: 68.0238\n",
      "Epoch [17/310] Loss: 1.2099   Acc: 69.5714\n",
      "Epoch [18/310] Loss: 1.1647   Acc: 70.9405\n",
      "Epoch [19/310] Loss: 1.1221   Acc: 72.2976\n",
      "Epoch [20/310] Loss: 1.0819   Acc: 73.4643\n",
      "Epoch [21/310] Loss: 1.0443   Acc: 74.4048\n",
      "Epoch [22/310] Loss: 1.0089   Acc: 75.2262\n",
      "Epoch [23/310] Loss: 0.9756   Acc: 76.0238\n",
      "Epoch [24/310] Loss: 0.9442   Acc: 76.4762\n",
      "Epoch [25/310] Loss: 0.9147   Acc: 76.8095\n",
      "Epoch [26/310] Loss: 0.8867   Acc: 77.3810\n",
      "Epoch [27/310] Loss: 0.8602   Acc: 78.0238\n",
      "Epoch [28/310] Loss: 0.8350   Acc: 78.4762\n",
      "Epoch [29/310] Loss: 0.8108   Acc: 78.8333\n",
      "Epoch [30/310] Loss: 0.7876   Acc: 79.3571\n",
      "Epoch [31/310] Loss: 0.7654   Acc: 79.8095\n",
      "Epoch [32/310] Loss: 0.7439   Acc: 80.1548\n",
      "Epoch [33/310] Loss: 0.7233   Acc: 80.8571\n",
      "Epoch [34/310] Loss: 0.7036   Acc: 81.4048\n",
      "Epoch [35/310] Loss: 0.6847   Acc: 81.9167\n",
      "Epoch [36/310] Loss: 0.6666   Acc: 82.2500\n",
      "Epoch [37/310] Loss: 0.6493   Acc: 82.6071\n",
      "Epoch [38/310] Loss: 0.6328   Acc: 83.1190\n",
      "Epoch [39/310] Loss: 0.6169   Acc: 83.6548\n",
      "Epoch [40/310] Loss: 0.6016   Acc: 84.0476\n",
      "Epoch [41/310] Loss: 0.5869   Acc: 84.3929\n",
      "Epoch [42/310] Loss: 0.5727   Acc: 84.8452\n",
      "Epoch [43/310] Loss: 0.5591   Acc: 85.3690\n",
      "Epoch [44/310] Loss: 0.5460   Acc: 85.6071\n",
      "Epoch [45/310] Loss: 0.5335   Acc: 85.8690\n",
      "Epoch [46/310] Loss: 0.5214   Acc: 86.2024\n",
      "Epoch [47/310] Loss: 0.5099   Acc: 86.6548\n",
      "Epoch [48/310] Loss: 0.4988   Acc: 87.1548\n",
      "Epoch [49/310] Loss: 0.4882   Acc: 87.5000\n",
      "Epoch [50/310] Loss: 0.4780   Acc: 87.8333\n",
      "Epoch [51/310] Loss: 0.4682   Acc: 88.0833\n",
      "Epoch [52/310] Loss: 0.4588   Acc: 88.4524\n",
      "Epoch [53/310] Loss: 0.4497   Acc: 88.5952\n",
      "Epoch [54/310] Loss: 0.4410   Acc: 88.8333\n",
      "Epoch [55/310] Loss: 0.4326   Acc: 89.0238\n",
      "Epoch [56/310] Loss: 0.4245   Acc: 89.2262\n",
      "Epoch [57/310] Loss: 0.4167   Acc: 89.5000\n",
      "Epoch [58/310] Loss: 0.4092   Acc: 89.6667\n",
      "Epoch [59/310] Loss: 0.4019   Acc: 89.7143\n",
      "Epoch [60/310] Loss: 0.3949   Acc: 89.9643\n",
      "Epoch [61/310] Loss: 0.3881   Acc: 90.1310\n",
      "Epoch [62/310] Loss: 0.3816   Acc: 90.3333\n",
      "Epoch [63/310] Loss: 0.3753   Acc: 90.4762\n",
      "Epoch [64/310] Loss: 0.3692   Acc: 90.6071\n",
      "Epoch [65/310] Loss: 0.3632   Acc: 90.7381\n",
      "Epoch [66/310] Loss: 0.3575   Acc: 90.7976\n",
      "Epoch [67/310] Loss: 0.3520   Acc: 90.8690\n",
      "Epoch [68/310] Loss: 0.3466   Acc: 90.9524\n",
      "Epoch [69/310] Loss: 0.3413   Acc: 91.0476\n",
      "Epoch [70/310] Loss: 0.3363   Acc: 91.1786\n",
      "Epoch [71/310] Loss: 0.3314   Acc: 91.2143\n",
      "Epoch [72/310] Loss: 0.3266   Acc: 91.3333\n",
      "Epoch [73/310] Loss: 0.3220   Acc: 91.4524\n",
      "Epoch [74/310] Loss: 0.3174   Acc: 91.5476\n",
      "Epoch [75/310] Loss: 0.3131   Acc: 91.6786\n",
      "Epoch [76/310] Loss: 0.3088   Acc: 91.8214\n",
      "Epoch [77/310] Loss: 0.3046   Acc: 91.9286\n",
      "Epoch [78/310] Loss: 0.3006   Acc: 92.0952\n",
      "Epoch [79/310] Loss: 0.2967   Acc: 92.2024\n",
      "Epoch [80/310] Loss: 0.2928   Acc: 92.3095\n",
      "Epoch [81/310] Loss: 0.2891   Acc: 92.4048\n",
      "Epoch [82/310] Loss: 0.2855   Acc: 92.4881\n",
      "Epoch [83/310] Loss: 0.2819   Acc: 92.6548\n",
      "Epoch [84/310] Loss: 0.2784   Acc: 92.7500\n",
      "Epoch [85/310] Loss: 0.2750   Acc: 92.8810\n",
      "Epoch [86/310] Loss: 0.2717   Acc: 93.0357\n",
      "Epoch [87/310] Loss: 0.2685   Acc: 93.1071\n",
      "Epoch [88/310] Loss: 0.2653   Acc: 93.1667\n",
      "Epoch [89/310] Loss: 0.2622   Acc: 93.1548\n",
      "Epoch [90/310] Loss: 0.2592   Acc: 93.1667\n",
      "Epoch [91/310] Loss: 0.2562   Acc: 93.2143\n",
      "Epoch [92/310] Loss: 0.2533   Acc: 93.3095\n",
      "Epoch [93/310] Loss: 0.2505   Acc: 93.3333\n",
      "Epoch [94/310] Loss: 0.2477   Acc: 93.4643\n",
      "Epoch [95/310] Loss: 0.2450   Acc: 93.5238\n",
      "Epoch [96/310] Loss: 0.2423   Acc: 93.6548\n",
      "Epoch [97/310] Loss: 0.2397   Acc: 93.7143\n",
      "Epoch [98/310] Loss: 0.2372   Acc: 93.7976\n",
      "Epoch [99/310] Loss: 0.2347   Acc: 93.8452\n",
      "Epoch [100/310] Loss: 0.2322   Acc: 93.9405\n",
      "Epoch [101/310] Loss: 0.2298   Acc: 93.9524\n",
      "Epoch [102/310] Loss: 0.2274   Acc: 93.9643\n",
      "Epoch [103/310] Loss: 0.2251   Acc: 94.0000\n",
      "Epoch [104/310] Loss: 0.2228   Acc: 94.0357\n",
      "Epoch [105/310] Loss: 0.2206   Acc: 94.0833\n",
      "Epoch [106/310] Loss: 0.2184   Acc: 94.1548\n",
      "Epoch [107/310] Loss: 0.2162   Acc: 94.1905\n",
      "Epoch [108/310] Loss: 0.2141   Acc: 94.2143\n",
      "Epoch [109/310] Loss: 0.2120   Acc: 94.3214\n",
      "Epoch [110/310] Loss: 0.2099   Acc: 94.3929\n",
      "Epoch [111/310] Loss: 0.2079   Acc: 94.4405\n",
      "Epoch [112/310] Loss: 0.2059   Acc: 94.4762\n",
      "Epoch [113/310] Loss: 0.2039   Acc: 94.5833\n",
      "Epoch [114/310] Loss: 0.2020   Acc: 94.6310\n",
      "Epoch [115/310] Loss: 0.2001   Acc: 94.7024\n",
      "Epoch [116/310] Loss: 0.1983   Acc: 94.7857\n",
      "Epoch [117/310] Loss: 0.1964   Acc: 94.8571\n",
      "Epoch [118/310] Loss: 0.1946   Acc: 94.9167\n",
      "Epoch [119/310] Loss: 0.1928   Acc: 94.9405\n",
      "Epoch [120/310] Loss: 0.1911   Acc: 94.9643\n",
      "Epoch [121/310] Loss: 0.1894   Acc: 95.0476\n",
      "Epoch [122/310] Loss: 0.1876   Acc: 95.1071\n",
      "Epoch [123/310] Loss: 0.1860   Acc: 95.1786\n",
      "Epoch [124/310] Loss: 0.1843   Acc: 95.2024\n",
      "Epoch [125/310] Loss: 0.1827   Acc: 95.2976\n",
      "Epoch [126/310] Loss: 0.1811   Acc: 95.3571\n",
      "Epoch [127/310] Loss: 0.1795   Acc: 95.4048\n",
      "Epoch [128/310] Loss: 0.1779   Acc: 95.4286\n",
      "Epoch [129/310] Loss: 0.1763   Acc: 95.5476\n",
      "Epoch [130/310] Loss: 0.1748   Acc: 95.5595\n",
      "Epoch [131/310] Loss: 0.1733   Acc: 95.5595\n",
      "Epoch [132/310] Loss: 0.1718   Acc: 95.5833\n",
      "Epoch [133/310] Loss: 0.1703   Acc: 95.6310\n",
      "Epoch [134/310] Loss: 0.1689   Acc: 95.6905\n",
      "Epoch [135/310] Loss: 0.1674   Acc: 95.7262\n",
      "Epoch [136/310] Loss: 0.1660   Acc: 95.7500\n",
      "Epoch [137/310] Loss: 0.1646   Acc: 95.8214\n",
      "Epoch [138/310] Loss: 0.1632   Acc: 95.8690\n",
      "Epoch [139/310] Loss: 0.1619   Acc: 95.9643\n",
      "Epoch [140/310] Loss: 0.1605   Acc: 95.9881\n",
      "Epoch [141/310] Loss: 0.1592   Acc: 95.9643\n",
      "Epoch [142/310] Loss: 0.1579   Acc: 96.0476\n",
      "Epoch [143/310] Loss: 0.1566   Acc: 96.0476\n",
      "Epoch [144/310] Loss: 0.1553   Acc: 96.1548\n",
      "Epoch [145/310] Loss: 0.1540   Acc: 96.2024\n",
      "Epoch [146/310] Loss: 0.1528   Acc: 96.2500\n",
      "Epoch [147/310] Loss: 0.1515   Acc: 96.3690\n",
      "Epoch [148/310] Loss: 0.1503   Acc: 96.3810\n",
      "Epoch [149/310] Loss: 0.1491   Acc: 96.3929\n",
      "Epoch [150/310] Loss: 0.1479   Acc: 96.4167\n",
      "Epoch [151/310] Loss: 0.1467   Acc: 96.4167\n",
      "Epoch [152/310] Loss: 0.1455   Acc: 96.4524\n",
      "Epoch [153/310] Loss: 0.1444   Acc: 96.4643\n",
      "Epoch [154/310] Loss: 0.1432   Acc: 96.4643\n",
      "Epoch [155/310] Loss: 0.1421   Acc: 96.5119\n",
      "Epoch [156/310] Loss: 0.1410   Acc: 96.5833\n",
      "Epoch [157/310] Loss: 0.1399   Acc: 96.5833\n",
      "Epoch [158/310] Loss: 0.1388   Acc: 96.6190\n",
      "Epoch [159/310] Loss: 0.1377   Acc: 96.6429\n",
      "Epoch [160/310] Loss: 0.1366   Acc: 96.6667\n",
      "Epoch [161/310] Loss: 0.1355   Acc: 96.7024\n",
      "Epoch [162/310] Loss: 0.1345   Acc: 96.7024\n",
      "Epoch [163/310] Loss: 0.1335   Acc: 96.7500\n",
      "Epoch [164/310] Loss: 0.1324   Acc: 96.7738\n",
      "Epoch [165/310] Loss: 0.1314   Acc: 96.8333\n",
      "Epoch [166/310] Loss: 0.1304   Acc: 96.8452\n",
      "Epoch [167/310] Loss: 0.1294   Acc: 96.8571\n",
      "Epoch [168/310] Loss: 0.1284   Acc: 96.9405\n",
      "Epoch [169/310] Loss: 0.1274   Acc: 96.9643\n",
      "Epoch [170/310] Loss: 0.1265   Acc: 97.0000\n",
      "Epoch [171/310] Loss: 0.1255   Acc: 97.0000\n",
      "Epoch [172/310] Loss: 0.1246   Acc: 97.0476\n",
      "Epoch [173/310] Loss: 0.1236   Acc: 97.0833\n",
      "Epoch [174/310] Loss: 0.1227   Acc: 97.1071\n",
      "Epoch [175/310] Loss: 0.1218   Acc: 97.1310\n",
      "Epoch [176/310] Loss: 0.1209   Acc: 97.1548\n",
      "Epoch [177/310] Loss: 0.1200   Acc: 97.2143\n",
      "Epoch [178/310] Loss: 0.1191   Acc: 97.2262\n",
      "Epoch [179/310] Loss: 0.1182   Acc: 97.2857\n",
      "Epoch [180/310] Loss: 0.1173   Acc: 97.2857\n",
      "Epoch [181/310] Loss: 0.1165   Acc: 97.3333\n",
      "Epoch [182/310] Loss: 0.1156   Acc: 97.3929\n",
      "Epoch [183/310] Loss: 0.1147   Acc: 97.3929\n",
      "Epoch [184/310] Loss: 0.1139   Acc: 97.4167\n",
      "Epoch [185/310] Loss: 0.1131   Acc: 97.4167\n",
      "Epoch [186/310] Loss: 0.1122   Acc: 97.4167\n",
      "Epoch [187/310] Loss: 0.1114   Acc: 97.4762\n",
      "Epoch [188/310] Loss: 0.1106   Acc: 97.5238\n",
      "Epoch [189/310] Loss: 0.1098   Acc: 97.5357\n",
      "Epoch [190/310] Loss: 0.1090   Acc: 97.5714\n",
      "Epoch [191/310] Loss: 0.1082   Acc: 97.5714\n",
      "Epoch [192/310] Loss: 0.1075   Acc: 97.5833\n",
      "Epoch [193/310] Loss: 0.1067   Acc: 97.6071\n",
      "Epoch [194/310] Loss: 0.1059   Acc: 97.6310\n",
      "Epoch [195/310] Loss: 0.1052   Acc: 97.6310\n",
      "Epoch [196/310] Loss: 0.1044   Acc: 97.6310\n",
      "Epoch [197/310] Loss: 0.1037   Acc: 97.6429\n",
      "Epoch [198/310] Loss: 0.1030   Acc: 97.6786\n",
      "Epoch [199/310] Loss: 0.1022   Acc: 97.6786\n",
      "Epoch [200/310] Loss: 0.1015   Acc: 97.7500\n",
      "Epoch [201/310] Loss: 0.1008   Acc: 97.7976\n",
      "Epoch [202/310] Loss: 0.1001   Acc: 97.7976\n",
      "Epoch [203/310] Loss: 0.0994   Acc: 97.7976\n",
      "Epoch [204/310] Loss: 0.0987   Acc: 97.8214\n",
      "Epoch [205/310] Loss: 0.0980   Acc: 97.8333\n",
      "Epoch [206/310] Loss: 0.0973   Acc: 97.8333\n",
      "Epoch [207/310] Loss: 0.0967   Acc: 97.8095\n",
      "Epoch [208/310] Loss: 0.0960   Acc: 97.8214\n",
      "Epoch [209/310] Loss: 0.0954   Acc: 97.8214\n",
      "Epoch [210/310] Loss: 0.0947   Acc: 97.7976\n",
      "Epoch [211/310] Loss: 0.0941   Acc: 97.8214\n",
      "Epoch [212/310] Loss: 0.0934   Acc: 97.8571\n",
      "Epoch [213/310] Loss: 0.0928   Acc: 97.8690\n",
      "Epoch [214/310] Loss: 0.0922   Acc: 97.8929\n",
      "Epoch [215/310] Loss: 0.0915   Acc: 97.9048\n",
      "Epoch [216/310] Loss: 0.0909   Acc: 97.9524\n",
      "Epoch [217/310] Loss: 0.0903   Acc: 97.9643\n",
      "Epoch [218/310] Loss: 0.0897   Acc: 97.9762\n",
      "Epoch [219/310] Loss: 0.0891   Acc: 97.9762\n",
      "Epoch [220/310] Loss: 0.0885   Acc: 97.9762\n",
      "Epoch [221/310] Loss: 0.0879   Acc: 97.9643\n",
      "Epoch [222/310] Loss: 0.0873   Acc: 97.9643\n",
      "Epoch [223/310] Loss: 0.0868   Acc: 98.0000\n",
      "Epoch [224/310] Loss: 0.0862   Acc: 98.0238\n",
      "Epoch [225/310] Loss: 0.0856   Acc: 98.0595\n",
      "Epoch [226/310] Loss: 0.0851   Acc: 98.0952\n",
      "Epoch [227/310] Loss: 0.0845   Acc: 98.0952\n",
      "Epoch [228/310] Loss: 0.0840   Acc: 98.1190\n",
      "Epoch [229/310] Loss: 0.0834   Acc: 98.1429\n",
      "Epoch [230/310] Loss: 0.0829   Acc: 98.1548\n",
      "Epoch [231/310] Loss: 0.0823   Acc: 98.1548\n",
      "Epoch [232/310] Loss: 0.0818   Acc: 98.1548\n",
      "Epoch [233/310] Loss: 0.0813   Acc: 98.1548\n",
      "Epoch [234/310] Loss: 0.0807   Acc: 98.1786\n",
      "Epoch [235/310] Loss: 0.0802   Acc: 98.1905\n",
      "Epoch [236/310] Loss: 0.0797   Acc: 98.2143\n",
      "Epoch [237/310] Loss: 0.0792   Acc: 98.2262\n",
      "Epoch [238/310] Loss: 0.0787   Acc: 98.2500\n",
      "Epoch [239/310] Loss: 0.0782   Acc: 98.2738\n",
      "Epoch [240/310] Loss: 0.0777   Acc: 98.2857\n",
      "Epoch [241/310] Loss: 0.0772   Acc: 98.2857\n",
      "Epoch [242/310] Loss: 0.0767   Acc: 98.2857\n",
      "Epoch [243/310] Loss: 0.0762   Acc: 98.2857\n",
      "Epoch [244/310] Loss: 0.0757   Acc: 98.3095\n",
      "Epoch [245/310] Loss: 0.0753   Acc: 98.3095\n",
      "Epoch [246/310] Loss: 0.0748   Acc: 98.3095\n",
      "Epoch [247/310] Loss: 0.0743   Acc: 98.3214\n",
      "Epoch [248/310] Loss: 0.0739   Acc: 98.3452\n",
      "Epoch [249/310] Loss: 0.0734   Acc: 98.3571\n",
      "Epoch [250/310] Loss: 0.0729   Acc: 98.3690\n",
      "Epoch [251/310] Loss: 0.0725   Acc: 98.3690\n",
      "Epoch [252/310] Loss: 0.0720   Acc: 98.4048\n",
      "Epoch [253/310] Loss: 0.0716   Acc: 98.4286\n",
      "Epoch [254/310] Loss: 0.0711   Acc: 98.4524\n",
      "Epoch [255/310] Loss: 0.0707   Acc: 98.4524\n",
      "Epoch [256/310] Loss: 0.0703   Acc: 98.4524\n",
      "Epoch [257/310] Loss: 0.0698   Acc: 98.4524\n",
      "Epoch [258/310] Loss: 0.0694   Acc: 98.4643\n",
      "Epoch [259/310] Loss: 0.0690   Acc: 98.4881\n",
      "Epoch [260/310] Loss: 0.0686   Acc: 98.4881\n",
      "Epoch [261/310] Loss: 0.0681   Acc: 98.5000\n",
      "Epoch [262/310] Loss: 0.0677   Acc: 98.5238\n",
      "Epoch [263/310] Loss: 0.0673   Acc: 98.5238\n",
      "Epoch [264/310] Loss: 0.0669   Acc: 98.5595\n",
      "Epoch [265/310] Loss: 0.0665   Acc: 98.5595\n",
      "Epoch [266/310] Loss: 0.0661   Acc: 98.5595\n",
      "Epoch [267/310] Loss: 0.0657   Acc: 98.5595\n",
      "Epoch [268/310] Loss: 0.0653   Acc: 98.5833\n",
      "Epoch [269/310] Loss: 0.0649   Acc: 98.6310\n",
      "Epoch [270/310] Loss: 0.0645   Acc: 98.6548\n",
      "Epoch [271/310] Loss: 0.0642   Acc: 98.7143\n",
      "Epoch [272/310] Loss: 0.0638   Acc: 98.7143\n",
      "Epoch [273/310] Loss: 0.0634   Acc: 98.7143\n",
      "Epoch [274/310] Loss: 0.0630   Acc: 98.7262\n",
      "Epoch [275/310] Loss: 0.0626   Acc: 98.7381\n",
      "Epoch [276/310] Loss: 0.0623   Acc: 98.7619\n",
      "Epoch [277/310] Loss: 0.0619   Acc: 98.7738\n",
      "Epoch [278/310] Loss: 0.0615   Acc: 98.7857\n",
      "Epoch [279/310] Loss: 0.0612   Acc: 98.8333\n",
      "Epoch [280/310] Loss: 0.0608   Acc: 98.8333\n",
      "Epoch [281/310] Loss: 0.0605   Acc: 98.8333\n",
      "Epoch [282/310] Loss: 0.0601   Acc: 98.8452\n",
      "Epoch [283/310] Loss: 0.0598   Acc: 98.8571\n",
      "Epoch [284/310] Loss: 0.0594   Acc: 98.8810\n",
      "Epoch [285/310] Loss: 0.0591   Acc: 98.9048\n",
      "Epoch [286/310] Loss: 0.0587   Acc: 98.9048\n",
      "Epoch [287/310] Loss: 0.0584   Acc: 98.9286\n",
      "Epoch [288/310] Loss: 0.0580   Acc: 98.9524\n",
      "Epoch [289/310] Loss: 0.0577   Acc: 98.9762\n",
      "Epoch [290/310] Loss: 0.0574   Acc: 98.9881\n",
      "Epoch [291/310] Loss: 0.0570   Acc: 99.0000\n",
      "Epoch [292/310] Loss: 0.0567   Acc: 99.0238\n",
      "Epoch [293/310] Loss: 0.0564   Acc: 99.0238\n",
      "Epoch [294/310] Loss: 0.0560   Acc: 99.0238\n",
      "Epoch [295/310] Loss: 0.0557   Acc: 99.0238\n",
      "Epoch [296/310] Loss: 0.0554   Acc: 99.0238\n",
      "Epoch [297/310] Loss: 0.0551   Acc: 99.0357\n",
      "Epoch [298/310] Loss: 0.0548   Acc: 99.0595\n",
      "Epoch [299/310] Loss: 0.0545   Acc: 99.0833\n",
      "Epoch [300/310] Loss: 0.0542   Acc: 99.0833\n",
      "Epoch [301/310] Loss: 0.0539   Acc: 99.1190\n",
      "Epoch [302/310] Loss: 0.0535   Acc: 99.1429\n",
      "Epoch [303/310] Loss: 0.0532   Acc: 99.1667\n",
      "Epoch [304/310] Loss: 0.0529   Acc: 99.1786\n",
      "Epoch [305/310] Loss: 0.0526   Acc: 99.2143\n",
      "Epoch [306/310] Loss: 0.0523   Acc: 99.2262\n",
      "Epoch [307/310] Loss: 0.0521   Acc: 99.2262\n",
      "Epoch [308/310] Loss: 0.0518   Acc: 99.2262\n",
      "Epoch [309/310] Loss: 0.0515   Acc: 99.2262\n",
      "Epoch [310/310] Loss: 0.0512   Acc: 99.2262\n"
     ]
    }
   ],
   "source": [
    "\n",
    "X_train = torch.tensor(X_train, dtype=torch.float)\n",
    "y_train = torch.tensor(y_train, dtype=torch.long)\n",
    "X_test = torch.tensor(X_test, dtype=torch.float)\n",
    "y_test = torch.tensor(y_test, dtype=torch.long)\n",
    "net.train()\n",
    "#print(X_train.shape, y_train.shape)\n",
    "for epoch in range(num_epoch):\n",
    "    #feedforward - backprop\n",
    "    optimizer.zero_grad()\n",
    "    out = net(X_train)\n",
    "    loss = criterion(out, y_train)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    acc = 100 * torch.sum(y_train==torch.max(out.data, 1)[1]).double() / len(y_train)\n",
    "    print ('Epoch [%d/%d] Loss: %.4f   Acc: %.4f' \n",
    "                  %(epoch+1, num_epoch, loss.item(), acc.item()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 54
    },
    "colab_type": "code",
    "id": "jUKi1hQPjKpo",
    "outputId": "6a83f14a-927e-4052-b563-b81814b28983"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1, 6, 1, 9, 0, 9, 5, 8, 2, 4])\n",
      "Accuracy of the network 98.0833 %\n"
     ]
    }
   ],
   "source": [
    "#X_test = torch.tensor(X_test, dtype=torch.float)\n",
    "#y_test = torch.tensor(y_test, dtype=torch.long)\n",
    "net.eval()\n",
    "out = net(X_test)\n",
    "_, predicted = torch.max(out.data, 1)\n",
    "print(predicted[0:10])\n",
    "#get accuration\n",
    "print('Accuracy of the network %.4f %%' % (100 * torch.sum(y_test==predicted).double() / len(y_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 147
    },
    "colab_type": "code",
    "id": "XRjj_lVnkjFe",
    "outputId": "499233ed-75a5-4a48-9138-77d2cf6e96e6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([ 7,  3,  3, 10,  6,  3,  0,  7,  4,  5])\n",
      "Accuracy of the network 99.2262 %\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:1: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n",
      "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:2: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  \n"
     ]
    }
   ],
   "source": [
    "X_train = torch.tensor(X_train, dtype=torch.float)\n",
    "y_train= torch.tensor(y_train, dtype=torch.long)\n",
    "net.eval()\n",
    "out = net(X_train)\n",
    "_, predicted = torch.max(out.data, 1)\n",
    "print(predicted[0:10])\n",
    "#get accuration\n",
    "print('Accuracy of the network %.4f %%' % (100 * torch.sum(y_train==predicted).double() / len(y_train)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "sCU-z7q21iya"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "name": "Simple_ANN.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
