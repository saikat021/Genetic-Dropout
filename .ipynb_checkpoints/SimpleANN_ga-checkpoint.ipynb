{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "id": "ArFnLMfvvPAG",
    "outputId": "95d7b013-857d-47ba-f439-e366c70ae1b0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
     ]
    }
   ],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "fsqWgtdkweov"
   },
   "outputs": [],
   "source": [
    "import math\n",
    "import random\n",
    "from torchvision import datasets ,transforms\n",
    "from torchvision.datasets import ImageFolder\n",
    "from torch.utils.data import DataLoader,Dataset\n",
    "from torch import nn \n",
    "from torch.nn.functional import relu ,softmax \n",
    "import copy \n",
    "from torch.utils.data import Subset\n",
    "import pandas as pd \n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import time\n",
    "import torch\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import torch.nn.functional as F\n",
    "#device=torch.device(\"cuda:0\"if torch.cuda.is_available () else \"cpu\")\n",
    "device=\"cpu\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 256
    },
    "colab_type": "code",
    "id": "Bzm5sf5zwiV_",
    "outputId": "927911e5-363d-4f8b-abb5-a23101a2361a"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>att_1</th>\n",
       "      <th>att_2</th>\n",
       "      <th>att_3</th>\n",
       "      <th>att_4</th>\n",
       "      <th>att_5</th>\n",
       "      <th>att_6</th>\n",
       "      <th>att_7</th>\n",
       "      <th>att_8</th>\n",
       "      <th>att_9</th>\n",
       "      <th>att_10</th>\n",
       "      <th>att_11</th>\n",
       "      <th>att_12</th>\n",
       "      <th>att_13</th>\n",
       "      <th>att_14</th>\n",
       "      <th>att_15</th>\n",
       "      <th>att_16</th>\n",
       "      <th>att_17</th>\n",
       "      <th>att_18</th>\n",
       "      <th>att_19</th>\n",
       "      <th>att_20</th>\n",
       "      <th>att_21</th>\n",
       "      <th>att_22</th>\n",
       "      <th>att_23</th>\n",
       "      <th>att_24</th>\n",
       "      <th>att_25</th>\n",
       "      <th>att_26</th>\n",
       "      <th>att_27</th>\n",
       "      <th>att_28</th>\n",
       "      <th>att_29</th>\n",
       "      <th>att_30</th>\n",
       "      <th>att_31</th>\n",
       "      <th>att_32</th>\n",
       "      <th>att_33</th>\n",
       "      <th>att_34</th>\n",
       "      <th>att_35</th>\n",
       "      <th>att_36</th>\n",
       "      <th>att_37</th>\n",
       "      <th>att_38</th>\n",
       "      <th>att_39</th>\n",
       "      <th>att_40</th>\n",
       "      <th>...</th>\n",
       "      <th>att_142</th>\n",
       "      <th>att_143</th>\n",
       "      <th>att_144</th>\n",
       "      <th>att_145</th>\n",
       "      <th>att_146</th>\n",
       "      <th>att_147</th>\n",
       "      <th>att_148</th>\n",
       "      <th>att_149</th>\n",
       "      <th>att_150</th>\n",
       "      <th>att_151</th>\n",
       "      <th>att_152</th>\n",
       "      <th>att_153</th>\n",
       "      <th>att_154</th>\n",
       "      <th>att_155</th>\n",
       "      <th>att_156</th>\n",
       "      <th>att_157</th>\n",
       "      <th>att_158</th>\n",
       "      <th>att_159</th>\n",
       "      <th>att_160</th>\n",
       "      <th>att_161</th>\n",
       "      <th>att_162</th>\n",
       "      <th>att_163</th>\n",
       "      <th>att_164</th>\n",
       "      <th>att_165</th>\n",
       "      <th>att_166</th>\n",
       "      <th>att_167</th>\n",
       "      <th>att_168</th>\n",
       "      <th>att_169</th>\n",
       "      <th>att_170</th>\n",
       "      <th>att_171</th>\n",
       "      <th>att_172</th>\n",
       "      <th>att_173</th>\n",
       "      <th>att_174</th>\n",
       "      <th>att_175</th>\n",
       "      <th>att_176</th>\n",
       "      <th>att_177</th>\n",
       "      <th>att_178</th>\n",
       "      <th>att_179</th>\n",
       "      <th>att_180</th>\n",
       "      <th>class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>12.184764</td>\n",
       "      <td>18.534243</td>\n",
       "      <td>28.945810</td>\n",
       "      <td>22.241393</td>\n",
       "      <td>7.353465</td>\n",
       "      <td>17.095399</td>\n",
       "      <td>26.996429</td>\n",
       "      <td>41.427220</td>\n",
       "      <td>24.394928</td>\n",
       "      <td>3.888953</td>\n",
       "      <td>16.692057</td>\n",
       "      <td>26.119638</td>\n",
       "      <td>39.722458</td>\n",
       "      <td>23.561138</td>\n",
       "      <td>4.422741</td>\n",
       "      <td>11.639512</td>\n",
       "      <td>20.120012</td>\n",
       "      <td>29.659610</td>\n",
       "      <td>20.367249</td>\n",
       "      <td>6.726386</td>\n",
       "      <td>14.140901</td>\n",
       "      <td>24.451595</td>\n",
       "      <td>33.493874</td>\n",
       "      <td>21.272691</td>\n",
       "      <td>8.236757</td>\n",
       "      <td>16.860680</td>\n",
       "      <td>31.564917</td>\n",
       "      <td>40.518904</td>\n",
       "      <td>23.521893</td>\n",
       "      <td>9.398887</td>\n",
       "      <td>15.659533</td>\n",
       "      <td>30.509884</td>\n",
       "      <td>36.151632</td>\n",
       "      <td>24.099052</td>\n",
       "      <td>9.867436</td>\n",
       "      <td>14.418768</td>\n",
       "      <td>29.246418</td>\n",
       "      <td>34.769063</td>\n",
       "      <td>22.928747</td>\n",
       "      <td>17.098696</td>\n",
       "      <td>...</td>\n",
       "      <td>0.011635</td>\n",
       "      <td>0.014132</td>\n",
       "      <td>0.015190</td>\n",
       "      <td>0.018073</td>\n",
       "      <td>0.007951</td>\n",
       "      <td>0.011116</td>\n",
       "      <td>0.012599</td>\n",
       "      <td>0.011804</td>\n",
       "      <td>0.009471</td>\n",
       "      <td>0.007398</td>\n",
       "      <td>0.011174</td>\n",
       "      <td>0.013660</td>\n",
       "      <td>0.012094</td>\n",
       "      <td>0.007816</td>\n",
       "      <td>0.006827</td>\n",
       "      <td>0.011113</td>\n",
       "      <td>0.014481</td>\n",
       "      <td>0.013526</td>\n",
       "      <td>0.008810</td>\n",
       "      <td>0.007137</td>\n",
       "      <td>0.010558</td>\n",
       "      <td>0.015235</td>\n",
       "      <td>0.014294</td>\n",
       "      <td>0.007546</td>\n",
       "      <td>0.007302</td>\n",
       "      <td>0.010320</td>\n",
       "      <td>0.014466</td>\n",
       "      <td>0.014560</td>\n",
       "      <td>0.008232</td>\n",
       "      <td>0.007450</td>\n",
       "      <td>0.010558</td>\n",
       "      <td>0.014658</td>\n",
       "      <td>0.016326</td>\n",
       "      <td>0.009556</td>\n",
       "      <td>0.007948</td>\n",
       "      <td>0.011146</td>\n",
       "      <td>0.016568</td>\n",
       "      <td>0.017170</td>\n",
       "      <td>0.008511</td>\n",
       "      <td>A</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>25.639720</td>\n",
       "      <td>46.945876</td>\n",
       "      <td>63.464762</td>\n",
       "      <td>44.700204</td>\n",
       "      <td>23.165613</td>\n",
       "      <td>18.646121</td>\n",
       "      <td>28.515218</td>\n",
       "      <td>41.926007</td>\n",
       "      <td>29.490375</td>\n",
       "      <td>8.800943</td>\n",
       "      <td>13.905585</td>\n",
       "      <td>20.948323</td>\n",
       "      <td>29.197588</td>\n",
       "      <td>24.777035</td>\n",
       "      <td>11.693705</td>\n",
       "      <td>12.900319</td>\n",
       "      <td>23.106127</td>\n",
       "      <td>28.827459</td>\n",
       "      <td>20.651541</td>\n",
       "      <td>12.113402</td>\n",
       "      <td>16.001618</td>\n",
       "      <td>28.730252</td>\n",
       "      <td>33.722841</td>\n",
       "      <td>22.691698</td>\n",
       "      <td>14.884630</td>\n",
       "      <td>12.886395</td>\n",
       "      <td>22.841184</td>\n",
       "      <td>29.999213</td>\n",
       "      <td>21.743363</td>\n",
       "      <td>14.644041</td>\n",
       "      <td>10.872357</td>\n",
       "      <td>17.554802</td>\n",
       "      <td>26.507216</td>\n",
       "      <td>25.717512</td>\n",
       "      <td>12.146767</td>\n",
       "      <td>12.578959</td>\n",
       "      <td>20.806199</td>\n",
       "      <td>30.296498</td>\n",
       "      <td>26.063166</td>\n",
       "      <td>11.027926</td>\n",
       "      <td>...</td>\n",
       "      <td>0.018399</td>\n",
       "      <td>0.024574</td>\n",
       "      <td>0.023404</td>\n",
       "      <td>0.019674</td>\n",
       "      <td>0.010260</td>\n",
       "      <td>0.016181</td>\n",
       "      <td>0.021939</td>\n",
       "      <td>0.023295</td>\n",
       "      <td>0.019952</td>\n",
       "      <td>0.010074</td>\n",
       "      <td>0.015923</td>\n",
       "      <td>0.022831</td>\n",
       "      <td>0.022867</td>\n",
       "      <td>0.014010</td>\n",
       "      <td>0.009507</td>\n",
       "      <td>0.014709</td>\n",
       "      <td>0.020469</td>\n",
       "      <td>0.019867</td>\n",
       "      <td>0.012323</td>\n",
       "      <td>0.009276</td>\n",
       "      <td>0.014710</td>\n",
       "      <td>0.020125</td>\n",
       "      <td>0.019822</td>\n",
       "      <td>0.009504</td>\n",
       "      <td>0.009147</td>\n",
       "      <td>0.014281</td>\n",
       "      <td>0.020338</td>\n",
       "      <td>0.019589</td>\n",
       "      <td>0.010720</td>\n",
       "      <td>0.009711</td>\n",
       "      <td>0.014752</td>\n",
       "      <td>0.022414</td>\n",
       "      <td>0.024294</td>\n",
       "      <td>0.014430</td>\n",
       "      <td>0.010937</td>\n",
       "      <td>0.016972</td>\n",
       "      <td>0.025050</td>\n",
       "      <td>0.023918</td>\n",
       "      <td>0.014216</td>\n",
       "      <td>A</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>14.381637</td>\n",
       "      <td>24.706305</td>\n",
       "      <td>35.822925</td>\n",
       "      <td>29.553382</td>\n",
       "      <td>22.935236</td>\n",
       "      <td>13.837493</td>\n",
       "      <td>22.689212</td>\n",
       "      <td>36.892506</td>\n",
       "      <td>33.376486</td>\n",
       "      <td>9.703304</td>\n",
       "      <td>11.278967</td>\n",
       "      <td>19.369300</td>\n",
       "      <td>29.284697</td>\n",
       "      <td>28.393769</td>\n",
       "      <td>8.605397</td>\n",
       "      <td>10.737174</td>\n",
       "      <td>18.724029</td>\n",
       "      <td>22.954104</td>\n",
       "      <td>22.718452</td>\n",
       "      <td>12.324788</td>\n",
       "      <td>13.249558</td>\n",
       "      <td>26.019134</td>\n",
       "      <td>26.479294</td>\n",
       "      <td>21.735270</td>\n",
       "      <td>13.723333</td>\n",
       "      <td>14.321044</td>\n",
       "      <td>38.548935</td>\n",
       "      <td>47.383356</td>\n",
       "      <td>28.481238</td>\n",
       "      <td>20.100181</td>\n",
       "      <td>13.700423</td>\n",
       "      <td>34.864279</td>\n",
       "      <td>46.987831</td>\n",
       "      <td>27.429569</td>\n",
       "      <td>25.531466</td>\n",
       "      <td>14.113249</td>\n",
       "      <td>32.314696</td>\n",
       "      <td>43.158068</td>\n",
       "      <td>27.469746</td>\n",
       "      <td>19.794746</td>\n",
       "      <td>...</td>\n",
       "      <td>0.014387</td>\n",
       "      <td>0.020853</td>\n",
       "      <td>0.022117</td>\n",
       "      <td>0.020079</td>\n",
       "      <td>0.009182</td>\n",
       "      <td>0.013964</td>\n",
       "      <td>0.018120</td>\n",
       "      <td>0.020490</td>\n",
       "      <td>0.016575</td>\n",
       "      <td>0.009079</td>\n",
       "      <td>0.013714</td>\n",
       "      <td>0.017907</td>\n",
       "      <td>0.018449</td>\n",
       "      <td>0.016712</td>\n",
       "      <td>0.008954</td>\n",
       "      <td>0.014038</td>\n",
       "      <td>0.018255</td>\n",
       "      <td>0.017512</td>\n",
       "      <td>0.014393</td>\n",
       "      <td>0.008906</td>\n",
       "      <td>0.014206</td>\n",
       "      <td>0.017487</td>\n",
       "      <td>0.016339</td>\n",
       "      <td>0.014050</td>\n",
       "      <td>0.008712</td>\n",
       "      <td>0.013781</td>\n",
       "      <td>0.017666</td>\n",
       "      <td>0.019439</td>\n",
       "      <td>0.016598</td>\n",
       "      <td>0.008739</td>\n",
       "      <td>0.013782</td>\n",
       "      <td>0.019098</td>\n",
       "      <td>0.024171</td>\n",
       "      <td>0.020194</td>\n",
       "      <td>0.009770</td>\n",
       "      <td>0.014811</td>\n",
       "      <td>0.021661</td>\n",
       "      <td>0.021909</td>\n",
       "      <td>0.015823</td>\n",
       "      <td>A</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>20.123721</td>\n",
       "      <td>39.021980</td>\n",
       "      <td>59.427789</td>\n",
       "      <td>33.768032</td>\n",
       "      <td>16.884216</td>\n",
       "      <td>14.235512</td>\n",
       "      <td>24.346061</td>\n",
       "      <td>33.150834</td>\n",
       "      <td>17.558946</td>\n",
       "      <td>5.433499</td>\n",
       "      <td>11.368576</td>\n",
       "      <td>16.641580</td>\n",
       "      <td>16.312035</td>\n",
       "      <td>12.704722</td>\n",
       "      <td>7.584464</td>\n",
       "      <td>11.794634</td>\n",
       "      <td>17.600989</td>\n",
       "      <td>15.405322</td>\n",
       "      <td>12.884437</td>\n",
       "      <td>18.272439</td>\n",
       "      <td>12.702698</td>\n",
       "      <td>21.817848</td>\n",
       "      <td>21.906229</td>\n",
       "      <td>15.560347</td>\n",
       "      <td>13.799468</td>\n",
       "      <td>11.923086</td>\n",
       "      <td>23.846825</td>\n",
       "      <td>27.034937</td>\n",
       "      <td>21.597422</td>\n",
       "      <td>14.734907</td>\n",
       "      <td>10.231710</td>\n",
       "      <td>20.986869</td>\n",
       "      <td>27.847546</td>\n",
       "      <td>21.334325</td>\n",
       "      <td>9.338959</td>\n",
       "      <td>11.167240</td>\n",
       "      <td>20.317803</td>\n",
       "      <td>29.955248</td>\n",
       "      <td>24.897731</td>\n",
       "      <td>9.992332</td>\n",
       "      <td>...</td>\n",
       "      <td>0.015341</td>\n",
       "      <td>0.020819</td>\n",
       "      <td>0.020321</td>\n",
       "      <td>0.021944</td>\n",
       "      <td>0.009273</td>\n",
       "      <td>0.013473</td>\n",
       "      <td>0.017387</td>\n",
       "      <td>0.016286</td>\n",
       "      <td>0.012100</td>\n",
       "      <td>0.009066</td>\n",
       "      <td>0.013612</td>\n",
       "      <td>0.019132</td>\n",
       "      <td>0.019185</td>\n",
       "      <td>0.010803</td>\n",
       "      <td>0.008737</td>\n",
       "      <td>0.013403</td>\n",
       "      <td>0.018748</td>\n",
       "      <td>0.017529</td>\n",
       "      <td>0.009745</td>\n",
       "      <td>0.008372</td>\n",
       "      <td>0.013575</td>\n",
       "      <td>0.018175</td>\n",
       "      <td>0.015580</td>\n",
       "      <td>0.012225</td>\n",
       "      <td>0.008549</td>\n",
       "      <td>0.013577</td>\n",
       "      <td>0.020318</td>\n",
       "      <td>0.018893</td>\n",
       "      <td>0.015797</td>\n",
       "      <td>0.009185</td>\n",
       "      <td>0.014727</td>\n",
       "      <td>0.022686</td>\n",
       "      <td>0.022876</td>\n",
       "      <td>0.013031</td>\n",
       "      <td>0.010639</td>\n",
       "      <td>0.016706</td>\n",
       "      <td>0.026014</td>\n",
       "      <td>0.023591</td>\n",
       "      <td>0.012991</td>\n",
       "      <td>A</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>20.855774</td>\n",
       "      <td>31.241891</td>\n",
       "      <td>46.056155</td>\n",
       "      <td>36.617243</td>\n",
       "      <td>20.152911</td>\n",
       "      <td>15.612698</td>\n",
       "      <td>27.206975</td>\n",
       "      <td>42.754484</td>\n",
       "      <td>29.983009</td>\n",
       "      <td>7.457517</td>\n",
       "      <td>11.662058</td>\n",
       "      <td>18.905805</td>\n",
       "      <td>31.409227</td>\n",
       "      <td>24.829528</td>\n",
       "      <td>4.523870</td>\n",
       "      <td>12.044599</td>\n",
       "      <td>25.853903</td>\n",
       "      <td>34.339422</td>\n",
       "      <td>17.169122</td>\n",
       "      <td>6.675385</td>\n",
       "      <td>11.463279</td>\n",
       "      <td>29.172892</td>\n",
       "      <td>33.184479</td>\n",
       "      <td>14.121998</td>\n",
       "      <td>8.962364</td>\n",
       "      <td>11.977819</td>\n",
       "      <td>31.257173</td>\n",
       "      <td>36.656117</td>\n",
       "      <td>19.558887</td>\n",
       "      <td>13.133167</td>\n",
       "      <td>13.351911</td>\n",
       "      <td>32.172718</td>\n",
       "      <td>41.949929</td>\n",
       "      <td>30.115787</td>\n",
       "      <td>14.969102</td>\n",
       "      <td>14.343485</td>\n",
       "      <td>35.341644</td>\n",
       "      <td>44.988423</td>\n",
       "      <td>27.579509</td>\n",
       "      <td>9.464650</td>\n",
       "      <td>...</td>\n",
       "      <td>0.015889</td>\n",
       "      <td>0.022721</td>\n",
       "      <td>0.022730</td>\n",
       "      <td>0.022472</td>\n",
       "      <td>0.010109</td>\n",
       "      <td>0.015527</td>\n",
       "      <td>0.022441</td>\n",
       "      <td>0.023648</td>\n",
       "      <td>0.013539</td>\n",
       "      <td>0.010414</td>\n",
       "      <td>0.015549</td>\n",
       "      <td>0.021899</td>\n",
       "      <td>0.022117</td>\n",
       "      <td>0.011333</td>\n",
       "      <td>0.010250</td>\n",
       "      <td>0.015448</td>\n",
       "      <td>0.021103</td>\n",
       "      <td>0.018298</td>\n",
       "      <td>0.010251</td>\n",
       "      <td>0.010060</td>\n",
       "      <td>0.014660</td>\n",
       "      <td>0.020015</td>\n",
       "      <td>0.017280</td>\n",
       "      <td>0.009596</td>\n",
       "      <td>0.009790</td>\n",
       "      <td>0.013656</td>\n",
       "      <td>0.017717</td>\n",
       "      <td>0.016867</td>\n",
       "      <td>0.011999</td>\n",
       "      <td>0.010235</td>\n",
       "      <td>0.014497</td>\n",
       "      <td>0.019523</td>\n",
       "      <td>0.023203</td>\n",
       "      <td>0.016096</td>\n",
       "      <td>0.011517</td>\n",
       "      <td>0.016587</td>\n",
       "      <td>0.024086</td>\n",
       "      <td>0.022221</td>\n",
       "      <td>0.015074</td>\n",
       "      <td>A</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 181 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       att_1      att_2      att_3  ...   att_179   att_180  class\n",
       "0  12.184764  18.534243  28.945810  ...  0.017170  0.008511      A\n",
       "1  25.639720  46.945876  63.464762  ...  0.023918  0.014216      A\n",
       "2  14.381637  24.706305  35.822925  ...  0.021909  0.015823      A\n",
       "3  20.123721  39.021980  59.427789  ...  0.023591  0.012991      A\n",
       "4  20.855774  31.241891  46.056155  ...  0.022221  0.015074      A\n",
       "\n",
       "[5 rows x 181 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "path ='drive/My Drive/csvUCI/Riktim_Testing/Word-level-Features/MLG_180_Features.csv'\n",
    "df = pd.read_csv(path) #header=None\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "id": "q9lhL3iVwyc6",
    "outputId": "1e35db16-a4c7-4523-d023-9e9d6feb633d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(12000, 181)\n"
     ]
    }
   ],
   "source": [
    "print(df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 127
    },
    "colab_type": "code",
    "id": "S4K9LYdmw4iW",
    "outputId": "6a2c8931-b341-4de4-b864-c9d9771e1595"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(12000, 181)\n",
      "<class 'numpy.ndarray'>\n",
      "(12000, 180)\n",
      "\n",
      "['A' 'B' 'C' 'D' 'E' 'F' 'G' 'H' 'I' 'J' 'K' 'L']\n",
      "[0 0 0 0 0 0 0 0 0 0]\n"
     ]
    }
   ],
   "source": [
    "print(df.shape)\n",
    "#X=df.loc[:,df.columns != 'Classifier'].values\n",
    "X=df.loc[:,df.columns != df.columns[-1]].values\n",
    "print(type(X))\n",
    "print(X.shape)\n",
    "#y_label=df['Classifier'].values \n",
    "y_label =df.iloc[:,-1].values\n",
    "#print(type(y_label))\n",
    "#print(X[0])\n",
    "print()\n",
    "from sklearn import preprocessing\n",
    "le = preprocessing.LabelEncoder()\n",
    "le.fit(y_label)\n",
    "print(le.classes_)\n",
    "y=le.transform(y_label)\n",
    "print(y[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 54
    },
    "colab_type": "code",
    "id": "ASaktl2pxAXk",
    "outputId": "95a10aee-01f1-4229-ffb4-fbd5825700d3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11} {0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11}\n",
      "(12000, 180) (12000,)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler #for normalization\n",
    "sc = StandardScaler()\n",
    "X = sc.fit_transform(X)\n",
    "# from sklearn.preprocessing import MinMaxScaler\n",
    "# scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "# X = scaler.fit_transform(X)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42, shuffle=True)\n",
    "# print(y_train)\n",
    "# print(y_test)\n",
    "#print(len(X_train), len(y_train), len(X_test), len(y_test))\n",
    "# # X=X_train\n",
    "# # y=y_train\n",
    "print(set(y_train), set(y_test))\n",
    "print(X.shape, y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "IU4XOA5fy_4d"
   },
   "outputs": [],
   "source": [
    "X_train = torch.tensor(X_train, dtype=torch.float)\n",
    "y_train = torch.tensor(y_train, dtype=torch.long)\n",
    "X_test = torch.tensor(X_test, dtype=torch.float)\n",
    "y_test = torch.tensor(y_test, dtype=torch.long)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "AeGznOD8xEPs"
   },
   "outputs": [],
   "source": [
    "#n_inputs = X_train.shape[1]\n",
    "#n_hidden = 180\n",
    "#n_classes = len(le.classes_)\n",
    "#N = X_train.shape[0] # Number of samples\n",
    "\n",
    "#print(N)\n",
    "#hyperparameters\n",
    "input_size = X_train.shape[1]    # The image size = 28 x 28 = 784\n",
    "hidden_size = X_train.shape[1]    # The number of nodes at the hidden layer\n",
    "num_classes = len(le.classes_)       # The number of output classes. In this case, from 0 to 9\n",
    "num_epochs = 100      # The number of times entire dataset is trained\n",
    "batch_size = X_train.shape[0]    # The size of input data took for one iteration\n",
    "lr = 0.001  # The speed of convergence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "fatO2VKDxILM"
   },
   "outputs": [],
   "source": [
    "# #build model\n",
    "# class Net(nn.Module):\n",
    "\n",
    "#     def __init__(self):\n",
    "#         super(Net, self).__init__()\n",
    "#         self.fc1 = nn.Linear(n_inputs, n_hidden)\n",
    "#         self.drop = nn.Dropout (p=0.5)\n",
    "#         self.fc2 = nn.Linear(n_hidden,n_classes )\n",
    "\n",
    "#     def forward(self, x):\n",
    "#         x = F.relu(self.fc1(x))\n",
    "#         #x=self.drop(x)\n",
    "#         x = self.fc2(x)\n",
    "#         return x\n",
    "        \n",
    "# net = Net()\n",
    "class Model(nn.Module):\n",
    "    def __init__(self,input_size, hidden_size, num_classes):\n",
    "        super(Model, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_size , hidden_size)\n",
    "        self.fc2 = nn.Linear(hidden_size, num_classes)\n",
    "        self.drop = nn.Dropout (p=0.4)\n",
    "\n",
    "    def masking (self,act1,mask,p):\n",
    "        if (self.training ==True ):\n",
    "            return ((act1*mask)/p)\n",
    "        else :\n",
    "            return (act1)\n",
    "        \n",
    "\n",
    "    def Forward(self, x,mask,p):\n",
    "        #print(x.shape)\n",
    "        #x = x.view(-1, 32 * 32 * 3)\n",
    "        #print(x.shape)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        #x = self.drop(x)\n",
    "        x = self.masking (x,mask,p)\n",
    "        x = self.fc2(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "xnf3Kd39xosT"
   },
   "outputs": [],
   "source": [
    "def train_model(model, criterion, optimizer,mask,p, num_epochs=25):\n",
    "  # the mask has to be specified \n",
    "      since = time.time()\n",
    "\n",
    "      best_model_wts = copy.deepcopy(model.state_dict())\n",
    "      best_acc = 0.0\n",
    "      losses=[]\n",
    "      accuracies=[]\n",
    "\n",
    "      for epoch in range(num_epochs):\n",
    "          print('Epoch {}/{}'.format(epoch, num_epochs - 1))\n",
    "          print('-' * 10)\n",
    "\n",
    "          # Each epoch has a training and validation phase\n",
    "          for phase in ['train', 'val']:\n",
    "              if phase == 'train':\n",
    "                  model.train()  # Set model to training mode\n",
    "                  optimizer.zero_grad()\n",
    "                  with torch.set_grad_enabled(phase == 'train'):\n",
    "                    outputs= model.Forward(X_train,mask,p) #inputs\n",
    "                    train_loss = criterion(outputs, y_train)# backward + optimize only if in training phase,labels\n",
    "                    train_loss.backward()\n",
    "                    optimizer.step()\n",
    "                  train_acc = 100 * torch.sum(y_train==torch.max(outputs.data, 1)[1]).double() / len(y_train)\n",
    "                  print('********************{}*************'.format(phase))\n",
    "                  print ('Epoch [%d/%d] Loss: %.4f   Acc: %.4f'%(epoch+1, num_epochs, train_loss.item(), train_acc.item()))\n",
    "                  losses.append(train_loss)\n",
    "                  accuracies.append(train_acc)\n",
    "              else:\n",
    "                  model.eval()   # Set model to evaluate mode\n",
    "                  outputs= model.Forward(X_test,mask,p) #inputs\n",
    "                  _, predicted = torch.max(outputs, 1)\n",
    "                  test_loss = criterion(outputs, y_test)\n",
    "                  test_acc = 100 * torch.sum(y_test==predicted).double() / len(y_test)\n",
    "                  print('********************{}*************'.format(phase))\n",
    "                  print ('Epoch [%d/%d] Loss: %.4f   Acc: %.4f'%(epoch+1, num_epochs, test_loss.item(), test_acc.item()))\n",
    "                  if test_acc > best_acc:\n",
    "                    best_acc = test_acc\n",
    "                    best_model_wts = copy.deepcopy(model.state_dict())\n",
    "\n",
    "              \n",
    "\n",
    "\n",
    "\n",
    "      time_elapsed = time.time() - since\n",
    "      print('Training complete in {:.0f}m {:.0f}s'.format(\n",
    "          time_elapsed // 60, time_elapsed % 60))\n",
    "      print('Best val Acc: {:4f}'.format(best_acc))\n",
    "\n",
    "      # load best model weights\n",
    "      model.load_state_dict(best_model_wts)\n",
    "      return model,losses,accuracies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "_rFn0uEUxvQt"
   },
   "outputs": [],
   "source": [
    "class Population:   \n",
    "    def __init__(self,m,num,maskLength):\n",
    "        # constructor for initialising the population list\n",
    "        #list of DNA objects\n",
    "        self.population=[]\n",
    "        #muation rate for mutation\n",
    "        self.mutation_rate=m\n",
    "        #maximum number of entities in the population\n",
    "        self.popmax=num\n",
    "        self.maskLength=maskLength\n",
    "        for i in range (num):\n",
    "            #creating a dna object\n",
    "            #an initial random population created \n",
    "            dna =DNA(self.maskLength)\n",
    "            self.population.append (dna)\n",
    "      \n",
    "        self.matingPool=[]\n",
    "\n",
    "    def calcFitness (self,model):\n",
    "        # going through all the entities of population \n",
    "        #finding fitness of all population entities \n",
    "        for i in range(0,self.popmax):\n",
    "            self.population[i].fitness (model)\n",
    "\n",
    "    def naturalSelection(self):\n",
    "        self.matingPool=[]\n",
    "        maxFitness=0\n",
    "        for i in range (self.popmax):\n",
    "            # moving throught the entire population \n",
    "            if (self.population[i].fit>maxFitness):\n",
    "                maxFitness=self.population[i].fit\n",
    "       \n",
    "        # max Fitness has the maximum loss score of the entire population  \n",
    "        for i in range (self.popmax ):\n",
    "        # iterating through the all inviduals of the population\n",
    "            n=self.Mymap(self.population[i].fit,0,maxFitness,0,1)\n",
    "        \n",
    "            n=math.floor(n*100)\n",
    "            \n",
    "            for j in range (n):\n",
    "                #creating mating pool\n",
    "                self.matingPool.append (self.population[i])\n",
    "\n",
    "    def Mymap(self,num,prevlow,prevhigh,nextlow,nexthigh):\n",
    "        \n",
    "        prevrange =float((num-prevlow)/(prevhigh-prevlow))\n",
    "        return nextlow+(nexthigh-nextlow)*prevrange\n",
    "\n",
    "    def   generate (self):\n",
    "        for i in range (self.popmax ):\n",
    "            index_1=math.floor(random.randint  (0,len(self.matingPool)-1))\n",
    "            index_2=math.floor (random.randint (0,len(self.matingPool)-1))\n",
    "            parent1=self.matingPool[index_1]\n",
    "            parent2=self.matingPool[index_2]\n",
    "            child=parent1.crossover(parent2)\n",
    "            child.mutate(self.mutation_rate)\n",
    "            self.population[i]=child \n",
    "\n",
    "    def fittest(self):\n",
    "        #returns the fiitest individual mask of the population \n",
    "        #also returns the keeping probability of the fittest mask \n",
    "        fittest=self.population[0]\n",
    "        for i  in range (self.popmax):\n",
    "            if (fittest.fit<self.population[i].fit):\n",
    "                fittest=self.population[i]\n",
    "        return fittest,fittest.keep_prob()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "dcIsijbQxxbM"
   },
   "outputs": [],
   "source": [
    "class DNA:  \n",
    "    def __init__(self,maskLength):\n",
    "        #constructor for the creation of the mask as a gene object \n",
    "        self.maskLength=maskLength\n",
    "        #creation of mask \n",
    "        self.gene=torch.bernoulli(torch.empty(1,maskLength).uniform_(0,1))\n",
    "        self.fit=0\n",
    "\n",
    "    def keep_prob (self):\n",
    "        num_one =0\n",
    "        for i in range (self.maskLength):\n",
    "            if (self.gene[0,i]==1):\n",
    "                num_one=num_one+1\n",
    "        return float(num_one/self.maskLength)\n",
    "\n",
    "    def fitness(self,model):\n",
    "        # finding the fitness of a particular mask\n",
    "        #accuracy of all training set is the fitness in one epoch\n",
    "        #putting model in train mode \n",
    "        running_loss=0\n",
    "        running_corrects=0\n",
    "        model.train()\n",
    "\n",
    "        # for inputs,labels in dataloader ['train']:   ->>>>>>>>>>>>>>changed by riktim\n",
    "        #     inputs=inputs.to(device)\n",
    "        #     labels=labels.to(device)\n",
    "        #     outputs=model.Forward(inputs,self.gene,self.keep_prob())\n",
    "        #     _,preds=torch.max(outputs,1)\n",
    "        #     loss=criterion (outputs,labels)\n",
    "        #     running_loss += loss.item() * inputs.size(0)\n",
    "        #     running_corrects += torch.sum(preds == labels.data)\n",
    "        # epoch_loss = running_loss / dataset_sizes['train']\n",
    "        # epoch_acc = running_corrects.double() / dataset_sizes['train']      \n",
    "        # self.fit=epoch_acc\n",
    "        # return epoch_acc\n",
    "        \n",
    "        outputs=model.Forward(X_train,self.gene,self.keep_prob())\n",
    "        _,preds=torch.max(outputs,1)\n",
    "        loss=criterion (outputs,y_train)\n",
    "        acc = 100*torch.sum(y_train==torch.max(outputs.data, 1)[1]).double() / len(y_train)\n",
    "        self.fit=acc\n",
    "        return acc\n",
    "        \n",
    "\n",
    "    def crossover (self,parent2):\n",
    "        #one parent is the passed in the argument \n",
    "        #another parent is the one from which this function is called \n",
    "        #another parent is self.gene\n",
    "        child =DNA(self.maskLength)\n",
    "        midpoint =random .randint (0,self.maskLength-1)\n",
    "        for i in range (0,self.maskLength):\n",
    "            if (i>midpoint):\n",
    "                child.gene [0,i]=self.gene[0,i]\n",
    "            else :\n",
    "                child.gene [0,i]=parent2.gene[0,i]\n",
    "        \n",
    "        return child \n",
    "\n",
    "    def mutate(self,mutation_rate):\n",
    "        #randomly activate some of the nodes  \n",
    "        #mutate some of the genes \n",
    "        for i in range (self.maskLength):\n",
    "            if (random.randint (0,99)<=mutation_rate*100):\n",
    "                self.gene[0,i]=1 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "X3LuAn4Oxz6q"
   },
   "outputs": [],
   "source": [
    "mutation_rate =0.20\n",
    "max_population=30\n",
    "maskLength=X_train.shape[1]\n",
    "#seeded so that each time same initial weights generated \n",
    "#torch.manual_seed(6)\n",
    "model=Model(input_size, hidden_size, num_classes)#creating the object of the class\n",
    "model.to(device)\n",
    "criterion=nn.CrossEntropyLoss()\n",
    "#optimizer=torch.optim.SGD(model.parameters(),lr=0.001,momentum=0.9)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "apf1SKKNx9Q5",
    "outputId": "b002f9b0-bcc6-429c-afd8-789fb968c68d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch generations ( 0 /100) :accuracy(fittest mask) tensor(14.7143, dtype=torch.float64) keep_prob 0.42777777777777776\n",
      "Epoch 0/29\n",
      "----------\n",
      "********************train*************\n",
      "Epoch [1/30] Loss: 2.5309   Acc: 14.7143\n",
      "********************val*************\n",
      "Epoch [1/30] Loss: 2.4659   Acc: 5.1111\n",
      "Epoch 1/29\n",
      "----------\n",
      "********************train*************\n",
      "Epoch [2/30] Loss: 2.3832   Acc: 18.3452\n",
      "********************val*************\n",
      "Epoch [2/30] Loss: 2.4030   Acc: 13.6944\n",
      "Epoch 2/29\n",
      "----------\n",
      "********************train*************\n",
      "Epoch [3/30] Loss: 2.2467   Acc: 26.0238\n",
      "********************val*************\n",
      "Epoch [3/30] Loss: 2.3427   Acc: 25.1389\n",
      "Epoch 3/29\n",
      "----------\n",
      "********************train*************\n",
      "Epoch [4/30] Loss: 2.1224   Acc: 33.2619\n",
      "********************val*************\n",
      "Epoch [4/30] Loss: 2.2850   Acc: 30.3056\n",
      "Epoch 4/29\n",
      "----------\n",
      "********************train*************\n",
      "Epoch [5/30] Loss: 2.0104   Acc: 40.3690\n",
      "********************val*************\n",
      "Epoch [5/30] Loss: 2.2299   Acc: 33.8889\n",
      "Epoch 5/29\n",
      "----------\n",
      "********************train*************\n",
      "Epoch [6/30] Loss: 1.9098   Acc: 47.4286\n",
      "********************val*************\n",
      "Epoch [6/30] Loss: 2.1772   Acc: 37.8056\n",
      "Epoch 6/29\n",
      "----------\n",
      "********************train*************\n",
      "Epoch [7/30] Loss: 1.8196   Acc: 52.1190\n",
      "********************val*************\n",
      "Epoch [7/30] Loss: 2.1268   Acc: 41.7500\n",
      "Epoch 7/29\n",
      "----------\n",
      "********************train*************\n",
      "Epoch [8/30] Loss: 1.7385   Acc: 54.8095\n",
      "********************val*************\n",
      "Epoch [8/30] Loss: 2.0785   Acc: 45.0556\n",
      "Epoch 8/29\n",
      "----------\n",
      "********************train*************\n",
      "Epoch [9/30] Loss: 1.6653   Acc: 56.6667\n",
      "********************val*************\n",
      "Epoch [9/30] Loss: 2.0323   Acc: 48.4722\n",
      "Epoch 9/29\n",
      "----------\n",
      "********************train*************\n",
      "Epoch [10/30] Loss: 1.5982   Acc: 58.0000\n",
      "********************val*************\n",
      "Epoch [10/30] Loss: 1.9878   Acc: 51.5278\n",
      "Epoch 10/29\n",
      "----------\n",
      "********************train*************\n",
      "Epoch [11/30] Loss: 1.5358   Acc: 58.9762\n",
      "********************val*************\n",
      "Epoch [11/30] Loss: 1.9450   Acc: 54.3611\n",
      "Epoch 11/29\n",
      "----------\n",
      "********************train*************\n",
      "Epoch [12/30] Loss: 1.4771   Acc: 60.2857\n",
      "********************val*************\n",
      "Epoch [12/30] Loss: 1.9037   Acc: 56.6944\n",
      "Epoch 12/29\n",
      "----------\n",
      "********************train*************\n",
      "Epoch [13/30] Loss: 1.4211   Acc: 61.5357\n",
      "********************val*************\n",
      "Epoch [13/30] Loss: 1.8639   Acc: 59.1944\n",
      "Epoch 13/29\n",
      "----------\n",
      "********************train*************\n",
      "Epoch [14/30] Loss: 1.3675   Acc: 63.0119\n",
      "********************val*************\n",
      "Epoch [14/30] Loss: 1.8257   Acc: 61.5833\n",
      "Epoch 14/29\n",
      "----------\n",
      "********************train*************\n",
      "Epoch [15/30] Loss: 1.3160   Acc: 64.6786\n",
      "********************val*************\n",
      "Epoch [15/30] Loss: 1.7889   Acc: 63.4444\n",
      "Epoch 15/29\n",
      "----------\n",
      "********************train*************\n",
      "Epoch [16/30] Loss: 1.2668   Acc: 66.1071\n",
      "********************val*************\n",
      "Epoch [16/30] Loss: 1.7536   Acc: 65.8333\n",
      "Epoch 16/29\n",
      "----------\n",
      "********************train*************\n",
      "Epoch [17/30] Loss: 1.2198   Acc: 67.3214\n",
      "********************val*************\n",
      "Epoch [17/30] Loss: 1.7198   Acc: 67.7500\n",
      "Epoch 17/29\n",
      "----------\n",
      "********************train*************\n",
      "Epoch [18/30] Loss: 1.1752   Acc: 68.5119\n",
      "********************val*************\n",
      "Epoch [18/30] Loss: 1.6873   Acc: 69.2500\n",
      "Epoch 18/29\n",
      "----------\n",
      "********************train*************\n",
      "Epoch [19/30] Loss: 1.1330   Acc: 70.0119\n",
      "********************val*************\n",
      "Epoch [19/30] Loss: 1.6559   Acc: 70.5556\n",
      "Epoch 19/29\n",
      "----------\n",
      "********************train*************\n",
      "Epoch [20/30] Loss: 1.0931   Acc: 71.0238\n",
      "********************val*************\n",
      "Epoch [20/30] Loss: 1.6255   Acc: 71.7222\n",
      "Epoch 20/29\n",
      "----------\n",
      "********************train*************\n",
      "Epoch [21/30] Loss: 1.0556   Acc: 72.0119\n",
      "********************val*************\n",
      "Epoch [21/30] Loss: 1.5960   Acc: 72.3889\n",
      "Epoch 21/29\n",
      "----------\n",
      "********************train*************\n",
      "Epoch [22/30] Loss: 1.0204   Acc: 72.9286\n",
      "********************val*************\n",
      "Epoch [22/30] Loss: 1.5671   Acc: 73.1111\n",
      "Epoch 22/29\n",
      "----------\n",
      "********************train*************\n",
      "Epoch [23/30] Loss: 0.9873   Acc: 73.9286\n",
      "********************val*************\n",
      "Epoch [23/30] Loss: 1.5388   Acc: 73.6389\n",
      "Epoch 23/29\n",
      "----------\n",
      "********************train*************\n",
      "Epoch [24/30] Loss: 0.9560   Acc: 74.8452\n",
      "********************val*************\n",
      "Epoch [24/30] Loss: 1.5109   Acc: 74.3056\n",
      "Epoch 24/29\n",
      "----------\n",
      "********************train*************\n",
      "Epoch [25/30] Loss: 0.9265   Acc: 75.5357\n",
      "********************val*************\n",
      "Epoch [25/30] Loss: 1.4833   Acc: 74.8333\n",
      "Epoch 25/29\n",
      "----------\n",
      "********************train*************\n",
      "Epoch [26/30] Loss: 0.8985   Acc: 76.2024\n",
      "********************val*************\n",
      "Epoch [26/30] Loss: 1.4561   Acc: 75.7778\n",
      "Epoch 26/29\n",
      "----------\n",
      "********************train*************\n",
      "Epoch [27/30] Loss: 0.8717   Acc: 76.8690\n",
      "********************val*************\n",
      "Epoch [27/30] Loss: 1.4294   Acc: 76.4167\n",
      "Epoch 27/29\n",
      "----------\n",
      "********************train*************\n",
      "Epoch [28/30] Loss: 0.8463   Acc: 77.3214\n",
      "********************val*************\n",
      "Epoch [28/30] Loss: 1.4031   Acc: 77.2222\n",
      "Epoch 28/29\n",
      "----------\n",
      "********************train*************\n",
      "Epoch [29/30] Loss: 0.8219   Acc: 77.8214\n",
      "********************val*************\n",
      "Epoch [29/30] Loss: 1.3775   Acc: 77.5833\n",
      "Epoch 29/29\n",
      "----------\n",
      "********************train*************\n",
      "Epoch [30/30] Loss: 0.7987   Acc: 78.1786\n",
      "********************val*************\n",
      "Epoch [30/30] Loss: 1.3525   Acc: 78.2500\n",
      "Training complete in 0m 2s\n",
      "Best val Acc: 78.250000\n",
      "Epoch generations ( 1 /100) :accuracy(fittest mask) tensor(77.9048, dtype=torch.float64) keep_prob 0.5833333333333334\n",
      "Epoch generations ( 2 /100) :accuracy(fittest mask) tensor(79.4048, dtype=torch.float64) keep_prob 0.6388888888888888\n",
      "Epoch generations ( 3 /100) :accuracy(fittest mask) tensor(79.2500, dtype=torch.float64) keep_prob 0.7388888888888889\n",
      "Epoch generations ( 4 /100) :accuracy(fittest mask) tensor(79.4643, dtype=torch.float64) keep_prob 0.8055555555555556\n",
      "Epoch generations ( 5 /100) :Epoch generations ( 6 /100) :Epoch generations ( 7 /100) :Epoch generations ( 8 /100) :Epoch generations ( 9 /100) :Epoch generations ( 10 /100) :Epoch 0/29\n",
      "----------\n",
      "********************train*************\n",
      "Epoch [1/30] Loss: 1.1981   Acc: 79.4643\n",
      "********************val*************\n",
      "Epoch [1/30] Loss: 1.3093   Acc: 79.3056\n",
      "Epoch 1/29\n",
      "----------\n",
      "********************train*************\n",
      "Epoch [2/30] Loss: 1.1539   Acc: 79.6429\n",
      "********************val*************\n",
      "Epoch [2/30] Loss: 1.2612   Acc: 79.6389\n",
      "Epoch 2/29\n",
      "----------\n",
      "********************train*************\n",
      "Epoch [3/30] Loss: 1.1058   Acc: 79.5119\n",
      "********************val*************\n",
      "Epoch [3/30] Loss: 1.2121   Acc: 79.1944\n",
      "Epoch 3/29\n",
      "----------\n",
      "********************train*************\n",
      "Epoch [4/30] Loss: 1.0581   Acc: 79.6786\n",
      "********************val*************\n",
      "Epoch [4/30] Loss: 1.1643   Acc: 78.7500\n",
      "Epoch 4/29\n",
      "----------\n",
      "********************train*************\n",
      "Epoch [5/30] Loss: 1.0133   Acc: 79.3571\n",
      "********************val*************\n",
      "Epoch [5/30] Loss: 1.1192   Acc: 79.0000\n",
      "Epoch 5/29\n",
      "----------\n",
      "********************train*************\n",
      "Epoch [6/30] Loss: 0.9725   Acc: 79.2262\n",
      "********************val*************\n",
      "Epoch [6/30] Loss: 1.0772   Acc: 78.8611\n",
      "Epoch 6/29\n",
      "----------\n",
      "********************train*************\n",
      "Epoch [7/30] Loss: 0.9353   Acc: 79.0476\n",
      "********************val*************\n",
      "Epoch [7/30] Loss: 1.0382   Acc: 78.9444\n",
      "Epoch 7/29\n",
      "----------\n",
      "********************train*************\n",
      "Epoch [8/30] Loss: 0.9012   Acc: 79.1667\n",
      "********************val*************\n",
      "Epoch [8/30] Loss: 1.0019   Acc: 79.1111\n",
      "Epoch 8/29\n",
      "----------\n",
      "********************train*************\n",
      "Epoch [9/30] Loss: 0.8693   Acc: 79.2619\n",
      "********************val*************\n",
      "Epoch [9/30] Loss: 0.9679   Acc: 79.4444\n",
      "Epoch 9/29\n",
      "----------\n",
      "********************train*************\n",
      "Epoch [10/30] Loss: 0.8390   Acc: 79.6905\n",
      "********************val*************\n",
      "Epoch [10/30] Loss: 0.9360   Acc: 79.8889\n",
      "Epoch 10/29\n",
      "----------\n",
      "********************train*************\n",
      "Epoch [11/30] Loss: 0.8098   Acc: 80.2262\n",
      "********************val*************\n",
      "Epoch [11/30] Loss: 0.9060   Acc: 80.4167\n",
      "Epoch 11/29\n",
      "----------\n",
      "********************train*************\n",
      "Epoch [12/30] Loss: 0.7817   Acc: 80.6905\n",
      "********************val*************\n",
      "Epoch [12/30] Loss: 0.8777   Acc: 80.7222\n",
      "Epoch 12/29\n",
      "----------\n",
      "********************train*************\n",
      "Epoch [13/30] Loss: 0.7548   Acc: 80.9643\n",
      "********************val*************\n",
      "Epoch [13/30] Loss: 0.8512   Acc: 81.3333\n",
      "Epoch 13/29\n",
      "----------\n",
      "********************train*************\n",
      "Epoch [14/30] Loss: 0.7292   Acc: 81.6429\n",
      "********************val*************\n",
      "Epoch [14/30] Loss: 0.8262   Acc: 82.1389\n",
      "Epoch 14/29\n",
      "----------\n",
      "********************train*************\n",
      "Epoch [15/30] Loss: 0.7050   Acc: 82.4643\n",
      "********************val*************\n",
      "Epoch [15/30] Loss: 0.8027   Acc: 83.1111\n",
      "Epoch 15/29\n",
      "----------\n",
      "********************train*************\n",
      "Epoch [16/30] Loss: 0.6822   Acc: 83.0000\n",
      "********************val*************\n",
      "Epoch [16/30] Loss: 0.7803   Acc: 83.5556\n",
      "Epoch 16/29\n",
      "----------\n",
      "********************train*************\n",
      "Epoch [17/30] Loss: 0.6608   Acc: 83.7024\n",
      "********************val*************\n",
      "Epoch [17/30] Loss: 0.7587   Acc: 84.0833\n",
      "Epoch 17/29\n",
      "----------\n",
      "********************train*************\n",
      "Epoch [18/30] Loss: 0.6406   Acc: 84.0357\n",
      "********************val*************\n",
      "Epoch [18/30] Loss: 0.7378   Acc: 84.5000\n",
      "Epoch 18/29\n",
      "----------\n",
      "********************train*************\n",
      "Epoch [19/30] Loss: 0.6213   Acc: 84.4048\n",
      "********************val*************\n",
      "Epoch [19/30] Loss: 0.7173   Acc: 84.8056\n",
      "Epoch 19/29\n",
      "----------\n",
      "********************train*************\n",
      "Epoch [20/30] Loss: 0.6028   Acc: 84.8810\n",
      "********************val*************\n",
      "Epoch [20/30] Loss: 0.6972   Acc: 85.3056\n",
      "Epoch 20/29\n",
      "----------\n",
      "********************train*************\n",
      "Epoch [21/30] Loss: 0.5849   Acc: 85.1310\n",
      "********************val*************\n",
      "Epoch [21/30] Loss: 0.6777   Acc: 85.3333\n",
      "Epoch 21/29\n",
      "----------\n",
      "********************train*************\n",
      "Epoch [22/30] Loss: 0.5677   Acc: 85.6667\n",
      "********************val*************\n",
      "Epoch [22/30] Loss: 0.6588   Acc: 85.4444\n",
      "Epoch 22/29\n",
      "----------\n",
      "********************train*************\n",
      "Epoch [23/30] Loss: 0.5513   Acc: 85.8929\n",
      "********************val*************\n",
      "Epoch [23/30] Loss: 0.6407   Acc: 85.7500\n",
      "Epoch 23/29\n",
      "----------\n",
      "********************train*************\n",
      "Epoch [24/30] Loss: 0.5357   Acc: 86.0238\n",
      "********************val*************\n",
      "Epoch [24/30] Loss: 0.6235   Acc: 85.9722\n",
      "Epoch 24/29\n",
      "----------\n",
      "********************train*************\n",
      "Epoch [25/30] Loss: 0.5208   Acc: 86.2857\n",
      "********************val*************\n",
      "Epoch [25/30] Loss: 0.6071   Acc: 86.1667\n",
      "Epoch 25/29\n",
      "----------\n",
      "********************train*************\n",
      "Epoch [26/30] Loss: 0.5065   Acc: 86.5119\n",
      "********************val*************\n",
      "Epoch [26/30] Loss: 0.5916   Acc: 86.4722\n",
      "Epoch 26/29\n",
      "----------\n",
      "********************train*************\n",
      "Epoch [27/30] Loss: 0.4929   Acc: 86.9167\n",
      "********************val*************\n",
      "Epoch [27/30] Loss: 0.5769   Acc: 86.8056\n",
      "Epoch 27/29\n",
      "----------\n",
      "********************train*************\n",
      "Epoch [28/30] Loss: 0.4797   Acc: 87.2381\n",
      "********************val*************\n",
      "Epoch [28/30] Loss: 0.5631   Acc: 87.2222\n",
      "Epoch 28/29\n",
      "----------\n",
      "********************train*************\n",
      "Epoch [29/30] Loss: 0.4671   Acc: 87.4167\n",
      "********************val*************\n",
      "Epoch [29/30] Loss: 0.5499   Acc: 87.6111\n",
      "Epoch 29/29\n",
      "----------\n",
      "********************train*************\n",
      "Epoch [30/30] Loss: 0.4549   Acc: 87.7738\n",
      "********************val*************\n",
      "Epoch [30/30] Loss: 0.5374   Acc: 87.8611\n",
      "Training complete in 0m 2s\n",
      "Best val Acc: 87.861111\n",
      "Epoch generations ( 11 /100) :Epoch generations ( 12 /100) :Epoch generations ( 13 /100) :Epoch generations ( 14 /100) :Epoch generations ( 15 /100) :Epoch generations ( 16 /100) :Epoch generations ( 17 /100) :Epoch generations ( 18 /100) :Epoch generations ( 19 /100) :Epoch generations ( 20 /100) :Epoch 0/29\n",
      "----------\n",
      "********************train*************\n",
      "Epoch [1/30] Loss: 0.4433   Acc: 88.2976\n",
      "********************val*************\n",
      "Epoch [1/30] Loss: 0.5255   Acc: 88.1111\n",
      "Epoch 1/29\n",
      "----------\n",
      "********************train*************\n",
      "Epoch [2/30] Loss: 0.4322   Acc: 88.7500\n",
      "********************val*************\n",
      "Epoch [2/30] Loss: 0.5142   Acc: 88.3056\n",
      "Epoch 2/29\n",
      "----------\n",
      "********************train*************\n",
      "Epoch [3/30] Loss: 0.4217   Acc: 88.9524\n",
      "********************val*************\n",
      "Epoch [3/30] Loss: 0.5032   Acc: 88.4167\n",
      "Epoch 3/29\n",
      "----------\n",
      "********************train*************\n",
      "Epoch [4/30] Loss: 0.4116   Acc: 89.2143\n",
      "********************val*************\n",
      "Epoch [4/30] Loss: 0.4927   Acc: 88.6944\n",
      "Epoch 4/29\n",
      "----------\n",
      "********************train*************\n",
      "Epoch [5/30] Loss: 0.4019   Acc: 89.5119\n",
      "********************val*************\n",
      "Epoch [5/30] Loss: 0.4824   Acc: 89.1389\n",
      "Epoch 5/29\n",
      "----------\n",
      "********************train*************\n",
      "Epoch [6/30] Loss: 0.3927   Acc: 89.8095\n",
      "********************val*************\n",
      "Epoch [6/30] Loss: 0.4725   Acc: 89.4722\n",
      "Epoch 6/29\n",
      "----------\n",
      "********************train*************\n",
      "Epoch [7/30] Loss: 0.3838   Acc: 90.0000\n",
      "********************val*************\n",
      "Epoch [7/30] Loss: 0.4630   Acc: 89.6389\n",
      "Epoch 7/29\n",
      "----------\n",
      "********************train*************\n",
      "Epoch [8/30] Loss: 0.3754   Acc: 90.1310\n",
      "********************val*************\n",
      "Epoch [8/30] Loss: 0.4538   Acc: 89.8056\n",
      "Epoch 8/29\n",
      "----------\n",
      "********************train*************\n",
      "Epoch [9/30] Loss: 0.3673   Acc: 90.2857\n",
      "********************val*************\n",
      "Epoch [9/30] Loss: 0.4449   Acc: 90.0278\n",
      "Epoch 9/29\n",
      "----------\n",
      "********************train*************\n",
      "Epoch [10/30] Loss: 0.3595   Acc: 90.5000\n",
      "********************val*************\n",
      "Epoch [10/30] Loss: 0.4364   Acc: 90.2778\n",
      "Epoch 10/29\n",
      "----------\n",
      "********************train*************\n",
      "Epoch [11/30] Loss: 0.3521   Acc: 90.7381\n",
      "********************val*************\n",
      "Epoch [11/30] Loss: 0.4283   Acc: 90.3611\n",
      "Epoch 11/29\n",
      "----------\n",
      "********************train*************\n",
      "Epoch [12/30] Loss: 0.3449   Acc: 90.7143\n",
      "********************val*************\n",
      "Epoch [12/30] Loss: 0.4205   Acc: 90.4167\n",
      "Epoch 12/29\n",
      "----------\n",
      "********************train*************\n",
      "Epoch [13/30] Loss: 0.3379   Acc: 90.9524\n",
      "********************val*************\n",
      "Epoch [13/30] Loss: 0.4130   Acc: 90.5556\n",
      "Epoch 13/29\n",
      "----------\n",
      "********************train*************\n",
      "Epoch [14/30] Loss: 0.3313   Acc: 91.0476\n",
      "********************val*************\n",
      "Epoch [14/30] Loss: 0.4058   Acc: 90.7500\n",
      "Epoch 14/29\n",
      "----------\n",
      "********************train*************\n",
      "Epoch [15/30] Loss: 0.3249   Acc: 91.3095\n",
      "********************val*************\n",
      "Epoch [15/30] Loss: 0.3988   Acc: 91.0556\n",
      "Epoch 15/29\n",
      "----------\n",
      "********************train*************\n",
      "Epoch [16/30] Loss: 0.3187   Acc: 91.5357\n",
      "********************val*************\n",
      "Epoch [16/30] Loss: 0.3921   Acc: 91.1111\n",
      "Epoch 16/29\n",
      "----------\n",
      "********************train*************\n",
      "Epoch [17/30] Loss: 0.3127   Acc: 91.6548\n",
      "********************val*************\n",
      "Epoch [17/30] Loss: 0.3856   Acc: 91.3889\n",
      "Epoch 17/29\n",
      "----------\n",
      "********************train*************\n",
      "Epoch [18/30] Loss: 0.3069   Acc: 91.8571\n",
      "********************val*************\n",
      "Epoch [18/30] Loss: 0.3793   Acc: 91.5833\n",
      "Epoch 18/29\n",
      "----------\n",
      "********************train*************\n",
      "Epoch [19/30] Loss: 0.3013   Acc: 91.9881\n",
      "********************val*************\n",
      "Epoch [19/30] Loss: 0.3731   Acc: 91.6667\n",
      "Epoch 19/29\n",
      "----------\n",
      "********************train*************\n",
      "Epoch [20/30] Loss: 0.2960   Acc: 92.0714\n",
      "********************val*************\n",
      "Epoch [20/30] Loss: 0.3672   Acc: 91.9167\n",
      "Epoch 20/29\n",
      "----------\n",
      "********************train*************\n",
      "Epoch [21/30] Loss: 0.2908   Acc: 92.2500\n",
      "********************val*************\n",
      "Epoch [21/30] Loss: 0.3614   Acc: 91.9722\n",
      "Epoch 21/29\n",
      "----------\n",
      "********************train*************\n",
      "Epoch [22/30] Loss: 0.2857   Acc: 92.3214\n",
      "********************val*************\n",
      "Epoch [22/30] Loss: 0.3559   Acc: 92.1944\n",
      "Epoch 22/29\n",
      "----------\n",
      "********************train*************\n",
      "Epoch [23/30] Loss: 0.2809   Acc: 92.4167\n",
      "********************val*************\n",
      "Epoch [23/30] Loss: 0.3505   Acc: 92.3056\n",
      "Epoch 23/29\n",
      "----------\n",
      "********************train*************\n",
      "Epoch [24/30] Loss: 0.2762   Acc: 92.4881\n",
      "********************val*************\n",
      "Epoch [24/30] Loss: 0.3452   Acc: 92.3056\n",
      "Epoch 24/29\n",
      "----------\n",
      "********************train*************\n",
      "Epoch [25/30] Loss: 0.2716   Acc: 92.7500\n",
      "********************val*************\n",
      "Epoch [25/30] Loss: 0.3402   Acc: 92.4722\n",
      "Epoch 25/29\n",
      "----------\n",
      "********************train*************\n",
      "Epoch [26/30] Loss: 0.2672   Acc: 92.8452\n",
      "********************val*************\n",
      "Epoch [26/30] Loss: 0.3352   Acc: 92.5000\n",
      "Epoch 26/29\n",
      "----------\n",
      "********************train*************\n",
      "Epoch [27/30] Loss: 0.2629   Acc: 92.8571\n",
      "********************val*************\n",
      "Epoch [27/30] Loss: 0.3305   Acc: 92.6389\n",
      "Epoch 27/29\n",
      "----------\n",
      "********************train*************\n",
      "Epoch [28/30] Loss: 0.2587   Acc: 93.0119\n",
      "********************val*************\n",
      "Epoch [28/30] Loss: 0.3258   Acc: 92.8333\n",
      "Epoch 28/29\n",
      "----------\n",
      "********************train*************\n",
      "Epoch [29/30] Loss: 0.2546   Acc: 93.1548\n",
      "********************val*************\n",
      "Epoch [29/30] Loss: 0.3213   Acc: 92.8611\n",
      "Epoch 29/29\n",
      "----------\n",
      "********************train*************\n",
      "Epoch [30/30] Loss: 0.2507   Acc: 93.2738\n",
      "********************val*************\n",
      "Epoch [30/30] Loss: 0.3169   Acc: 92.9167\n",
      "Training complete in 0m 2s\n",
      "Best val Acc: 92.916667\n",
      "Epoch generations ( 21 /100) :Epoch generations ( 22 /100) :Epoch generations ( 23 /100) :Epoch generations ( 24 /100) :Epoch generations ( 25 /100) :Epoch generations ( 26 /100) :Epoch generations ( 27 /100) :Epoch generations ( 28 /100) :Epoch generations ( 29 /100) :Epoch generations ( 30 /100) :Epoch 0/29\n",
      "----------\n",
      "********************train*************\n",
      "Epoch [1/30] Loss: 0.2469   Acc: 93.3214\n",
      "********************val*************\n",
      "Epoch [1/30] Loss: 0.3126   Acc: 92.9444\n",
      "Epoch 1/29\n",
      "----------\n",
      "********************train*************\n",
      "Epoch [2/30] Loss: 0.2432   Acc: 93.4762\n",
      "********************val*************\n",
      "Epoch [2/30] Loss: 0.3085   Acc: 93.1389\n",
      "Epoch 2/29\n",
      "----------\n",
      "********************train*************\n",
      "Epoch [3/30] Loss: 0.2396   Acc: 93.6429\n",
      "********************val*************\n",
      "Epoch [3/30] Loss: 0.3045   Acc: 93.1667\n",
      "Epoch 3/29\n",
      "----------\n",
      "********************train*************\n",
      "Epoch [4/30] Loss: 0.2360   Acc: 93.7738\n",
      "********************val*************\n",
      "Epoch [4/30] Loss: 0.3007   Acc: 93.2778\n",
      "Epoch 4/29\n",
      "----------\n",
      "********************train*************\n",
      "Epoch [5/30] Loss: 0.2326   Acc: 93.7857\n",
      "********************val*************\n",
      "Epoch [5/30] Loss: 0.2970   Acc: 93.3611\n",
      "Epoch 5/29\n",
      "----------\n",
      "********************train*************\n",
      "Epoch [6/30] Loss: 0.2293   Acc: 93.9524\n",
      "********************val*************\n",
      "Epoch [6/30] Loss: 0.2933   Acc: 93.4167\n",
      "Epoch 6/29\n",
      "----------\n",
      "********************train*************\n",
      "Epoch [7/30] Loss: 0.2260   Acc: 94.0000\n",
      "********************val*************\n",
      "Epoch [7/30] Loss: 0.2898   Acc: 93.5000\n",
      "Epoch 7/29\n",
      "----------\n",
      "********************train*************\n",
      "Epoch [8/30] Loss: 0.2228   Acc: 94.0952\n",
      "********************val*************\n",
      "Epoch [8/30] Loss: 0.2863   Acc: 93.5000\n",
      "Epoch 8/29\n",
      "----------\n",
      "********************train*************\n",
      "Epoch [9/30] Loss: 0.2197   Acc: 94.2024\n",
      "********************val*************\n",
      "Epoch [9/30] Loss: 0.2829   Acc: 93.5000\n",
      "Epoch 9/29\n",
      "----------\n",
      "********************train*************\n",
      "Epoch [10/30] Loss: 0.2167   Acc: 94.2738\n",
      "********************val*************\n",
      "Epoch [10/30] Loss: 0.2795   Acc: 93.6389\n",
      "Epoch 10/29\n",
      "----------\n",
      "********************train*************\n",
      "Epoch [11/30] Loss: 0.2137   Acc: 94.3690\n",
      "********************val*************\n",
      "Epoch [11/30] Loss: 0.2762   Acc: 93.7778\n",
      "Epoch 11/29\n",
      "----------\n",
      "********************train*************\n",
      "Epoch [12/30] Loss: 0.2108   Acc: 94.4762\n",
      "********************val*************\n",
      "Epoch [12/30] Loss: 0.2730   Acc: 93.8056\n",
      "Epoch 12/29\n",
      "----------\n",
      "********************train*************\n",
      "Epoch [13/30] Loss: 0.2080   Acc: 94.5595\n",
      "********************val*************\n",
      "Epoch [13/30] Loss: 0.2698   Acc: 93.8611\n",
      "Epoch 13/29\n",
      "----------\n",
      "********************train*************\n",
      "Epoch [14/30] Loss: 0.2052   Acc: 94.5833\n",
      "********************val*************\n",
      "Epoch [14/30] Loss: 0.2668   Acc: 93.9444\n",
      "Epoch 14/29\n",
      "----------\n",
      "********************train*************\n",
      "Epoch [15/30] Loss: 0.2025   Acc: 94.6786\n",
      "********************val*************\n",
      "Epoch [15/30] Loss: 0.2638   Acc: 93.9167\n",
      "Epoch 15/29\n",
      "----------\n",
      "********************train*************\n",
      "Epoch [16/30] Loss: 0.1998   Acc: 94.8095\n",
      "********************val*************\n",
      "Epoch [16/30] Loss: 0.2610   Acc: 93.9444\n",
      "Epoch 16/29\n",
      "----------\n",
      "********************train*************\n",
      "Epoch [17/30] Loss: 0.1972   Acc: 94.9167\n",
      "********************val*************\n",
      "Epoch [17/30] Loss: 0.2582   Acc: 94.0278\n",
      "Epoch 17/29\n",
      "----------\n",
      "********************train*************\n",
      "Epoch [18/30] Loss: 0.1947   Acc: 94.9405\n",
      "********************val*************\n",
      "Epoch [18/30] Loss: 0.2554   Acc: 94.1111\n",
      "Epoch 18/29\n",
      "----------\n",
      "********************train*************\n",
      "Epoch [19/30] Loss: 0.1922   Acc: 95.0476\n",
      "********************val*************\n",
      "Epoch [19/30] Loss: 0.2528   Acc: 94.1389\n",
      "Epoch 19/29\n",
      "----------\n",
      "********************train*************\n",
      "Epoch [20/30] Loss: 0.1897   Acc: 95.0595\n",
      "********************val*************\n",
      "Epoch [20/30] Loss: 0.2501   Acc: 94.1667\n",
      "Epoch 20/29\n",
      "----------\n",
      "********************train*************\n",
      "Epoch [21/30] Loss: 0.1873   Acc: 95.1071\n",
      "********************val*************\n",
      "Epoch [21/30] Loss: 0.2475   Acc: 94.1667\n",
      "Epoch 21/29\n",
      "----------\n",
      "********************train*************\n",
      "Epoch [22/30] Loss: 0.1850   Acc: 95.1786\n",
      "********************val*************\n",
      "Epoch [22/30] Loss: 0.2450   Acc: 94.1667\n",
      "Epoch 22/29\n",
      "----------\n",
      "********************train*************\n",
      "Epoch [23/30] Loss: 0.1827   Acc: 95.2024\n",
      "********************val*************\n",
      "Epoch [23/30] Loss: 0.2425   Acc: 94.2222\n",
      "Epoch 23/29\n",
      "----------\n",
      "********************train*************\n",
      "Epoch [24/30] Loss: 0.1804   Acc: 95.2024\n",
      "********************val*************\n",
      "Epoch [24/30] Loss: 0.2400   Acc: 94.2222\n",
      "Epoch 24/29\n",
      "----------\n",
      "********************train*************\n",
      "Epoch [25/30] Loss: 0.1782   Acc: 95.3214\n",
      "********************val*************\n",
      "Epoch [25/30] Loss: 0.2376   Acc: 94.2500\n",
      "Epoch 25/29\n",
      "----------\n",
      "********************train*************\n",
      "Epoch [26/30] Loss: 0.1760   Acc: 95.4167\n",
      "********************val*************\n",
      "Epoch [26/30] Loss: 0.2353   Acc: 94.3333\n",
      "Epoch 26/29\n",
      "----------\n",
      "********************train*************\n",
      "Epoch [27/30] Loss: 0.1739   Acc: 95.5000\n",
      "********************val*************\n",
      "Epoch [27/30] Loss: 0.2330   Acc: 94.3056\n",
      "Epoch 27/29\n",
      "----------\n",
      "********************train*************\n",
      "Epoch [28/30] Loss: 0.1718   Acc: 95.5476\n",
      "********************val*************\n",
      "Epoch [28/30] Loss: 0.2308   Acc: 94.3333\n",
      "Epoch 28/29\n",
      "----------\n",
      "********************train*************\n",
      "Epoch [29/30] Loss: 0.1698   Acc: 95.6071\n",
      "********************val*************\n",
      "Epoch [29/30] Loss: 0.2286   Acc: 94.3611\n",
      "Epoch 29/29\n",
      "----------\n",
      "********************train*************\n",
      "Epoch [30/30] Loss: 0.1677   Acc: 95.6667\n",
      "********************val*************\n",
      "Epoch [30/30] Loss: 0.2265   Acc: 94.3889\n",
      "Training complete in 0m 2s\n",
      "Best val Acc: 94.388889\n",
      "Epoch generations ( 31 /100) :Epoch generations ( 32 /100) :Epoch generations ( 33 /100) :Epoch generations ( 34 /100) :Epoch generations ( 35 /100) :Epoch generations ( 36 /100) :Epoch generations ( 37 /100) :Epoch generations ( 38 /100) :Epoch generations ( 39 /100) :Epoch generations ( 40 /100) :Epoch 0/29\n",
      "----------\n",
      "********************train*************\n",
      "Epoch [1/30] Loss: 0.1657   Acc: 95.6667\n",
      "********************val*************\n",
      "Epoch [1/30] Loss: 0.2243   Acc: 94.5278\n",
      "Epoch 1/29\n",
      "----------\n",
      "********************train*************\n",
      "Epoch [2/30] Loss: 0.1638   Acc: 95.7500\n",
      "********************val*************\n",
      "Epoch [2/30] Loss: 0.2222   Acc: 94.5833\n",
      "Epoch 2/29\n",
      "----------\n",
      "********************train*************\n",
      "Epoch [3/30] Loss: 0.1619   Acc: 95.7976\n",
      "********************val*************\n",
      "Epoch [3/30] Loss: 0.2202   Acc: 94.5833\n",
      "Epoch 3/29\n",
      "----------\n",
      "********************train*************\n",
      "Epoch [4/30] Loss: 0.1600   Acc: 95.8690\n",
      "********************val*************\n",
      "Epoch [4/30] Loss: 0.2182   Acc: 94.6389\n",
      "Epoch 4/29\n",
      "----------\n",
      "********************train*************\n",
      "Epoch [5/30] Loss: 0.1581   Acc: 95.9405\n",
      "********************val*************\n",
      "Epoch [5/30] Loss: 0.2162   Acc: 94.6389\n",
      "Epoch 5/29\n",
      "----------\n",
      "********************train*************\n",
      "Epoch [6/30] Loss: 0.1563   Acc: 96.0238\n",
      "********************val*************\n",
      "Epoch [6/30] Loss: 0.2142   Acc: 94.6389\n",
      "Epoch 6/29\n",
      "----------\n",
      "********************train*************\n",
      "Epoch [7/30] Loss: 0.1545   Acc: 96.0476\n",
      "********************val*************\n",
      "Epoch [7/30] Loss: 0.2123   Acc: 94.6667\n",
      "Epoch 7/29\n",
      "----------\n",
      "********************train*************\n",
      "Epoch [8/30] Loss: 0.1527   Acc: 96.0833\n",
      "********************val*************\n",
      "Epoch [8/30] Loss: 0.2104   Acc: 94.6667\n",
      "Epoch 8/29\n",
      "----------\n",
      "********************train*************\n",
      "Epoch [9/30] Loss: 0.1510   Acc: 96.1071\n",
      "********************val*************\n",
      "Epoch [9/30] Loss: 0.2085   Acc: 94.7222\n",
      "Epoch 9/29\n",
      "----------\n",
      "********************train*************\n",
      "Epoch [10/30] Loss: 0.1493   Acc: 96.1429\n",
      "********************val*************\n",
      "Epoch [10/30] Loss: 0.2066   Acc: 94.8056\n",
      "Epoch 10/29\n",
      "----------\n",
      "********************train*************\n",
      "Epoch [11/30] Loss: 0.1476   Acc: 96.2024\n",
      "********************val*************\n",
      "Epoch [11/30] Loss: 0.2048   Acc: 94.8889\n",
      "Epoch 11/29\n",
      "----------\n",
      "********************train*************\n",
      "Epoch [12/30] Loss: 0.1459   Acc: 96.2262\n",
      "********************val*************\n",
      "Epoch [12/30] Loss: 0.2030   Acc: 94.9444\n",
      "Epoch 12/29\n",
      "----------\n",
      "********************train*************\n",
      "Epoch [13/30] Loss: 0.1443   Acc: 96.2619\n",
      "********************val*************\n",
      "Epoch [13/30] Loss: 0.2012   Acc: 95.0000\n",
      "Epoch 13/29\n",
      "----------\n",
      "********************train*************\n",
      "Epoch [14/30] Loss: 0.1427   Acc: 96.3095\n",
      "********************val*************\n",
      "Epoch [14/30] Loss: 0.1995   Acc: 95.0278\n",
      "Epoch 14/29\n",
      "----------\n",
      "********************train*************\n",
      "Epoch [15/30] Loss: 0.1411   Acc: 96.3452\n",
      "********************val*************\n",
      "Epoch [15/30] Loss: 0.1978   Acc: 95.0556\n",
      "Epoch 15/29\n",
      "----------\n",
      "********************train*************\n",
      "Epoch [16/30] Loss: 0.1396   Acc: 96.4048\n",
      "********************val*************\n",
      "Epoch [16/30] Loss: 0.1962   Acc: 95.0556\n",
      "Epoch 16/29\n",
      "----------\n",
      "********************train*************\n",
      "Epoch [17/30] Loss: 0.1381   Acc: 96.4286\n",
      "********************val*************\n",
      "Epoch [17/30] Loss: 0.1946   Acc: 95.1111\n",
      "Epoch 17/29\n",
      "----------\n",
      "********************train*************\n",
      "Epoch [18/30] Loss: 0.1366   Acc: 96.4524\n",
      "********************val*************\n",
      "Epoch [18/30] Loss: 0.1930   Acc: 95.2500\n",
      "Epoch 18/29\n",
      "----------\n",
      "********************train*************\n",
      "Epoch [19/30] Loss: 0.1351   Acc: 96.5000\n",
      "********************val*************\n",
      "Epoch [19/30] Loss: 0.1914   Acc: 95.3056\n",
      "Epoch 19/29\n",
      "----------\n",
      "********************train*************\n",
      "Epoch [20/30] Loss: 0.1337   Acc: 96.5952\n",
      "********************val*************\n",
      "Epoch [20/30] Loss: 0.1898   Acc: 95.3056\n",
      "Epoch 20/29\n",
      "----------\n",
      "********************train*************\n",
      "Epoch [21/30] Loss: 0.1322   Acc: 96.6071\n",
      "********************val*************\n",
      "Epoch [21/30] Loss: 0.1883   Acc: 95.3333\n",
      "Epoch 21/29\n",
      "----------\n",
      "********************train*************\n",
      "Epoch [22/30] Loss: 0.1308   Acc: 96.6548\n",
      "********************val*************\n",
      "Epoch [22/30] Loss: 0.1867   Acc: 95.3333\n",
      "Epoch 22/29\n",
      "----------\n",
      "********************train*************\n",
      "Epoch [23/30] Loss: 0.1295   Acc: 96.7024\n",
      "********************val*************\n",
      "Epoch [23/30] Loss: 0.1852   Acc: 95.3333\n",
      "Epoch 23/29\n",
      "----------\n",
      "********************train*************\n",
      "Epoch [24/30] Loss: 0.1281   Acc: 96.7024\n",
      "********************val*************\n",
      "Epoch [24/30] Loss: 0.1838   Acc: 95.3333\n",
      "Epoch 24/29\n",
      "----------\n",
      "********************train*************\n",
      "Epoch [25/30] Loss: 0.1268   Acc: 96.7738\n",
      "********************val*************\n",
      "Epoch [25/30] Loss: 0.1823   Acc: 95.4167\n",
      "Epoch 25/29\n",
      "----------\n",
      "********************train*************\n",
      "Epoch [26/30] Loss: 0.1254   Acc: 96.7976\n",
      "********************val*************\n",
      "Epoch [26/30] Loss: 0.1809   Acc: 95.4167\n",
      "Epoch 26/29\n",
      "----------\n",
      "********************train*************\n",
      "Epoch [27/30] Loss: 0.1241   Acc: 96.8095\n",
      "********************val*************\n",
      "Epoch [27/30] Loss: 0.1795   Acc: 95.4444\n",
      "Epoch 27/29\n",
      "----------\n",
      "********************train*************\n",
      "Epoch [28/30] Loss: 0.1228   Acc: 96.8571\n",
      "********************val*************\n",
      "Epoch [28/30] Loss: 0.1781   Acc: 95.5000\n",
      "Epoch 28/29\n",
      "----------\n",
      "********************train*************\n",
      "Epoch [29/30] Loss: 0.1216   Acc: 96.9286\n",
      "********************val*************\n",
      "Epoch [29/30] Loss: 0.1767   Acc: 95.5000\n",
      "Epoch 29/29\n",
      "----------\n",
      "********************train*************\n",
      "Epoch [30/30] Loss: 0.1203   Acc: 97.0119\n",
      "********************val*************\n",
      "Epoch [30/30] Loss: 0.1754   Acc: 95.5000\n",
      "Training complete in 0m 2s\n",
      "Best val Acc: 95.500000\n",
      "Epoch generations ( 41 /100) :Epoch generations ( 42 /100) :Epoch generations ( 43 /100) :Epoch generations ( 44 /100) :Epoch generations ( 45 /100) :Epoch generations ( 46 /100) :Epoch generations ( 47 /100) :Epoch generations ( 48 /100) :Epoch generations ( 49 /100) :Epoch generations ( 50 /100) :Epoch 0/29\n",
      "----------\n",
      "********************train*************\n",
      "Epoch [1/30] Loss: 0.1216   Acc: 96.9286\n",
      "********************val*************\n",
      "Epoch [1/30] Loss: 0.1767   Acc: 95.5000\n",
      "Epoch 1/29\n",
      "----------\n",
      "********************train*************\n",
      "Epoch [2/30] Loss: 0.1203   Acc: 97.0000\n",
      "********************val*************\n",
      "Epoch [2/30] Loss: 0.1754   Acc: 95.5000\n",
      "Epoch 2/29\n",
      "----------\n",
      "********************train*************\n",
      "Epoch [3/30] Loss: 0.1191   Acc: 97.0357\n",
      "********************val*************\n",
      "Epoch [3/30] Loss: 0.1741   Acc: 95.5000\n",
      "Epoch 3/29\n",
      "----------\n",
      "********************train*************\n",
      "Epoch [4/30] Loss: 0.1179   Acc: 97.0833\n",
      "********************val*************\n",
      "Epoch [4/30] Loss: 0.1728   Acc: 95.5278\n",
      "Epoch 4/29\n",
      "----------\n",
      "********************train*************\n",
      "Epoch [5/30] Loss: 0.1167   Acc: 97.1071\n",
      "********************val*************\n",
      "Epoch [5/30] Loss: 0.1715   Acc: 95.5556\n",
      "Epoch 5/29\n",
      "----------\n",
      "********************train*************\n",
      "Epoch [6/30] Loss: 0.1156   Acc: 97.1310\n",
      "********************val*************\n",
      "Epoch [6/30] Loss: 0.1702   Acc: 95.6111\n",
      "Epoch 6/29\n",
      "----------\n",
      "********************train*************\n",
      "Epoch [7/30] Loss: 0.1144   Acc: 97.1310\n",
      "********************val*************\n",
      "Epoch [7/30] Loss: 0.1690   Acc: 95.6111\n",
      "Epoch 7/29\n",
      "----------\n",
      "********************train*************\n",
      "Epoch [8/30] Loss: 0.1133   Acc: 97.1905\n",
      "********************val*************\n",
      "Epoch [8/30] Loss: 0.1677   Acc: 95.6111\n",
      "Epoch 8/29\n",
      "----------\n",
      "********************train*************\n",
      "Epoch [9/30] Loss: 0.1121   Acc: 97.2262\n",
      "********************val*************\n",
      "Epoch [9/30] Loss: 0.1665   Acc: 95.7500\n",
      "Epoch 9/29\n",
      "----------\n",
      "********************train*************\n",
      "Epoch [10/30] Loss: 0.1110   Acc: 97.2381\n",
      "********************val*************\n",
      "Epoch [10/30] Loss: 0.1653   Acc: 95.7222\n",
      "Epoch 10/29\n",
      "----------\n",
      "********************train*************\n",
      "Epoch [11/30] Loss: 0.1099   Acc: 97.2738\n",
      "********************val*************\n",
      "Epoch [11/30] Loss: 0.1641   Acc: 95.7222\n",
      "Epoch 11/29\n",
      "----------\n",
      "********************train*************\n",
      "Epoch [12/30] Loss: 0.1089   Acc: 97.3571\n",
      "********************val*************\n",
      "Epoch [12/30] Loss: 0.1629   Acc: 95.7778\n",
      "Epoch 12/29\n",
      "----------\n",
      "********************train*************\n",
      "Epoch [13/30] Loss: 0.1078   Acc: 97.3571\n",
      "********************val*************\n",
      "Epoch [13/30] Loss: 0.1617   Acc: 95.7778\n",
      "Epoch 13/29\n",
      "----------\n",
      "********************train*************\n",
      "Epoch [14/30] Loss: 0.1067   Acc: 97.3929\n",
      "********************val*************\n",
      "Epoch [14/30] Loss: 0.1606   Acc: 95.8056\n",
      "Epoch 14/29\n",
      "----------\n",
      "********************train*************\n",
      "Epoch [15/30] Loss: 0.1057   Acc: 97.4048\n",
      "********************val*************\n",
      "Epoch [15/30] Loss: 0.1594   Acc: 95.8333\n",
      "Epoch 15/29\n",
      "----------\n",
      "********************train*************\n",
      "Epoch [16/30] Loss: 0.1047   Acc: 97.4286\n",
      "********************val*************\n",
      "Epoch [16/30] Loss: 0.1583   Acc: 95.8333\n",
      "Epoch 16/29\n",
      "----------\n",
      "********************train*************\n",
      "Epoch [17/30] Loss: 0.1037   Acc: 97.4405\n",
      "********************val*************\n",
      "Epoch [17/30] Loss: 0.1572   Acc: 95.8611\n",
      "Epoch 17/29\n",
      "----------\n",
      "********************train*************\n",
      "Epoch [18/30] Loss: 0.1027   Acc: 97.5238\n",
      "********************val*************\n",
      "Epoch [18/30] Loss: 0.1561   Acc: 95.8611\n",
      "Epoch 18/29\n",
      "----------\n",
      "********************train*************\n",
      "Epoch [19/30] Loss: 0.1017   Acc: 97.5714\n",
      "********************val*************\n",
      "Epoch [19/30] Loss: 0.1550   Acc: 95.8611\n",
      "Epoch 19/29\n",
      "----------\n",
      "********************train*************\n",
      "Epoch [20/30] Loss: 0.1007   Acc: 97.5952\n",
      "********************val*************\n",
      "Epoch [20/30] Loss: 0.1540   Acc: 95.8611\n",
      "Epoch 20/29\n",
      "----------\n",
      "********************train*************\n",
      "Epoch [21/30] Loss: 0.0997   Acc: 97.6310\n",
      "********************val*************\n",
      "Epoch [21/30] Loss: 0.1529   Acc: 95.8333\n",
      "Epoch 21/29\n",
      "----------\n",
      "********************train*************\n",
      "Epoch [22/30] Loss: 0.0988   Acc: 97.6786\n",
      "********************val*************\n",
      "Epoch [22/30] Loss: 0.1519   Acc: 95.9167\n",
      "Epoch 22/29\n",
      "----------\n",
      "********************train*************\n",
      "Epoch [23/30] Loss: 0.0979   Acc: 97.6905\n",
      "********************val*************\n",
      "Epoch [23/30] Loss: 0.1509   Acc: 95.9444\n",
      "Epoch 23/29\n",
      "----------\n",
      "********************train*************\n",
      "Epoch [24/30] Loss: 0.0969   Acc: 97.7381\n",
      "********************val*************\n",
      "Epoch [24/30] Loss: 0.1499   Acc: 96.1389\n",
      "Epoch 24/29\n",
      "----------\n",
      "********************train*************\n",
      "Epoch [25/30] Loss: 0.0960   Acc: 97.7619\n",
      "********************val*************\n",
      "Epoch [25/30] Loss: 0.1489   Acc: 96.1944\n",
      "Epoch 25/29\n",
      "----------\n",
      "********************train*************\n",
      "Epoch [26/30] Loss: 0.0951   Acc: 97.7619\n",
      "********************val*************\n",
      "Epoch [26/30] Loss: 0.1479   Acc: 96.1944\n",
      "Epoch 26/29\n",
      "----------\n",
      "********************train*************\n",
      "Epoch [27/30] Loss: 0.0942   Acc: 97.8214\n",
      "********************val*************\n",
      "Epoch [27/30] Loss: 0.1469   Acc: 96.2500\n",
      "Epoch 27/29\n",
      "----------\n",
      "********************train*************\n",
      "Epoch [28/30] Loss: 0.0934   Acc: 97.8214\n",
      "********************val*************\n",
      "Epoch [28/30] Loss: 0.1459   Acc: 96.2500\n",
      "Epoch 28/29\n",
      "----------\n",
      "********************train*************\n",
      "Epoch [29/30] Loss: 0.0925   Acc: 97.8333\n",
      "********************val*************\n",
      "Epoch [29/30] Loss: 0.1449   Acc: 96.2500\n",
      "Epoch 29/29\n",
      "----------\n",
      "********************train*************\n",
      "Epoch [30/30] Loss: 0.0916   Acc: 97.8571\n",
      "********************val*************\n",
      "Epoch [30/30] Loss: 0.1440   Acc: 96.2500\n",
      "Training complete in 0m 2s\n",
      "Best val Acc: 96.250000\n",
      "Epoch generations ( 51 /100) :Epoch generations ( 52 /100) :Epoch generations ( 53 /100) :Epoch generations ( 54 /100) :Epoch generations ( 55 /100) :Epoch generations ( 56 /100) :Epoch generations ( 57 /100) :Epoch generations ( 58 /100) :Epoch generations ( 59 /100) :Epoch generations ( 60 /100) :Epoch 0/29\n",
      "----------\n",
      "********************train*************\n",
      "Epoch [1/30] Loss: 0.0934   Acc: 97.8214\n",
      "********************val*************\n",
      "Epoch [1/30] Loss: 0.1459   Acc: 96.2500\n",
      "Epoch 1/29\n",
      "----------\n",
      "********************train*************\n",
      "Epoch [2/30] Loss: 0.0925   Acc: 97.8333\n",
      "********************val*************\n",
      "Epoch [2/30] Loss: 0.1449   Acc: 96.2500\n",
      "Epoch 2/29\n",
      "----------\n",
      "********************train*************\n",
      "Epoch [3/30] Loss: 0.0917   Acc: 97.8571\n",
      "********************val*************\n",
      "Epoch [3/30] Loss: 0.1440   Acc: 96.2500\n",
      "Epoch 3/29\n",
      "----------\n",
      "********************train*************\n",
      "Epoch [4/30] Loss: 0.0908   Acc: 97.9405\n",
      "********************val*************\n",
      "Epoch [4/30] Loss: 0.1430   Acc: 96.2500\n",
      "Epoch 4/29\n",
      "----------\n",
      "********************train*************\n",
      "Epoch [5/30] Loss: 0.0900   Acc: 97.9405\n",
      "********************val*************\n",
      "Epoch [5/30] Loss: 0.1421   Acc: 96.2500\n",
      "Epoch 5/29\n",
      "----------\n",
      "********************train*************\n",
      "Epoch [6/30] Loss: 0.0892   Acc: 97.9762\n",
      "********************val*************\n",
      "Epoch [6/30] Loss: 0.1412   Acc: 96.3611\n",
      "Epoch 6/29\n",
      "----------\n",
      "********************train*************\n",
      "Epoch [7/30] Loss: 0.0884   Acc: 97.9881\n",
      "********************val*************\n",
      "Epoch [7/30] Loss: 0.1403   Acc: 96.3889\n",
      "Epoch 7/29\n",
      "----------\n",
      "********************train*************\n",
      "Epoch [8/30] Loss: 0.0876   Acc: 97.9881\n",
      "********************val*************\n",
      "Epoch [8/30] Loss: 0.1393   Acc: 96.3889\n",
      "Epoch 8/29\n",
      "----------\n",
      "********************train*************\n",
      "Epoch [9/30] Loss: 0.0868   Acc: 97.9881\n",
      "********************val*************\n",
      "Epoch [9/30] Loss: 0.1385   Acc: 96.4167\n",
      "Epoch 9/29\n",
      "----------\n",
      "********************train*************\n",
      "Epoch [10/30] Loss: 0.0860   Acc: 98.0000\n",
      "********************val*************\n",
      "Epoch [10/30] Loss: 0.1376   Acc: 96.4722\n",
      "Epoch 10/29\n",
      "----------\n",
      "********************train*************\n",
      "Epoch [11/30] Loss: 0.0852   Acc: 98.0119\n",
      "********************val*************\n",
      "Epoch [11/30] Loss: 0.1367   Acc: 96.5278\n",
      "Epoch 11/29\n",
      "----------\n",
      "********************train*************\n",
      "Epoch [12/30] Loss: 0.0845   Acc: 98.0357\n",
      "********************val*************\n",
      "Epoch [12/30] Loss: 0.1358   Acc: 96.5556\n",
      "Epoch 12/29\n",
      "----------\n",
      "********************train*************\n",
      "Epoch [13/30] Loss: 0.0837   Acc: 98.0833\n",
      "********************val*************\n",
      "Epoch [13/30] Loss: 0.1350   Acc: 96.5833\n",
      "Epoch 13/29\n",
      "----------\n",
      "********************train*************\n",
      "Epoch [14/30] Loss: 0.0830   Acc: 98.0833\n",
      "********************val*************\n",
      "Epoch [14/30] Loss: 0.1341   Acc: 96.6389\n",
      "Epoch 14/29\n",
      "----------\n",
      "********************train*************\n",
      "Epoch [15/30] Loss: 0.0822   Acc: 98.1071\n",
      "********************val*************\n",
      "Epoch [15/30] Loss: 0.1333   Acc: 96.6389\n",
      "Epoch 15/29\n",
      "----------\n",
      "********************train*************\n",
      "Epoch [16/30] Loss: 0.0815   Acc: 98.1071\n",
      "********************val*************\n",
      "Epoch [16/30] Loss: 0.1324   Acc: 96.6111\n",
      "Epoch 16/29\n",
      "----------\n",
      "********************train*************\n",
      "Epoch [17/30] Loss: 0.0808   Acc: 98.1667\n",
      "********************val*************\n",
      "Epoch [17/30] Loss: 0.1316   Acc: 96.6111\n",
      "Epoch 17/29\n",
      "----------\n",
      "********************train*************\n",
      "Epoch [18/30] Loss: 0.0801   Acc: 98.1905\n",
      "********************val*************\n",
      "Epoch [18/30] Loss: 0.1308   Acc: 96.6667\n",
      "Epoch 18/29\n",
      "----------\n",
      "********************train*************\n",
      "Epoch [19/30] Loss: 0.0794   Acc: 98.2143\n",
      "********************val*************\n",
      "Epoch [19/30] Loss: 0.1300   Acc: 96.6667\n",
      "Epoch 19/29\n",
      "----------\n",
      "********************train*************\n",
      "Epoch [20/30] Loss: 0.0787   Acc: 98.2143\n",
      "********************val*************\n",
      "Epoch [20/30] Loss: 0.1292   Acc: 96.6944\n",
      "Epoch 20/29\n",
      "----------\n",
      "********************train*************\n",
      "Epoch [21/30] Loss: 0.0780   Acc: 98.2262\n",
      "********************val*************\n",
      "Epoch [21/30] Loss: 0.1284   Acc: 96.7222\n",
      "Epoch 21/29\n",
      "----------\n",
      "********************train*************\n",
      "Epoch [22/30] Loss: 0.0773   Acc: 98.2738\n",
      "********************val*************\n",
      "Epoch [22/30] Loss: 0.1276   Acc: 96.7500\n",
      "Epoch 22/29\n",
      "----------\n",
      "********************train*************\n",
      "Epoch [23/30] Loss: 0.0766   Acc: 98.2976\n",
      "********************val*************\n",
      "Epoch [23/30] Loss: 0.1269   Acc: 96.7500\n",
      "Epoch 23/29\n",
      "----------\n",
      "********************train*************\n",
      "Epoch [24/30] Loss: 0.0760   Acc: 98.2976\n",
      "********************val*************\n",
      "Epoch [24/30] Loss: 0.1261   Acc: 96.8056\n",
      "Epoch 24/29\n",
      "----------\n",
      "********************train*************\n",
      "Epoch [25/30] Loss: 0.0753   Acc: 98.3452\n",
      "********************val*************\n",
      "Epoch [25/30] Loss: 0.1254   Acc: 96.8056\n",
      "Epoch 25/29\n",
      "----------\n",
      "********************train*************\n",
      "Epoch [26/30] Loss: 0.0747   Acc: 98.3810\n",
      "********************val*************\n",
      "Epoch [26/30] Loss: 0.1246   Acc: 96.8056\n",
      "Epoch 26/29\n",
      "----------\n",
      "********************train*************\n",
      "Epoch [27/30] Loss: 0.0740   Acc: 98.4167\n",
      "********************val*************\n",
      "Epoch [27/30] Loss: 0.1239   Acc: 96.8889\n",
      "Epoch 27/29\n",
      "----------\n",
      "********************train*************\n",
      "Epoch [28/30] Loss: 0.0734   Acc: 98.4524\n",
      "********************val*************\n",
      "Epoch [28/30] Loss: 0.1232   Acc: 96.9444\n",
      "Epoch 28/29\n",
      "----------\n",
      "********************train*************\n",
      "Epoch [29/30] Loss: 0.0728   Acc: 98.5119\n",
      "********************val*************\n",
      "Epoch [29/30] Loss: 0.1225   Acc: 97.0000\n",
      "Epoch 29/29\n",
      "----------\n",
      "********************train*************\n",
      "Epoch [30/30] Loss: 0.0721   Acc: 98.5595\n",
      "********************val*************\n",
      "Epoch [30/30] Loss: 0.1217   Acc: 97.0833\n",
      "Training complete in 0m 2s\n",
      "Best val Acc: 97.083333\n",
      "Epoch generations ( 61 /100) :Epoch generations ( 62 /100) :Epoch generations ( 63 /100) :Epoch generations ( 64 /100) :Epoch generations ( 65 /100) :Epoch generations ( 66 /100) :Epoch generations ( 67 /100) :Epoch generations ( 68 /100) :Epoch generations ( 69 /100) :Epoch generations ( 70 /100) :Epoch 0/29\n",
      "----------\n",
      "********************train*************\n",
      "Epoch [1/30] Loss: 0.0715   Acc: 98.5595\n",
      "********************val*************\n",
      "Epoch [1/30] Loss: 0.1210   Acc: 97.0833\n",
      "Epoch 1/29\n",
      "----------\n",
      "********************train*************\n",
      "Epoch [2/30] Loss: 0.0709   Acc: 98.5833\n",
      "********************val*************\n",
      "Epoch [2/30] Loss: 0.1203   Acc: 97.0833\n",
      "Epoch 2/29\n",
      "----------\n",
      "********************train*************\n",
      "Epoch [3/30] Loss: 0.0703   Acc: 98.5833\n",
      "********************val*************\n",
      "Epoch [3/30] Loss: 0.1197   Acc: 97.0833\n",
      "Epoch 3/29\n",
      "----------\n",
      "********************train*************\n",
      "Epoch [4/30] Loss: 0.0697   Acc: 98.5952\n",
      "********************val*************\n",
      "Epoch [4/30] Loss: 0.1190   Acc: 97.0833\n",
      "Epoch 4/29\n",
      "----------\n",
      "********************train*************\n",
      "Epoch [5/30] Loss: 0.0692   Acc: 98.6429\n",
      "********************val*************\n",
      "Epoch [5/30] Loss: 0.1183   Acc: 97.1111\n",
      "Epoch 5/29\n",
      "----------\n",
      "********************train*************\n",
      "Epoch [6/30] Loss: 0.0686   Acc: 98.6667\n",
      "********************val*************\n",
      "Epoch [6/30] Loss: 0.1177   Acc: 97.1111\n",
      "Epoch 6/29\n",
      "----------\n",
      "********************train*************\n",
      "Epoch [7/30] Loss: 0.0680   Acc: 98.6786\n",
      "********************val*************\n",
      "Epoch [7/30] Loss: 0.1170   Acc: 97.1111\n",
      "Epoch 7/29\n",
      "----------\n",
      "********************train*************\n",
      "Epoch [8/30] Loss: 0.0675   Acc: 98.7262\n",
      "********************val*************\n",
      "Epoch [8/30] Loss: 0.1164   Acc: 97.1111\n",
      "Epoch 8/29\n",
      "----------\n",
      "********************train*************\n",
      "Epoch [9/30] Loss: 0.0669   Acc: 98.7500\n",
      "********************val*************\n",
      "Epoch [9/30] Loss: 0.1157   Acc: 97.1944\n",
      "Epoch 9/29\n",
      "----------\n",
      "********************train*************\n",
      "Epoch [10/30] Loss: 0.0664   Acc: 98.7500\n",
      "********************val*************\n",
      "Epoch [10/30] Loss: 0.1151   Acc: 97.2222\n",
      "Epoch 10/29\n",
      "----------\n",
      "********************train*************\n",
      "Epoch [11/30] Loss: 0.0658   Acc: 98.7500\n",
      "********************val*************\n",
      "Epoch [11/30] Loss: 0.1144   Acc: 97.2778\n",
      "Epoch 11/29\n",
      "----------\n",
      "********************train*************\n",
      "Epoch [12/30] Loss: 0.0653   Acc: 98.7619\n",
      "********************val*************\n",
      "Epoch [12/30] Loss: 0.1138   Acc: 97.2778\n",
      "Epoch 12/29\n",
      "----------\n",
      "********************train*************\n",
      "Epoch [13/30] Loss: 0.0647   Acc: 98.7738\n",
      "********************val*************\n",
      "Epoch [13/30] Loss: 0.1132   Acc: 97.3056\n",
      "Epoch 13/29\n",
      "----------\n",
      "********************train*************\n",
      "Epoch [14/30] Loss: 0.0642   Acc: 98.7976\n",
      "********************val*************\n",
      "Epoch [14/30] Loss: 0.1125   Acc: 97.3333\n",
      "Epoch 14/29\n",
      "----------\n",
      "********************train*************\n",
      "Epoch [15/30] Loss: 0.0637   Acc: 98.8214\n",
      "********************val*************\n",
      "Epoch [15/30] Loss: 0.1119   Acc: 97.3889\n",
      "Epoch 15/29\n",
      "----------\n",
      "********************train*************\n",
      "Epoch [16/30] Loss: 0.0632   Acc: 98.8214\n",
      "********************val*************\n",
      "Epoch [16/30] Loss: 0.1113   Acc: 97.4444\n",
      "Epoch 16/29\n",
      "----------\n",
      "********************train*************\n",
      "Epoch [17/30] Loss: 0.0627   Acc: 98.8333\n",
      "********************val*************\n",
      "Epoch [17/30] Loss: 0.1107   Acc: 97.4444\n",
      "Epoch 17/29\n",
      "----------\n",
      "********************train*************\n",
      "Epoch [18/30] Loss: 0.0622   Acc: 98.8452\n",
      "********************val*************\n",
      "Epoch [18/30] Loss: 0.1101   Acc: 97.5556\n",
      "Epoch 18/29\n",
      "----------\n",
      "********************train*************\n",
      "Epoch [19/30] Loss: 0.0617   Acc: 98.8452\n",
      "********************val*************\n",
      "Epoch [19/30] Loss: 0.1095   Acc: 97.5556\n",
      "Epoch 19/29\n",
      "----------\n",
      "********************train*************\n",
      "Epoch [20/30] Loss: 0.0612   Acc: 98.8571\n",
      "********************val*************\n",
      "Epoch [20/30] Loss: 0.1089   Acc: 97.5556\n",
      "Epoch 20/29\n",
      "----------\n",
      "********************train*************\n",
      "Epoch [21/30] Loss: 0.0607   Acc: 98.8690\n",
      "********************val*************\n",
      "Epoch [21/30] Loss: 0.1083   Acc: 97.5556\n",
      "Epoch 21/29\n",
      "----------\n",
      "********************train*************\n",
      "Epoch [22/30] Loss: 0.0602   Acc: 98.8690\n",
      "********************val*************\n",
      "Epoch [22/30] Loss: 0.1078   Acc: 97.5556\n",
      "Epoch 22/29\n",
      "----------\n",
      "********************train*************\n",
      "Epoch [23/30] Loss: 0.0597   Acc: 98.8929\n",
      "********************val*************\n",
      "Epoch [23/30] Loss: 0.1072   Acc: 97.6111\n",
      "Epoch 23/29\n",
      "----------\n",
      "********************train*************\n",
      "Epoch [24/30] Loss: 0.0593   Acc: 98.9048\n",
      "********************val*************\n",
      "Epoch [24/30] Loss: 0.1066   Acc: 97.6389\n",
      "Epoch 24/29\n",
      "----------\n",
      "********************train*************\n",
      "Epoch [25/30] Loss: 0.0588   Acc: 98.9048\n",
      "********************val*************\n",
      "Epoch [25/30] Loss: 0.1061   Acc: 97.6389\n",
      "Epoch 25/29\n",
      "----------\n",
      "********************train*************\n",
      "Epoch [26/30] Loss: 0.0583   Acc: 98.9524\n",
      "********************val*************\n",
      "Epoch [26/30] Loss: 0.1055   Acc: 97.6389\n",
      "Epoch 26/29\n",
      "----------\n",
      "********************train*************\n",
      "Epoch [27/30] Loss: 0.0579   Acc: 98.9762\n",
      "********************val*************\n",
      "Epoch [27/30] Loss: 0.1050   Acc: 97.6389\n",
      "Epoch 27/29\n",
      "----------\n",
      "********************train*************\n",
      "Epoch [28/30] Loss: 0.0574   Acc: 98.9762\n",
      "********************val*************\n",
      "Epoch [28/30] Loss: 0.1044   Acc: 97.6944\n",
      "Epoch 28/29\n",
      "----------\n",
      "********************train*************\n",
      "Epoch [29/30] Loss: 0.0570   Acc: 99.0000\n",
      "********************val*************\n",
      "Epoch [29/30] Loss: 0.1039   Acc: 97.6944\n",
      "Epoch 29/29\n",
      "----------\n",
      "********************train*************\n",
      "Epoch [30/30] Loss: 0.0565   Acc: 99.0119\n",
      "********************val*************\n",
      "Epoch [30/30] Loss: 0.1034   Acc: 97.6944\n",
      "Training complete in 0m 2s\n",
      "Best val Acc: 97.694444\n",
      "Epoch generations ( 71 /100) :Epoch generations ( 72 /100) :Epoch generations ( 73 /100) :Epoch generations ( 74 /100) :Epoch generations ( 75 /100) :Epoch generations ( 76 /100) :Epoch generations ( 77 /100) :Epoch generations ( 78 /100) :Epoch generations ( 79 /100) :Epoch generations ( 80 /100) :Epoch 0/29\n",
      "----------\n",
      "********************train*************\n",
      "Epoch [1/30] Loss: 0.0570   Acc: 99.0000\n",
      "********************val*************\n",
      "Epoch [1/30] Loss: 0.1039   Acc: 97.6944\n",
      "Epoch 1/29\n",
      "----------\n",
      "********************train*************\n",
      "Epoch [2/30] Loss: 0.0565   Acc: 99.0119\n",
      "********************val*************\n",
      "Epoch [2/30] Loss: 0.1034   Acc: 97.6944\n",
      "Epoch 2/29\n",
      "----------\n",
      "********************train*************\n",
      "Epoch [3/30] Loss: 0.0561   Acc: 99.0119\n",
      "********************val*************\n",
      "Epoch [3/30] Loss: 0.1028   Acc: 97.7500\n",
      "Epoch 3/29\n",
      "----------\n",
      "********************train*************\n",
      "Epoch [4/30] Loss: 0.0557   Acc: 99.0357\n",
      "********************val*************\n",
      "Epoch [4/30] Loss: 0.1023   Acc: 97.7778\n",
      "Epoch 4/29\n",
      "----------\n",
      "********************train*************\n",
      "Epoch [5/30] Loss: 0.0553   Acc: 99.0357\n",
      "********************val*************\n",
      "Epoch [5/30] Loss: 0.1018   Acc: 97.8333\n",
      "Epoch 5/29\n",
      "----------\n",
      "********************train*************\n",
      "Epoch [6/30] Loss: 0.0548   Acc: 99.1310\n",
      "********************val*************\n",
      "Epoch [6/30] Loss: 0.1013   Acc: 97.8611\n",
      "Epoch 6/29\n",
      "----------\n",
      "********************train*************\n",
      "Epoch [7/30] Loss: 0.0544   Acc: 99.1667\n",
      "********************val*************\n",
      "Epoch [7/30] Loss: 0.1008   Acc: 97.8611\n",
      "Epoch 7/29\n",
      "----------\n",
      "********************train*************\n",
      "Epoch [8/30] Loss: 0.0540   Acc: 99.1786\n",
      "********************val*************\n",
      "Epoch [8/30] Loss: 0.1003   Acc: 97.8611\n",
      "Epoch 8/29\n",
      "----------\n",
      "********************train*************\n",
      "Epoch [9/30] Loss: 0.0536   Acc: 99.2024\n",
      "********************val*************\n",
      "Epoch [9/30] Loss: 0.0998   Acc: 97.8611\n",
      "Epoch 9/29\n",
      "----------\n",
      "********************train*************\n",
      "Epoch [10/30] Loss: 0.0532   Acc: 99.2024\n",
      "********************val*************\n",
      "Epoch [10/30] Loss: 0.0993   Acc: 97.8611\n",
      "Epoch 10/29\n",
      "----------\n",
      "********************train*************\n",
      "Epoch [11/30] Loss: 0.0528   Acc: 99.2024\n",
      "********************val*************\n",
      "Epoch [11/30] Loss: 0.0988   Acc: 97.8611\n",
      "Epoch 11/29\n",
      "----------\n",
      "********************train*************\n",
      "Epoch [12/30] Loss: 0.0524   Acc: 99.2024\n",
      "********************val*************\n",
      "Epoch [12/30] Loss: 0.0983   Acc: 97.8611\n",
      "Epoch 12/29\n",
      "----------\n",
      "********************train*************\n",
      "Epoch [13/30] Loss: 0.0520   Acc: 99.2024\n",
      "********************val*************\n",
      "Epoch [13/30] Loss: 0.0978   Acc: 97.8611\n",
      "Epoch 13/29\n",
      "----------\n",
      "********************train*************\n",
      "Epoch [14/30] Loss: 0.0516   Acc: 99.2262\n",
      "********************val*************\n",
      "Epoch [14/30] Loss: 0.0973   Acc: 97.8611\n",
      "Epoch 14/29\n",
      "----------\n",
      "********************train*************\n",
      "Epoch [15/30] Loss: 0.0512   Acc: 99.2500\n",
      "********************val*************\n",
      "Epoch [15/30] Loss: 0.0968   Acc: 97.8611\n",
      "Epoch 15/29\n",
      "----------\n",
      "********************train*************\n",
      "Epoch [16/30] Loss: 0.0508   Acc: 99.2500\n",
      "********************val*************\n",
      "Epoch [16/30] Loss: 0.0964   Acc: 97.8611\n",
      "Epoch 16/29\n",
      "----------\n",
      "********************train*************\n",
      "Epoch [17/30] Loss: 0.0504   Acc: 99.2976\n",
      "********************val*************\n",
      "Epoch [17/30] Loss: 0.0959   Acc: 97.8889\n",
      "Epoch 17/29\n",
      "----------\n",
      "********************train*************\n",
      "Epoch [18/30] Loss: 0.0501   Acc: 99.3214\n",
      "********************val*************\n",
      "Epoch [18/30] Loss: 0.0954   Acc: 97.9444\n",
      "Epoch 18/29\n",
      "----------\n",
      "********************train*************\n",
      "Epoch [19/30] Loss: 0.0497   Acc: 99.3214\n",
      "********************val*************\n",
      "Epoch [19/30] Loss: 0.0950   Acc: 97.9444\n",
      "Epoch 19/29\n",
      "----------\n",
      "********************train*************\n",
      "Epoch [20/30] Loss: 0.0493   Acc: 99.3452\n",
      "********************val*************\n",
      "Epoch [20/30] Loss: 0.0945   Acc: 97.9722\n",
      "Epoch 20/29\n",
      "----------\n",
      "********************train*************\n",
      "Epoch [21/30] Loss: 0.0490   Acc: 99.3810\n",
      "********************val*************\n",
      "Epoch [21/30] Loss: 0.0941   Acc: 97.9722\n",
      "Epoch 21/29\n",
      "----------\n",
      "********************train*************\n",
      "Epoch [22/30] Loss: 0.0486   Acc: 99.3810\n",
      "********************val*************\n",
      "Epoch [22/30] Loss: 0.0936   Acc: 97.9722\n",
      "Epoch 22/29\n",
      "----------\n",
      "********************train*************\n",
      "Epoch [23/30] Loss: 0.0482   Acc: 99.3810\n",
      "********************val*************\n",
      "Epoch [23/30] Loss: 0.0932   Acc: 97.9722\n",
      "Epoch 23/29\n",
      "----------\n",
      "********************train*************\n",
      "Epoch [24/30] Loss: 0.0479   Acc: 99.3810\n",
      "********************val*************\n",
      "Epoch [24/30] Loss: 0.0927   Acc: 97.9722\n",
      "Epoch 24/29\n",
      "----------\n",
      "********************train*************\n",
      "Epoch [25/30] Loss: 0.0475   Acc: 99.3810\n",
      "********************val*************\n",
      "Epoch [25/30] Loss: 0.0923   Acc: 98.0556\n",
      "Epoch 25/29\n",
      "----------\n",
      "********************train*************\n",
      "Epoch [26/30] Loss: 0.0472   Acc: 99.3810\n",
      "********************val*************\n",
      "Epoch [26/30] Loss: 0.0918   Acc: 98.0833\n",
      "Epoch 26/29\n",
      "----------\n",
      "********************train*************\n",
      "Epoch [27/30] Loss: 0.0468   Acc: 99.3810\n",
      "********************val*************\n",
      "Epoch [27/30] Loss: 0.0914   Acc: 98.0833\n",
      "Epoch 27/29\n",
      "----------\n",
      "********************train*************\n",
      "Epoch [28/30] Loss: 0.0465   Acc: 99.3810\n",
      "********************val*************\n",
      "Epoch [28/30] Loss: 0.0910   Acc: 98.1111\n",
      "Epoch 28/29\n",
      "----------\n",
      "********************train*************\n",
      "Epoch [29/30] Loss: 0.0462   Acc: 99.3810\n",
      "********************val*************\n",
      "Epoch [29/30] Loss: 0.0906   Acc: 98.1111\n",
      "Epoch 29/29\n",
      "----------\n",
      "********************train*************\n",
      "Epoch [30/30] Loss: 0.0458   Acc: 99.3810\n",
      "********************val*************\n",
      "Epoch [30/30] Loss: 0.0901   Acc: 98.1389\n",
      "Training complete in 0m 2s\n",
      "Best val Acc: 98.138889\n",
      "Epoch generations ( 81 /100) :Epoch generations ( 82 /100) :Epoch generations ( 83 /100) :Epoch generations ( 84 /100) :Epoch generations ( 85 /100) :Epoch generations ( 86 /100) :Epoch generations ( 87 /100) :Epoch generations ( 88 /100) :Epoch generations ( 89 /100) :Epoch generations ( 90 /100) :Epoch 0/29\n",
      "----------\n",
      "********************train*************\n",
      "Epoch [1/30] Loss: 0.0455   Acc: 99.3929\n",
      "********************val*************\n",
      "Epoch [1/30] Loss: 0.0897   Acc: 98.1389\n",
      "Epoch 1/29\n",
      "----------\n",
      "********************train*************\n",
      "Epoch [2/30] Loss: 0.0452   Acc: 99.4167\n",
      "********************val*************\n",
      "Epoch [2/30] Loss: 0.0893   Acc: 98.1389\n",
      "Epoch 2/29\n",
      "----------\n",
      "********************train*************\n",
      "Epoch [3/30] Loss: 0.0449   Acc: 99.4286\n",
      "********************val*************\n",
      "Epoch [3/30] Loss: 0.0889   Acc: 98.1389\n",
      "Epoch 3/29\n",
      "----------\n",
      "********************train*************\n",
      "Epoch [4/30] Loss: 0.0445   Acc: 99.4286\n",
      "********************val*************\n",
      "Epoch [4/30] Loss: 0.0885   Acc: 98.1389\n",
      "Epoch 4/29\n",
      "----------\n",
      "********************train*************\n",
      "Epoch [5/30] Loss: 0.0442   Acc: 99.4286\n",
      "********************val*************\n",
      "Epoch [5/30] Loss: 0.0881   Acc: 98.1389\n",
      "Epoch 5/29\n",
      "----------\n",
      "********************train*************\n",
      "Epoch [6/30] Loss: 0.0439   Acc: 99.4286\n",
      "********************val*************\n",
      "Epoch [6/30] Loss: 0.0877   Acc: 98.1667\n",
      "Epoch 6/29\n",
      "----------\n",
      "********************train*************\n",
      "Epoch [7/30] Loss: 0.0436   Acc: 99.4286\n",
      "********************val*************\n",
      "Epoch [7/30] Loss: 0.0873   Acc: 98.1667\n",
      "Epoch 7/29\n",
      "----------\n",
      "********************train*************\n",
      "Epoch [8/30] Loss: 0.0433   Acc: 99.4286\n",
      "********************val*************\n",
      "Epoch [8/30] Loss: 0.0869   Acc: 98.2222\n",
      "Epoch 8/29\n",
      "----------\n",
      "********************train*************\n",
      "Epoch [9/30] Loss: 0.0430   Acc: 99.4286\n",
      "********************val*************\n",
      "Epoch [9/30] Loss: 0.0865   Acc: 98.2222\n",
      "Epoch 9/29\n",
      "----------\n",
      "********************train*************\n",
      "Epoch [10/30] Loss: 0.0427   Acc: 99.4286\n",
      "********************val*************\n",
      "Epoch [10/30] Loss: 0.0861   Acc: 98.2222\n",
      "Epoch 10/29\n",
      "----------\n",
      "********************train*************\n",
      "Epoch [11/30] Loss: 0.0424   Acc: 99.4405\n",
      "********************val*************\n",
      "Epoch [11/30] Loss: 0.0857   Acc: 98.2222\n",
      "Epoch 11/29\n",
      "----------\n",
      "********************train*************\n",
      "Epoch [12/30] Loss: 0.0421   Acc: 99.4643\n",
      "********************val*************\n",
      "Epoch [12/30] Loss: 0.0853   Acc: 98.2222\n",
      "Epoch 12/29\n",
      "----------\n",
      "********************train*************\n",
      "Epoch [13/30] Loss: 0.0418   Acc: 99.4762\n",
      "********************val*************\n",
      "Epoch [13/30] Loss: 0.0850   Acc: 98.2222\n",
      "Epoch 13/29\n",
      "----------\n",
      "********************train*************\n",
      "Epoch [14/30] Loss: 0.0415   Acc: 99.4762\n",
      "********************val*************\n",
      "Epoch [14/30] Loss: 0.0846   Acc: 98.2222\n",
      "Epoch 14/29\n",
      "----------\n",
      "********************train*************\n",
      "Epoch [15/30] Loss: 0.0412   Acc: 99.4762\n",
      "********************val*************\n",
      "Epoch [15/30] Loss: 0.0842   Acc: 98.2222\n",
      "Epoch 15/29\n",
      "----------\n",
      "********************train*************\n",
      "Epoch [16/30] Loss: 0.0409   Acc: 99.4762\n",
      "********************val*************\n",
      "Epoch [16/30] Loss: 0.0838   Acc: 98.2222\n",
      "Epoch 16/29\n",
      "----------\n",
      "********************train*************\n",
      "Epoch [17/30] Loss: 0.0406   Acc: 99.4762\n",
      "********************val*************\n",
      "Epoch [17/30] Loss: 0.0835   Acc: 98.2500\n",
      "Epoch 17/29\n",
      "----------\n",
      "********************train*************\n",
      "Epoch [18/30] Loss: 0.0404   Acc: 99.4762\n",
      "********************val*************\n",
      "Epoch [18/30] Loss: 0.0831   Acc: 98.2500\n",
      "Epoch 18/29\n",
      "----------\n",
      "********************train*************\n",
      "Epoch [19/30] Loss: 0.0401   Acc: 99.4762\n",
      "********************val*************\n",
      "Epoch [19/30] Loss: 0.0827   Acc: 98.2500\n",
      "Epoch 19/29\n",
      "----------\n",
      "********************train*************\n",
      "Epoch [20/30] Loss: 0.0398   Acc: 99.4762\n",
      "********************val*************\n",
      "Epoch [20/30] Loss: 0.0824   Acc: 98.2500\n",
      "Epoch 20/29\n",
      "----------\n",
      "********************train*************\n",
      "Epoch [21/30] Loss: 0.0395   Acc: 99.4762\n",
      "********************val*************\n",
      "Epoch [21/30] Loss: 0.0820   Acc: 98.2500\n",
      "Epoch 21/29\n",
      "----------\n",
      "********************train*************\n",
      "Epoch [22/30] Loss: 0.0393   Acc: 99.4881\n",
      "********************val*************\n",
      "Epoch [22/30] Loss: 0.0817   Acc: 98.2500\n",
      "Epoch 22/29\n",
      "----------\n",
      "********************train*************\n",
      "Epoch [23/30] Loss: 0.0390   Acc: 99.4881\n",
      "********************val*************\n",
      "Epoch [23/30] Loss: 0.0813   Acc: 98.2500\n",
      "Epoch 23/29\n",
      "----------\n",
      "********************train*************\n",
      "Epoch [24/30] Loss: 0.0387   Acc: 99.5119\n",
      "********************val*************\n",
      "Epoch [24/30] Loss: 0.0810   Acc: 98.2500\n",
      "Epoch 24/29\n",
      "----------\n",
      "********************train*************\n",
      "Epoch [25/30] Loss: 0.0385   Acc: 99.5357\n",
      "********************val*************\n",
      "Epoch [25/30] Loss: 0.0806   Acc: 98.2500\n",
      "Epoch 25/29\n",
      "----------\n",
      "********************train*************\n",
      "Epoch [26/30] Loss: 0.0382   Acc: 99.5357\n",
      "********************val*************\n",
      "Epoch [26/30] Loss: 0.0803   Acc: 98.2500\n",
      "Epoch 26/29\n",
      "----------\n",
      "********************train*************\n",
      "Epoch [27/30] Loss: 0.0380   Acc: 99.5357\n",
      "********************val*************\n",
      "Epoch [27/30] Loss: 0.0799   Acc: 98.2500\n",
      "Epoch 27/29\n",
      "----------\n",
      "********************train*************\n",
      "Epoch [28/30] Loss: 0.0377   Acc: 99.5357\n",
      "********************val*************\n",
      "Epoch [28/30] Loss: 0.0796   Acc: 98.2778\n",
      "Epoch 28/29\n",
      "----------\n",
      "********************train*************\n",
      "Epoch [29/30] Loss: 0.0374   Acc: 99.5357\n",
      "********************val*************\n",
      "Epoch [29/30] Loss: 0.0793   Acc: 98.2778\n",
      "Epoch 29/29\n",
      "----------\n",
      "********************train*************\n",
      "Epoch [30/30] Loss: 0.0372   Acc: 99.5357\n",
      "********************val*************\n",
      "Epoch [30/30] Loss: 0.0789   Acc: 98.2778\n",
      "Training complete in 0m 2s\n",
      "Best val Acc: 98.277778\n",
      "Epoch generations ( 91 /100) :Epoch generations ( 92 /100) :Epoch generations ( 93 /100) :Epoch generations ( 94 /100) :Epoch generations ( 95 /100) :Epoch generations ( 96 /100) :Epoch generations ( 97 /100) :Epoch generations ( 98 /100) :Epoch generations ( 99 /100) :Epoch generations ( 100 /100) :Epoch 0/29\n",
      "----------\n",
      "********************train*************\n",
      "Epoch [1/30] Loss: 0.0374   Acc: 99.5357\n",
      "********************val*************\n",
      "Epoch [1/30] Loss: 0.0793   Acc: 98.2778\n",
      "Epoch 1/29\n",
      "----------\n",
      "********************train*************\n",
      "Epoch [2/30] Loss: 0.0372   Acc: 99.5357\n",
      "********************val*************\n",
      "Epoch [2/30] Loss: 0.0789   Acc: 98.2778\n",
      "Epoch 2/29\n",
      "----------\n",
      "********************train*************\n",
      "Epoch [3/30] Loss: 0.0369   Acc: 99.5357\n",
      "********************val*************\n",
      "Epoch [3/30] Loss: 0.0786   Acc: 98.2778\n",
      "Epoch 3/29\n",
      "----------\n",
      "********************train*************\n",
      "Epoch [4/30] Loss: 0.0367   Acc: 99.5595\n",
      "********************val*************\n",
      "Epoch [4/30] Loss: 0.0783   Acc: 98.3333\n",
      "Epoch 4/29\n",
      "----------\n",
      "********************train*************\n",
      "Epoch [5/30] Loss: 0.0365   Acc: 99.5595\n",
      "********************val*************\n",
      "Epoch [5/30] Loss: 0.0780   Acc: 98.3611\n",
      "Epoch 5/29\n",
      "----------\n",
      "********************train*************\n",
      "Epoch [6/30] Loss: 0.0362   Acc: 99.5714\n",
      "********************val*************\n",
      "Epoch [6/30] Loss: 0.0776   Acc: 98.3611\n",
      "Epoch 6/29\n",
      "----------\n",
      "********************train*************\n",
      "Epoch [7/30] Loss: 0.0360   Acc: 99.5714\n",
      "********************val*************\n",
      "Epoch [7/30] Loss: 0.0773   Acc: 98.3611\n",
      "Epoch 7/29\n",
      "----------\n",
      "********************train*************\n",
      "Epoch [8/30] Loss: 0.0357   Acc: 99.5714\n",
      "********************val*************\n",
      "Epoch [8/30] Loss: 0.0770   Acc: 98.3611\n",
      "Epoch 8/29\n",
      "----------\n",
      "********************train*************\n",
      "Epoch [9/30] Loss: 0.0355   Acc: 99.5714\n",
      "********************val*************\n",
      "Epoch [9/30] Loss: 0.0767   Acc: 98.3611\n",
      "Epoch 9/29\n",
      "----------\n",
      "********************train*************\n",
      "Epoch [10/30] Loss: 0.0353   Acc: 99.5952\n",
      "********************val*************\n",
      "Epoch [10/30] Loss: 0.0764   Acc: 98.3611\n",
      "Epoch 10/29\n",
      "----------\n",
      "********************train*************\n",
      "Epoch [11/30] Loss: 0.0350   Acc: 99.5952\n",
      "********************val*************\n",
      "Epoch [11/30] Loss: 0.0761   Acc: 98.3611\n",
      "Epoch 11/29\n",
      "----------\n",
      "********************train*************\n",
      "Epoch [12/30] Loss: 0.0348   Acc: 99.5952\n",
      "********************val*************\n",
      "Epoch [12/30] Loss: 0.0758   Acc: 98.3611\n",
      "Epoch 12/29\n",
      "----------\n",
      "********************train*************\n",
      "Epoch [13/30] Loss: 0.0346   Acc: 99.5952\n",
      "********************val*************\n",
      "Epoch [13/30] Loss: 0.0755   Acc: 98.3611\n",
      "Epoch 13/29\n",
      "----------\n",
      "********************train*************\n",
      "Epoch [14/30] Loss: 0.0343   Acc: 99.6310\n",
      "********************val*************\n",
      "Epoch [14/30] Loss: 0.0752   Acc: 98.3611\n",
      "Epoch 14/29\n",
      "----------\n",
      "********************train*************\n",
      "Epoch [15/30] Loss: 0.0341   Acc: 99.6429\n",
      "********************val*************\n",
      "Epoch [15/30] Loss: 0.0749   Acc: 98.3889\n",
      "Epoch 15/29\n",
      "----------\n",
      "********************train*************\n",
      "Epoch [16/30] Loss: 0.0339   Acc: 99.6429\n",
      "********************val*************\n",
      "Epoch [16/30] Loss: 0.0746   Acc: 98.3889\n",
      "Epoch 16/29\n",
      "----------\n",
      "********************train*************\n",
      "Epoch [17/30] Loss: 0.0337   Acc: 99.6429\n",
      "********************val*************\n",
      "Epoch [17/30] Loss: 0.0743   Acc: 98.3889\n",
      "Epoch 17/29\n",
      "----------\n",
      "********************train*************\n",
      "Epoch [18/30] Loss: 0.0335   Acc: 99.6429\n",
      "********************val*************\n",
      "Epoch [18/30] Loss: 0.0740   Acc: 98.3889\n",
      "Epoch 18/29\n",
      "----------\n",
      "********************train*************\n",
      "Epoch [19/30] Loss: 0.0332   Acc: 99.6429\n",
      "********************val*************\n",
      "Epoch [19/30] Loss: 0.0737   Acc: 98.3889\n",
      "Epoch 19/29\n",
      "----------\n",
      "********************train*************\n",
      "Epoch [20/30] Loss: 0.0330   Acc: 99.6429\n",
      "********************val*************\n",
      "Epoch [20/30] Loss: 0.0734   Acc: 98.4167\n",
      "Epoch 20/29\n",
      "----------\n",
      "********************train*************\n",
      "Epoch [21/30] Loss: 0.0328   Acc: 99.6429\n",
      "********************val*************\n",
      "Epoch [21/30] Loss: 0.0731   Acc: 98.4444\n",
      "Epoch 21/29\n",
      "----------\n",
      "********************train*************\n",
      "Epoch [22/30] Loss: 0.0326   Acc: 99.6429\n",
      "********************val*************\n",
      "Epoch [22/30] Loss: 0.0728   Acc: 98.4444\n",
      "Epoch 22/29\n",
      "----------\n",
      "********************train*************\n",
      "Epoch [23/30] Loss: 0.0324   Acc: 99.6429\n",
      "********************val*************\n",
      "Epoch [23/30] Loss: 0.0725   Acc: 98.4444\n",
      "Epoch 23/29\n",
      "----------\n",
      "********************train*************\n",
      "Epoch [24/30] Loss: 0.0322   Acc: 99.6429\n",
      "********************val*************\n",
      "Epoch [24/30] Loss: 0.0723   Acc: 98.4444\n",
      "Epoch 24/29\n",
      "----------\n",
      "********************train*************\n",
      "Epoch [25/30] Loss: 0.0320   Acc: 99.6429\n",
      "********************val*************\n",
      "Epoch [25/30] Loss: 0.0720   Acc: 98.4444\n",
      "Epoch 25/29\n",
      "----------\n",
      "********************train*************\n",
      "Epoch [26/30] Loss: 0.0318   Acc: 99.6429\n",
      "********************val*************\n",
      "Epoch [26/30] Loss: 0.0717   Acc: 98.4722\n",
      "Epoch 26/29\n",
      "----------\n",
      "********************train*************\n",
      "Epoch [27/30] Loss: 0.0316   Acc: 99.6429\n",
      "********************val*************\n",
      "Epoch [27/30] Loss: 0.0714   Acc: 98.5000\n",
      "Epoch 27/29\n",
      "----------\n",
      "********************train*************\n",
      "Epoch [28/30] Loss: 0.0314   Acc: 99.6429\n",
      "********************val*************\n",
      "Epoch [28/30] Loss: 0.0712   Acc: 98.5000\n",
      "Epoch 28/29\n",
      "----------\n",
      "********************train*************\n",
      "Epoch [29/30] Loss: 0.0312   Acc: 99.6429\n",
      "********************val*************\n",
      "Epoch [29/30] Loss: 0.0709   Acc: 98.5278\n",
      "Epoch 29/29\n",
      "----------\n",
      "********************train*************\n",
      "Epoch [30/30] Loss: 0.0310   Acc: 99.6429\n",
      "********************val*************\n",
      "Epoch [30/30] Loss: 0.0706   Acc: 98.5278\n",
      "Training complete in 0m 2s\n",
      "Best val Acc: 98.527778\n"
     ]
    }
   ],
   "source": [
    "#control block controls the epochs and the generations of mask\n",
    "#step 1 an object of the population class randomly generating the first population \n",
    "#step2 :calculate fitness of each entitiy of the population \n",
    "#step3: creates a mating pool of the population based on the worst two performing parent \n",
    "#step 4 :fittest mask of the generating along with keep_prob found \n",
    "#step 5: if 0th ,10th ,20th, the epochs starts training on the worst performing mask /other wise new generation is created \n",
    "\n",
    "epochgens=0\n",
    "population =Population(mutation_rate,max_population,maskLength)\n",
    "total_acc=[]\n",
    "p=0\n",
    "while (epochgens<=num_epochs):\n",
    "    print ('Epoch generations (',epochgens,'/{})'.format(num_epochs),end=' :')\n",
    "    if p < 0.8:\n",
    "      population .calcFitness(model)\n",
    "      population.naturalSelection()\n",
    "      fittestmask,p = population .fittest()\n",
    "      accuracy=fittestmask.fitness(model)\n",
    "      print (\"accuracy(fittest mask)\",accuracy,\"keep_prob\",p,end='\\n')\n",
    "    if (epochgens%10==0):\n",
    "        model,losses,accuracies=train_model(model,criterion,optimizer,fittestmask.gene,p,30)\n",
    "        total_acc=total_acc+accuracies\n",
    "    population.generate()\n",
    "    epochgens+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "dZvvW_lI5j67"
   },
   "outputs": [],
   "source": [
    "0+(31*10)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "name": "SimpleANN_ga.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
