{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import random\n",
    "import torch\n",
    "import torchvision\n",
    "import time \n",
    "from torchvision import datasets ,transforms\n",
    "from torchvision.datasets import ImageFolder\n",
    "from torch.utils.data import DataLoader,Dataset\n",
    "from torch import nn \n",
    "from torch.nn.functional import relu ,softmax \n",
    "import copy \n",
    "from torch.utils.data import Subset\n",
    "import pandas as pd \n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import pyswarms as ps \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5000\n",
      "3000\n",
      "2000\n"
     ]
    }
   ],
   "source": [
    "\n",
    "data_transform=transforms.Compose ([transforms.ToTensor()])\n",
    "class light_source_dataset(Dataset):\n",
    "    def __init__(self,path,transform):\n",
    "        self.df_data=pd.read_csv(path)\n",
    "        self.df_data['Classifier'] = pd.Categorical(pd.factorize(self.df_data['Classifier'])[0])\n",
    "        self.labels=np.asarray(self.df_data.iloc[:,self.df_data.shape[1]-1])\n",
    "        self.image_as_np=np.asarray(self.df_data.iloc[:,0:self.df_data.shape[1]-1]).astype('uint8')\n",
    "        self.trans=transform\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.df_data.index)\n",
    "    def __getitem__(self,index):\n",
    "        image_np=self.image_as_np[index,:,None]\n",
    "       \n",
    "        pillow_image=Image.fromarray(image_np.astype('uint8'))\n",
    "        \n",
    "        single_label=self.labels[index]\n",
    "        if (self.trans is not None):\n",
    "            img_as_tensor=self.trans(pillow_image)\n",
    "        \n",
    "        \n",
    "            \n",
    "        return (img_as_tensor,single_label)\n",
    "        \n",
    "dataset=light_source_dataset('WaveformEW/WaveformEW.csv',data_transform)\n",
    "print(len(dataset))\n",
    "train_size=int (len(dataset)*0.6)\n",
    "test_size=len(dataset)-train_size\n",
    "trainloader=DataLoader(dataset,batch_size=1)\n",
    "torch.manual_seed(1)\n",
    "train_data,test_data=torch.utils.data.random_split(dataset,[int (train_size), int (test_size)])\n",
    "dataloader={'train':DataLoader(train_data,shuffle=False ,batch_size=16),\n",
    "            'val':DataLoader(test_data,shuffle=False,batch_size=16\n",
    "                            )}\n",
    "\n",
    "dataset_sizes={'train':len(train_data),\n",
    "               'val':len(test_data)}\n",
    "\n",
    "device=torch.device(\"cuda:0\"if torch.cuda.is_available () else \"cpu\")\n",
    "print(dataset_sizes[\"train\"])\n",
    "print (dataset_sizes[\"val\"])\n",
    "\n",
    " \n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class Model(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super (Model,self ).__init__()\n",
    "        self .linear1=nn.Linear (40,100)\n",
    "        self.linear2=nn.Linear (100,3)\n",
    "        \n",
    "    def Forward (self,x,mask,p):\n",
    "        #feed forward function \n",
    "        x=x.view(-1,40)\n",
    "        act1=relu(self.linear1(x))\n",
    "        act1_masked =self.masking (act1,mask,p)\n",
    "        act2=softmax(self.linear2(act1_masked ))\n",
    "        return act2\n",
    "    \n",
    "    \n",
    "    \n",
    "    def masking (self,act1,mask,p):\n",
    "        if (self.training ==True ):\n",
    "            return ((act1*mask)/p)\n",
    "        else :\n",
    "            return (act1)\n",
    "        \n",
    "\n",
    "\n",
    "\n",
    "  \n",
    "  \n",
    "  \n",
    "\n",
    "\n",
    "      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, criterion, optimizer,mask,p, num_epochs=25):\n",
    "  # the mask has to be specified \n",
    "\n",
    "      mask=torch.from_numpy(mask)\n",
    "      since = time.time()\n",
    "\n",
    "      best_model_wts = copy.deepcopy(model.state_dict())\n",
    "      best_acc = 0.0\n",
    "      losses=[]\n",
    "      accuracies=[]\n",
    "\n",
    "      for epoch in range(num_epochs):\n",
    "          print('Epoch {}/{}'.format(epoch, num_epochs - 1))\n",
    "          print('-' * 10)\n",
    "\n",
    "          # Each epoch has a training and validation phase\n",
    "          for phase in ['train', 'val']:\n",
    "              if phase == 'train':\n",
    "                  \n",
    "                  model.train()  # Set model to training mode\n",
    "              else:\n",
    "\n",
    "                  model.eval()   # Set model to evaluate mode\n",
    "\n",
    "              running_loss = 0.0\n",
    "              running_corrects = 0\n",
    "\n",
    "              # Iterate over data.\n",
    "              for inputs, labels in dataloader[phase]:\n",
    "                inputs=inputs.to(device)\n",
    "                labels=labels.to(device)\n",
    "                # zero the parameter gradients\n",
    "                optimizer.zero_grad()\n",
    "\n",
    "                # forward\n",
    "                # track history if only in train\n",
    "                with torch.set_grad_enabled(phase == 'train'):\n",
    "                    outputs= model.Forward(inputs,mask,p)\n",
    "\n",
    "                    _, preds = torch.max(outputs, 1)\n",
    "                    loss = criterion(outputs, labels)# backward + optimize only if in training phase\n",
    "                    if phase == 'train':\n",
    "                        loss.backward()\n",
    "                        optimizer.step()\n",
    "                        #print (model.linear1.weight.grad)\n",
    "                        #print (model.linear2.weight.grad)\n",
    "\n",
    "                  # statistics\n",
    "                running_loss += loss.item() * inputs.size(0)\n",
    "                running_corrects += torch.sum(preds == labels.data)\n",
    "                \n",
    "                  \n",
    "\n",
    "              epoch_loss = running_loss / dataset_sizes[phase]\n",
    "              epoch_acc = running_corrects.double() / dataset_sizes[phase]\n",
    "            \n",
    "\n",
    "              print('{} Loss: {:.4f} Acc: {:.4f} '.format(\n",
    "                  phase, epoch_loss, epoch_acc))\n",
    "\n",
    "              # deep copy the model\n",
    "              if phase == 'val' and epoch_acc > best_acc:\n",
    "                  best_acc = epoch_acc\n",
    "                  best_model_wts = copy.deepcopy(model.state_dict())\n",
    "              if phase=='train':\n",
    "                losses.append(epoch_loss)\n",
    "                accuracies.append(epoch_acc)\n",
    "\n",
    "\n",
    "          \n",
    "\n",
    "      time_elapsed = time.time() - since\n",
    "      print('Training complete in {:.0f}m {:.0f}s'.format(\n",
    "          time_elapsed // 60, time_elapsed % 60))\n",
    "      print('Best val Acc: {:4f}'.format(best_acc))\n",
    "\n",
    "      # load best model weights\n",
    "      model.load_state_dict(best_model_wts)\n",
    "      return model,losses,accuracies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fitness(mask):\n",
    "        # finding the fitness of a particular mask\n",
    "        #accuracy of all training set is the fitness in one epoch\n",
    "        #putting model in train mode\n",
    "        mask=sigmoid_new(mask)\n",
    "        mask=torch.from_numpy(mask)\n",
    "        running_loss=0\n",
    "        model.train()\n",
    "        \n",
    "        for inputs,labels in dataloader ['train']:\n",
    "            inputs=inputs.to(device)\n",
    "            labels=labels.to(device )\n",
    "            outputs=model.Forward(inputs,mask,keep_prob(mask))\n",
    "            _,preds=torch.max(outputs,1)\n",
    "            loss=criterion (outputs,labels)\n",
    "            running_loss += loss.item() * inputs.size(0)\n",
    "        epoch_loss = running_loss / dataset_sizes['train']\n",
    "        return epoch_loss\n",
    "\n",
    "    \n",
    "    \n",
    "    \n",
    "def f(x):\n",
    "    #Higher-level method to do forward_prop in the whole swarm.\n",
    "    #Inputs:x: numpy.ndarray of shape (n_particles, dimensions) The swarm that will perform the search\n",
    "    #Returns numpy.ndarray of shape (n_particles, )  The computed loss for each particle\n",
    "    \n",
    "    \n",
    "    \n",
    "    j = [fitness(x[i,:]) for i in range(num_particles)]\n",
    "    return np.array(j)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def keep_prob (mask):\n",
    "\n",
    "        num_one =0\n",
    "        for i in range (maskLength):\n",
    "            if (mask[i]==1):\n",
    "                num_one=num_one+1\n",
    "        return float(num_one/maskLength)\n",
    "def sigmoid_new(x):\n",
    "    z = 1/(1 + np.exp(-x))\n",
    "    z=(z>0.5).astype('float32')\n",
    "    return z\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "options = {'c1': 2, 'c2': 2, 'w':0.4}\n",
    "num_particles=30\n",
    "maskLength=100\n",
    "torch.manual_seed(6)\n",
    "model=Model()\n",
    "model.to(device)\n",
    "criterion=nn.CrossEntropyLoss()\n",
    "optimizer=torch.optim.SGD(model.parameters(),lr=0.001,momentum=0.9)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-04-10 16:27:00,508 - pyswarms.single.global_best - INFO - Optimize for 50 iters with {'c1': 2, 'c2': 2, 'w': 0.4}\n",
      "pyswarms.single.global_best:   0%|                                                                                |0/50"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch generations ( 0 /200) :"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\ana\\envs\\packt_torch\\lib\\site-packages\\ipykernel_launcher.py:12: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  if sys.path[0] == '':\n",
      "pyswarms.single.global_best:  90%|████████████████████████████████████████████████████████▋      |45/50, best_cost=1.09D:\\ana\\envs\\packt_torch\\lib\\site-packages\\ipykernel_launcher.py:44: RuntimeWarning: overflow encountered in exp\n",
      "pyswarms.single.global_best: 100%|███████████████████████████████████████████████████████████████|50/50, best_cost=1.09\n",
      "2020-04-10 16:44:27,075 - pyswarms.single.global_best - INFO - Optimization finished | best cost: 1.0942221593856811, best pos: [-8.8512602e+00 -9.9848309e+00  9.1996640e-01 -3.3152934e-02\n",
      " -5.7579327e+00  3.2401814e+00  3.4489696e+00  2.4132452e+00\n",
      "  1.5718050e+00 -7.7564836e+00  7.9179635e+00 -1.0397106e+01\n",
      "  1.4418080e+00  1.3985373e+00  7.3433501e-01  1.1687936e+00\n",
      " -2.2276324e-01 -3.8587027e+00  1.0844530e-01  1.6425638e+01\n",
      " -1.3722972e+00 -7.3250351e+00  5.6475645e-01 -1.6236851e-02\n",
      "  8.6659628e-01 -1.9246157e+01  2.3345940e+00 -4.0091534e+01\n",
      " -3.0988367e+00  5.6126218e+00 -2.9425141e-01 -6.1855540e+00\n",
      " -3.8169250e-01 -1.0794755e+01 -7.5297409e-01 -2.3514283e+00\n",
      " -2.2510344e-01 -2.7697153e+00 -2.5739074e-01 -5.2904016e-01\n",
      " -6.1313663e-02 -1.7676136e+01  2.3813245e+00  9.5859480e-01\n",
      " -5.9353445e-02 -8.3810872e-01  1.9868783e+00  4.1235275e+00\n",
      " -7.8204966e-01 -1.3915303e+00  2.6680837e+00  2.4673092e+00\n",
      " -1.1377388e-01 -1.2862940e+00  1.1629868e+00 -3.0018151e-01\n",
      " -7.0090121e-01 -1.3225609e+00 -9.2113190e+00 -1.0468200e+00\n",
      " -1.1103721e+00 -5.5607003e-01 -1.5919901e+00  2.5965784e+00\n",
      "  3.2351413e-01  2.2678256e+00 -1.1887431e+01 -2.1960859e+00\n",
      "  1.7805963e+00  5.8107340e-01 -1.8068160e+00 -8.7865229e+00\n",
      " -1.4720622e+00 -1.7658547e-01  7.0134401e-01 -9.9285744e-02\n",
      " -5.5427933e+00 -3.9514217e+00 -4.1865044e+00 -2.9676194e+00\n",
      " -3.1498995e+00  2.8448980e+00 -5.2737362e+01  1.4309908e+00\n",
      "  1.7542602e+00  9.9745077e-01 -4.9445552e-01 -8.6187563e+00\n",
      "  1.7410858e+00  4.6923783e-01  7.0144820e-01 -3.2211115e+00\n",
      " -3.9010414e-01  1.3602587e+00 -4.8608251e+00 -3.5560763e-01\n",
      " -3.3719814e+01 -1.5118080e+01  3.2343419e+00  2.4892416e+00]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "keep prob 0.39\n",
      "Epoch 0/29\n",
      "----------\n",
      "train Loss: 1.0936 Acc: 0.3837 \n",
      "val Loss: 1.0986 Acc: 0.3285 \n",
      "Epoch 1/29\n",
      "----------\n",
      "train Loss: 1.0919 Acc: 0.3977 \n",
      "val Loss: 1.0978 Acc: 0.3475 \n",
      "Epoch 2/29\n",
      "----------\n",
      "train Loss: 1.0899 Acc: 0.4113 \n",
      "val Loss: 1.0970 Acc: 0.3585 \n",
      "Epoch 3/29\n",
      "----------\n",
      "train Loss: 1.0875 Acc: 0.4220 \n",
      "val Loss: 1.0960 Acc: 0.3820 \n",
      "Epoch 4/29\n",
      "----------\n",
      "train Loss: 1.0847 Acc: 0.4320 \n",
      "val Loss: 1.0949 Acc: 0.3975 \n",
      "Epoch 5/29\n",
      "----------\n",
      "train Loss: 1.0814 Acc: 0.4490 \n",
      "val Loss: 1.0935 Acc: 0.4165 \n",
      "Epoch 6/29\n",
      "----------\n",
      "train Loss: 1.0775 Acc: 0.4537 \n",
      "val Loss: 1.0919 Acc: 0.4315 \n",
      "Epoch 7/29\n",
      "----------\n",
      "train Loss: 1.0729 Acc: 0.4617 \n",
      "val Loss: 1.0899 Acc: 0.4440 \n",
      "Epoch 8/29\n",
      "----------\n",
      "train Loss: 1.0675 Acc: 0.4727 \n",
      "val Loss: 1.0876 Acc: 0.4590 \n",
      "Epoch 9/29\n",
      "----------\n",
      "train Loss: 1.0614 Acc: 0.4833 \n",
      "val Loss: 1.0849 Acc: 0.4750 \n",
      "Epoch 10/29\n",
      "----------\n",
      "train Loss: 1.0546 Acc: 0.4893 \n",
      "val Loss: 1.0819 Acc: 0.4815 \n",
      "Epoch 11/29\n",
      "----------\n",
      "train Loss: 1.0475 Acc: 0.4953 \n",
      "val Loss: 1.0787 Acc: 0.4955 \n",
      "Epoch 12/29\n",
      "----------\n",
      "train Loss: 1.0403 Acc: 0.5037 \n",
      "val Loss: 1.0752 Acc: 0.5075 \n",
      "Epoch 13/29\n",
      "----------\n",
      "train Loss: 1.0331 Acc: 0.5067 \n",
      "val Loss: 1.0717 Acc: 0.5180 \n",
      "Epoch 14/29\n",
      "----------\n",
      "train Loss: 1.0263 Acc: 0.5183 \n",
      "val Loss: 1.0681 Acc: 0.5225 \n",
      "Epoch 15/29\n",
      "----------\n",
      "train Loss: 1.0199 Acc: 0.5300 \n",
      "val Loss: 1.0646 Acc: 0.5340 \n",
      "Epoch 16/29\n",
      "----------\n",
      "train Loss: 1.0140 Acc: 0.5370 \n",
      "val Loss: 1.0612 Acc: 0.5410 \n",
      "Epoch 17/29\n",
      "----------\n",
      "train Loss: 1.0086 Acc: 0.5417 \n",
      "val Loss: 1.0579 Acc: 0.5415 \n",
      "Epoch 18/29\n",
      "----------\n",
      "train Loss: 1.0036 Acc: 0.5520 \n",
      "val Loss: 1.0548 Acc: 0.5475 \n",
      "Epoch 19/29\n",
      "----------\n",
      "train Loss: 0.9992 Acc: 0.5593 \n",
      "val Loss: 1.0518 Acc: 0.5515 \n",
      "Epoch 20/29\n",
      "----------\n",
      "train Loss: 0.9952 Acc: 0.5643 \n",
      "val Loss: 1.0489 Acc: 0.5535 \n",
      "Epoch 21/29\n",
      "----------\n",
      "train Loss: 0.9916 Acc: 0.5687 \n",
      "val Loss: 1.0462 Acc: 0.5545 \n",
      "Epoch 22/29\n",
      "----------\n",
      "train Loss: 0.9884 Acc: 0.5700 \n",
      "val Loss: 1.0436 Acc: 0.5545 \n",
      "Epoch 23/29\n",
      "----------\n",
      "train Loss: 0.9855 Acc: 0.5727 \n",
      "val Loss: 1.0412 Acc: 0.5580 \n",
      "Epoch 24/29\n",
      "----------\n",
      "train Loss: 0.9829 Acc: 0.5747 \n",
      "val Loss: 1.0389 Acc: 0.5595 \n",
      "Epoch 25/29\n",
      "----------\n",
      "train Loss: 0.9806 Acc: 0.5737 \n",
      "val Loss: 1.0368 Acc: 0.5605 \n",
      "Epoch 26/29\n",
      "----------\n",
      "train Loss: 0.9785 Acc: 0.5743 \n",
      "val Loss: 1.0348 Acc: 0.5640 \n",
      "Epoch 27/29\n",
      "----------\n",
      "train Loss: 0.9766 Acc: 0.5757 \n",
      "val Loss: 1.0329 Acc: 0.5635 \n",
      "Epoch 28/29\n",
      "----------\n",
      "train Loss: 0.9749 Acc: 0.5753 \n",
      "val Loss: 1.0311 Acc: 0.5655 \n",
      "Epoch 29/29\n",
      "----------\n",
      "train Loss: 0.9734 Acc: 0.5757 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-04-10 16:44:53,581 - pyswarms.single.global_best - INFO - Optimize for 50 iters with {'c1': 2, 'c2': 2, 'w': 0.4}\n",
      "pyswarms.single.global_best:   0%|                                                                                |0/50"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val Loss: 1.0294 Acc: 0.5660 \n",
      "Training complete in 0m 26s\n",
      "Best val Acc: 0.566000\n",
      "Epoch generations ( 1 /200) :"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "pyswarms.single.global_best: 100%|██████████████████████████████████████████████████████████████|50/50, best_cost=0.962\n",
      "2020-04-10 17:01:13,066 - pyswarms.single.global_best - INFO - Optimization finished | best cost: 0.9622716280619303, best pos: [-5.7520971e+00 -2.8002111e+01  1.4827515e+00 -2.2970937e-01\n",
      " -7.0231996e+00  3.5561645e+00  3.3528976e+00  2.5580509e+00\n",
      "  1.5895739e+00 -1.2485556e+01 -5.5401436e+01 -7.8442283e+00\n",
      "  1.5176837e+00  9.2717916e-01 -4.5948420e+00 -7.0119171e+00\n",
      " -5.1339287e-01 -3.0512947e+01  4.8847619e-02 -9.3315102e+01\n",
      " -2.7660565e+00 -9.4141884e+00 -4.2333369e+00 -3.5399538e-02\n",
      "  7.5447279e-01 -1.5261070e+01 -9.1751261e+00 -5.3982738e+01\n",
      " -3.2034897e+01  1.1796020e+01 -3.9640531e-02 -6.4293816e+01\n",
      " -2.9626063e-01 -6.0058815e+01 -5.8295469e+00 -2.7451556e+00\n",
      " -3.3000380e-01 -2.9326141e+00 -1.4910896e-01 -5.4241580e-01\n",
      " -8.5742973e-02 -1.7807199e+00 -3.3269753e+00  8.9677181e+00\n",
      " -9.3709284e-01 -1.4757364e+00 -1.1448621e+01  2.3909483e+00\n",
      " -5.5478436e-01 -6.4767885e-01  2.6099916e+00  1.7924045e+00\n",
      " -2.8225258e-01 -8.7737906e-01  1.2911762e+00 -1.0363889e+00\n",
      " -8.5618395e-01 -9.6999054e+00 -1.7309017e+00 -3.3023984e+00\n",
      " -1.2057939e+00 -7.0486552e-01 -1.3278629e+00 -4.0688801e+00\n",
      "  8.2416958e-01 -2.2548016e+01 -4.7161255e+00 -1.4500324e+00\n",
      "  4.6947541e+00 -2.7699776e+01 -1.9815422e+00 -6.0400724e+00\n",
      " -4.0835128e+00 -8.7324142e+00 -5.4877706e+00 -1.9947496e-01\n",
      " -4.0173111e+01 -7.3956454e-01 -2.6353712e+01 -6.5417571e+00\n",
      " -1.0749293e+01  3.4682288e+00 -4.7297003e+02  1.3946421e+00\n",
      "  1.0015240e+00 -4.9060768e-01 -6.9163349e-03 -1.6165636e+01\n",
      "  1.9563631e+00  6.6560364e-01 -1.6714800e-02 -3.4878454e+00\n",
      " -9.1549206e+00 -3.1876007e-01 -1.3807087e+01 -3.1248382e-01\n",
      " -9.2858200e+01 -8.0179253e+01  6.4496756e+00 -1.1270519e+01]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "keep prob 0.23\n",
      "Epoch 0/29\n",
      "----------\n",
      "train Loss: 0.9625 Acc: 0.5743 \n",
      "val Loss: 1.0299 Acc: 0.5600 \n",
      "Epoch 1/29\n",
      "----------\n",
      "train Loss: 0.9616 Acc: 0.5760 \n",
      "val Loss: 1.0294 Acc: 0.5630 \n",
      "Epoch 2/29\n",
      "----------\n",
      "train Loss: 0.9608 Acc: 0.5767 \n",
      "val Loss: 1.0290 Acc: 0.5625 \n",
      "Epoch 3/29\n",
      "----------\n",
      "train Loss: 0.9601 Acc: 0.5780 \n",
      "val Loss: 1.0285 Acc: 0.5610 \n",
      "Epoch 4/29\n",
      "----------\n",
      "train Loss: 0.9594 Acc: 0.5787 \n",
      "val Loss: 1.0281 Acc: 0.5625 \n",
      "Epoch 5/29\n",
      "----------\n",
      "train Loss: 0.9588 Acc: 0.5787 \n",
      "val Loss: 1.0276 Acc: 0.5620 \n",
      "Epoch 6/29\n",
      "----------\n",
      "train Loss: 0.9582 Acc: 0.5797 \n",
      "val Loss: 1.0272 Acc: 0.5615 \n",
      "Epoch 7/29\n",
      "----------\n",
      "train Loss: 0.9577 Acc: 0.5813 \n",
      "val Loss: 1.0267 Acc: 0.5600 \n",
      "Epoch 8/29\n",
      "----------\n",
      "train Loss: 0.9573 Acc: 0.5820 \n",
      "val Loss: 1.0263 Acc: 0.5605 \n",
      "Epoch 9/29\n",
      "----------\n",
      "train Loss: 0.9568 Acc: 0.5820 \n",
      "val Loss: 1.0258 Acc: 0.5605 \n",
      "Epoch 10/29\n",
      "----------\n",
      "train Loss: 0.9564 Acc: 0.5817 \n",
      "val Loss: 1.0254 Acc: 0.5620 \n",
      "Epoch 11/29\n",
      "----------\n",
      "train Loss: 0.9560 Acc: 0.5817 \n",
      "val Loss: 1.0250 Acc: 0.5610 \n",
      "Epoch 12/29\n",
      "----------\n",
      "train Loss: 0.9557 Acc: 0.5820 \n",
      "val Loss: 1.0246 Acc: 0.5615 \n",
      "Epoch 13/29\n",
      "----------\n",
      "train Loss: 0.9553 Acc: 0.5823 \n",
      "val Loss: 1.0242 Acc: 0.5615 \n",
      "Epoch 14/29\n",
      "----------\n",
      "train Loss: 0.9549 Acc: 0.5823 \n",
      "val Loss: 1.0238 Acc: 0.5605 \n",
      "Epoch 15/29\n",
      "----------\n",
      "train Loss: 0.9546 Acc: 0.5830 \n",
      "val Loss: 1.0234 Acc: 0.5585 \n",
      "Epoch 16/29\n",
      "----------\n",
      "train Loss: 0.9543 Acc: 0.5830 \n",
      "val Loss: 1.0230 Acc: 0.5585 \n",
      "Epoch 17/29\n",
      "----------\n",
      "train Loss: 0.9540 Acc: 0.5837 \n",
      "val Loss: 1.0227 Acc: 0.5580 \n",
      "Epoch 18/29\n",
      "----------\n",
      "train Loss: 0.9537 Acc: 0.5843 \n",
      "val Loss: 1.0223 Acc: 0.5580 \n",
      "Epoch 19/29\n",
      "----------\n",
      "train Loss: 0.9534 Acc: 0.5850 \n",
      "val Loss: 1.0220 Acc: 0.5585 \n",
      "Epoch 20/29\n",
      "----------\n",
      "train Loss: 0.9531 Acc: 0.5850 \n",
      "val Loss: 1.0216 Acc: 0.5580 \n",
      "Epoch 21/29\n",
      "----------\n",
      "train Loss: 0.9528 Acc: 0.5853 \n",
      "val Loss: 1.0213 Acc: 0.5580 \n",
      "Epoch 22/29\n",
      "----------\n",
      "train Loss: 0.9525 Acc: 0.5857 \n",
      "val Loss: 1.0210 Acc: 0.5590 \n",
      "Epoch 23/29\n",
      "----------\n",
      "train Loss: 0.9523 Acc: 0.5863 \n",
      "val Loss: 1.0207 Acc: 0.5595 \n",
      "Epoch 24/29\n",
      "----------\n",
      "train Loss: 0.9520 Acc: 0.5863 \n",
      "val Loss: 1.0203 Acc: 0.5595 \n",
      "Epoch 25/29\n",
      "----------\n",
      "train Loss: 0.9517 Acc: 0.5873 \n",
      "val Loss: 1.0200 Acc: 0.5600 \n",
      "Epoch 26/29\n",
      "----------\n",
      "train Loss: 0.9514 Acc: 0.5887 \n",
      "val Loss: 1.0198 Acc: 0.5600 \n",
      "Epoch 27/29\n",
      "----------\n",
      "train Loss: 0.9512 Acc: 0.5883 \n",
      "val Loss: 1.0195 Acc: 0.5605 \n",
      "Epoch 28/29\n",
      "----------\n",
      "train Loss: 0.9509 Acc: 0.5883 \n",
      "val Loss: 1.0192 Acc: 0.5615 \n",
      "Epoch 29/29\n",
      "----------\n",
      "train Loss: 0.9507 Acc: 0.5893 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-04-10 17:01:38,198 - pyswarms.single.global_best - INFO - Optimize for 50 iters with {'c1': 2, 'c2': 2, 'w': 0.4}\n",
      "pyswarms.single.global_best:   0%|                                                                                |0/50"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val Loss: 1.0189 Acc: 0.5615 \n",
      "Training complete in 0m 25s\n",
      "Best val Acc: 0.563000\n",
      "Epoch generations ( 2 /200) :"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "pyswarms.single.global_best: 100%|██████████████████████████████████████████████████████████████|50/50, best_cost=0.959\n",
      "2020-04-10 17:17:25,424 - pyswarms.single.global_best - INFO - Optimization finished | best cost: 0.9588024584452312, best pos: [-3.1395800e+00 -3.2468010e+01 -1.9562253e-01 -7.7760935e-02\n",
      " -7.5192132e+00  3.5372279e+00  3.3579428e+00  2.5527413e+00\n",
      "  2.1067569e+00 -1.3633256e+01 -7.8265526e+01 -8.4526024e+00\n",
      "  1.5131575e+00  9.3678850e-01 -2.2964866e+00 -1.0432782e+01\n",
      " -5.2625465e-01 -2.8208824e+01  7.1470298e-02 -1.0699910e+02\n",
      " -3.7519257e+00 -1.0460265e+01 -3.3917971e+00 -4.1150067e-02\n",
      "  1.6014834e-01 -6.3894424e+00 -1.1305914e+01 -2.7225931e+02\n",
      " -2.3598717e+01  8.5475426e+00 -4.2752687e-02 -7.0768181e+01\n",
      " -3.3584812e-01 -2.9174011e+02 -5.9025879e+00 -2.3884561e+00\n",
      " -3.4663761e-01 -2.9744692e+00 -1.5307584e-01 -7.6345611e-01\n",
      " -9.1545396e-02 -7.8312421e+00 -4.2260180e+00 -1.1497127e+01\n",
      " -8.9831841e-01 -1.4186130e+00 -1.5126986e+01  2.5427637e+00\n",
      " -5.5118763e-01 -7.8039527e-01  1.6556408e+00  1.6325716e+00\n",
      " -2.4837059e-01 -9.2241919e-01  3.3447752e+00 -1.0088078e+00\n",
      " -7.0964956e-01 -9.0028105e+00 -1.9514176e+00 -7.1644688e+00\n",
      " -1.1859407e+00 -6.6378254e-01 -2.4970791e-01 -9.5475569e+00\n",
      "  8.0260992e-01 -7.5076408e+00 -4.7383981e+00 -2.1971796e+00\n",
      "  4.9379340e+02 -3.1267048e+01 -1.8568093e+00 -6.0275984e+00\n",
      " -1.4703274e+00 -1.0408343e+01 -9.4804602e+00 -2.1938857e-01\n",
      " -1.2044388e+02 -7.8609341e-01 -2.6574036e+01 -6.3960738e+00\n",
      " -7.7150693e+00  3.6594543e+00 -4.5290387e+02  9.8842776e-01\n",
      "  7.6144123e-01 -4.4140086e-01 -3.5439122e-02 -1.9464609e+01\n",
      "  2.0140541e+00  8.2376558e-01 -5.3848147e-01 -3.5417140e+00\n",
      " -1.0031466e+01 -3.2855240e-01 -1.7840166e+01 -3.0343547e-01\n",
      " -9.4015114e+01 -1.0214985e+02 -4.3887157e+00 -7.1790514e+00]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "keep prob 0.2\n",
      "Epoch 0/29\n",
      "----------\n",
      "train Loss: 0.9597 Acc: 0.5767 \n",
      "val Loss: 1.0293 Acc: 0.5630 \n",
      "Epoch 1/29\n",
      "----------\n",
      "train Loss: 0.9588 Acc: 0.5780 \n",
      "val Loss: 1.0292 Acc: 0.5610 \n",
      "Epoch 2/29\n",
      "----------\n",
      "train Loss: 0.9581 Acc: 0.5793 \n",
      "val Loss: 1.0289 Acc: 0.5630 \n",
      "Epoch 3/29\n",
      "----------\n",
      "train Loss: 0.9574 Acc: 0.5800 \n",
      "val Loss: 1.0287 Acc: 0.5605 \n",
      "Epoch 4/29\n",
      "----------\n",
      "train Loss: 0.9569 Acc: 0.5813 \n",
      "val Loss: 1.0285 Acc: 0.5595 \n",
      "Epoch 5/29\n",
      "----------\n",
      "train Loss: 0.9563 Acc: 0.5827 \n",
      "val Loss: 1.0283 Acc: 0.5590 \n",
      "Epoch 6/29\n",
      "----------\n",
      "train Loss: 0.9559 Acc: 0.5827 \n",
      "val Loss: 1.0280 Acc: 0.5570 \n",
      "Epoch 7/29\n",
      "----------\n",
      "train Loss: 0.9554 Acc: 0.5817 \n",
      "val Loss: 1.0277 Acc: 0.5560 \n",
      "Epoch 8/29\n",
      "----------\n",
      "train Loss: 0.9550 Acc: 0.5823 \n",
      "val Loss: 1.0274 Acc: 0.5570 \n",
      "Epoch 9/29\n",
      "----------\n",
      "train Loss: 0.9547 Acc: 0.5827 \n",
      "val Loss: 1.0272 Acc: 0.5575 \n",
      "Epoch 10/29\n",
      "----------\n",
      "train Loss: 0.9543 Acc: 0.5830 \n",
      "val Loss: 1.0269 Acc: 0.5595 \n",
      "Epoch 11/29\n",
      "----------\n",
      "train Loss: 0.9539 Acc: 0.5823 \n",
      "val Loss: 1.0266 Acc: 0.5590 \n",
      "Epoch 12/29\n",
      "----------\n",
      "train Loss: 0.9536 Acc: 0.5830 \n",
      "val Loss: 1.0263 Acc: 0.5575 \n",
      "Epoch 13/29\n",
      "----------\n",
      "train Loss: 0.9533 Acc: 0.5840 \n",
      "val Loss: 1.0260 Acc: 0.5570 \n",
      "Epoch 14/29\n",
      "----------\n",
      "train Loss: 0.9529 Acc: 0.5850 \n",
      "val Loss: 1.0258 Acc: 0.5575 \n",
      "Epoch 15/29\n",
      "----------\n",
      "train Loss: 0.9526 Acc: 0.5850 \n",
      "val Loss: 1.0255 Acc: 0.5580 \n",
      "Epoch 16/29\n",
      "----------\n",
      "train Loss: 0.9523 Acc: 0.5857 \n",
      "val Loss: 1.0252 Acc: 0.5580 \n",
      "Epoch 17/29\n",
      "----------\n",
      "train Loss: 0.9520 Acc: 0.5863 \n",
      "val Loss: 1.0250 Acc: 0.5580 \n",
      "Epoch 18/29\n",
      "----------\n",
      "train Loss: 0.9517 Acc: 0.5863 \n",
      "val Loss: 1.0247 Acc: 0.5580 \n",
      "Epoch 19/29\n",
      "----------\n",
      "train Loss: 0.9514 Acc: 0.5883 \n",
      "val Loss: 1.0244 Acc: 0.5575 \n",
      "Epoch 20/29\n",
      "----------\n",
      "train Loss: 0.9511 Acc: 0.5887 \n",
      "val Loss: 1.0242 Acc: 0.5575 \n",
      "Epoch 21/29\n",
      "----------\n",
      "train Loss: 0.9508 Acc: 0.5900 \n",
      "val Loss: 1.0239 Acc: 0.5585 \n",
      "Epoch 22/29\n",
      "----------\n",
      "train Loss: 0.9506 Acc: 0.5893 \n",
      "val Loss: 1.0237 Acc: 0.5585 \n",
      "Epoch 23/29\n",
      "----------\n",
      "train Loss: 0.9503 Acc: 0.5903 \n",
      "val Loss: 1.0234 Acc: 0.5585 \n",
      "Epoch 24/29\n",
      "----------\n",
      "train Loss: 0.9500 Acc: 0.5903 \n",
      "val Loss: 1.0232 Acc: 0.5590 \n",
      "Epoch 25/29\n",
      "----------\n",
      "train Loss: 0.9498 Acc: 0.5903 \n",
      "val Loss: 1.0229 Acc: 0.5600 \n",
      "Epoch 26/29\n",
      "----------\n",
      "train Loss: 0.9495 Acc: 0.5917 \n",
      "val Loss: 1.0227 Acc: 0.5605 \n",
      "Epoch 27/29\n",
      "----------\n",
      "train Loss: 0.9493 Acc: 0.5923 \n",
      "val Loss: 1.0224 Acc: 0.5600 \n",
      "Epoch 28/29\n",
      "----------\n",
      "train Loss: 0.9491 Acc: 0.5930 \n",
      "val Loss: 1.0222 Acc: 0.5605 \n",
      "Epoch 29/29\n",
      "----------\n",
      "train Loss: 0.9488 Acc: 0.5937 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-04-10 17:17:51,332 - pyswarms.single.global_best - INFO - Optimize for 50 iters with {'c1': 2, 'c2': 2, 'w': 0.4}\n",
      "pyswarms.single.global_best:   0%|                                                                                |0/50"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val Loss: 1.0220 Acc: 0.5615 \n",
      "Training complete in 0m 26s\n",
      "Best val Acc: 0.563000\n",
      "Epoch generations ( 3 /200) :"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "pyswarms.single.global_best: 100%|██████████████████████████████████████████████████████████████|50/50, best_cost=0.958\n",
      "2020-04-10 17:33:46,893 - pyswarms.single.global_best - INFO - Optimization finished | best cost: 0.9577839903831482, best pos: [-1.68796611e+00 -3.34625473e+01 -1.31689525e+00 -1.82968900e-01\n",
      " -7.91355705e+00  3.50670505e+00  3.33542562e+00  2.55462480e+00\n",
      "  1.79712772e+00 -1.33780899e+01 -9.02783813e+01 -9.60994816e+00\n",
      "  1.51030898e+00  9.38398719e-01 -4.07122993e+00 -1.30421476e+01\n",
      " -5.39348304e-01 -3.59654358e+02  1.08937986e-01 -9.58488846e+01\n",
      " -1.89808917e+00 -1.08478880e+01 -3.20329976e+00 -2.34911889e-02\n",
      "  1.75409353e+00 -2.04805260e+01 -1.27626572e+01 -1.09620781e+02\n",
      " -3.16312199e+01  2.17847862e+01 -3.85313034e-02 -6.97997589e+01\n",
      " -3.32445234e-01 -3.32099243e+02 -6.23216200e+00 -2.32736158e+00\n",
      " -2.92828977e-01 -4.49365139e+00 -1.48909613e-01 -5.40355027e-01\n",
      " -8.05517286e-02 -2.40026608e+01 -3.60395670e+00 -3.55044746e+01\n",
      " -9.31736171e-01 -1.02486420e+00 -1.42052679e+01  2.50594592e+00\n",
      " -5.52681684e-01 -7.51007318e-01  1.85264158e+00  1.76430094e+00\n",
      " -2.30480090e-01 -9.23265517e-01  4.31105137e+00 -7.25711703e-01\n",
      " -7.21981406e-01 -2.24264126e+01 -1.98466110e+00 -3.41010666e+00\n",
      " -1.30408144e+00 -6.47986889e-01 -3.86633188e-01 -6.51946783e+00\n",
      "  7.73087978e-01 -4.44727402e+01 -4.87425184e+00 -2.30016088e+00\n",
      "  1.07976933e+01 -3.22806549e+01 -2.02343178e+00 -5.98805666e+00\n",
      " -4.16145515e+00 -1.01225767e+01 -6.21142340e+00 -2.09030718e-01\n",
      " -8.32264633e+01 -1.82649074e-03 -3.02465801e+01 -6.63167715e+00\n",
      " -1.21754856e+01  4.20377541e+00 -7.66716919e+01  1.16801775e+00\n",
      " -1.67111385e+00 -6.95723116e-01 -1.22367311e-02 -2.72063618e+01\n",
      "  1.94401777e+00  7.99150467e-01 -3.63598943e-01 -3.56952620e+00\n",
      " -1.02586842e+01 -1.39945686e+00 -1.80602741e+01 -3.03234518e-01\n",
      " -9.10367966e+01 -1.37975683e+01 -5.12104654e+00 -3.22222590e+00]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "keep prob 0.19\n",
      "Epoch 0/29\n",
      "----------\n",
      "train Loss: 0.9588 Acc: 0.5767 \n",
      "val Loss: 1.0297 Acc: 0.5590 \n",
      "Epoch 1/29\n",
      "----------\n",
      "train Loss: 0.9580 Acc: 0.5800 \n",
      "val Loss: 1.0296 Acc: 0.5585 \n",
      "Epoch 2/29\n",
      "----------\n",
      "train Loss: 0.9573 Acc: 0.5810 \n",
      "val Loss: 1.0295 Acc: 0.5575 \n",
      "Epoch 3/29\n",
      "----------\n",
      "train Loss: 0.9567 Acc: 0.5813 \n",
      "val Loss: 1.0293 Acc: 0.5560 \n",
      "Epoch 4/29\n",
      "----------\n",
      "train Loss: 0.9561 Acc: 0.5833 \n",
      "val Loss: 1.0291 Acc: 0.5550 \n",
      "Epoch 5/29\n",
      "----------\n",
      "train Loss: 0.9556 Acc: 0.5833 \n",
      "val Loss: 1.0289 Acc: 0.5540 \n",
      "Epoch 6/29\n",
      "----------\n",
      "train Loss: 0.9552 Acc: 0.5820 \n",
      "val Loss: 1.0287 Acc: 0.5530 \n",
      "Epoch 7/29\n",
      "----------\n",
      "train Loss: 0.9547 Acc: 0.5820 \n",
      "val Loss: 1.0285 Acc: 0.5535 \n",
      "Epoch 8/29\n",
      "----------\n",
      "train Loss: 0.9543 Acc: 0.5817 \n",
      "val Loss: 1.0282 Acc: 0.5545 \n",
      "Epoch 9/29\n",
      "----------\n",
      "train Loss: 0.9540 Acc: 0.5827 \n",
      "val Loss: 1.0280 Acc: 0.5530 \n",
      "Epoch 10/29\n",
      "----------\n",
      "train Loss: 0.9536 Acc: 0.5827 \n",
      "val Loss: 1.0278 Acc: 0.5535 \n",
      "Epoch 11/29\n",
      "----------\n",
      "train Loss: 0.9533 Acc: 0.5823 \n",
      "val Loss: 1.0275 Acc: 0.5525 \n",
      "Epoch 12/29\n",
      "----------\n",
      "train Loss: 0.9529 Acc: 0.5820 \n",
      "val Loss: 1.0273 Acc: 0.5525 \n",
      "Epoch 13/29\n",
      "----------\n",
      "train Loss: 0.9526 Acc: 0.5830 \n",
      "val Loss: 1.0270 Acc: 0.5535 \n",
      "Epoch 14/29\n",
      "----------\n",
      "train Loss: 0.9523 Acc: 0.5843 \n",
      "val Loss: 1.0268 Acc: 0.5520 \n",
      "Epoch 15/29\n",
      "----------\n",
      "train Loss: 0.9519 Acc: 0.5850 \n",
      "val Loss: 1.0265 Acc: 0.5510 \n",
      "Epoch 16/29\n",
      "----------\n",
      "train Loss: 0.9516 Acc: 0.5853 \n",
      "val Loss: 1.0263 Acc: 0.5505 \n",
      "Epoch 17/29\n",
      "----------\n",
      "train Loss: 0.9513 Acc: 0.5870 \n",
      "val Loss: 1.0261 Acc: 0.5505 \n",
      "Epoch 18/29\n",
      "----------\n",
      "train Loss: 0.9510 Acc: 0.5890 \n",
      "val Loss: 1.0258 Acc: 0.5510 \n",
      "Epoch 19/29\n",
      "----------\n",
      "train Loss: 0.9507 Acc: 0.5893 \n",
      "val Loss: 1.0256 Acc: 0.5510 \n",
      "Epoch 20/29\n",
      "----------\n",
      "train Loss: 0.9504 Acc: 0.5900 \n",
      "val Loss: 1.0253 Acc: 0.5515 \n",
      "Epoch 21/29\n",
      "----------\n",
      "train Loss: 0.9501 Acc: 0.5913 \n",
      "val Loss: 1.0251 Acc: 0.5515 \n",
      "Epoch 22/29\n",
      "----------\n",
      "train Loss: 0.9498 Acc: 0.5920 \n",
      "val Loss: 1.0248 Acc: 0.5510 \n",
      "Epoch 23/29\n",
      "----------\n",
      "train Loss: 0.9496 Acc: 0.5933 \n",
      "val Loss: 1.0246 Acc: 0.5515 \n",
      "Epoch 24/29\n",
      "----------\n",
      "train Loss: 0.9493 Acc: 0.5930 \n",
      "val Loss: 1.0244 Acc: 0.5525 \n",
      "Epoch 25/29\n",
      "----------\n",
      "train Loss: 0.9491 Acc: 0.5937 \n",
      "val Loss: 1.0241 Acc: 0.5535 \n",
      "Epoch 26/29\n",
      "----------\n",
      "train Loss: 0.9488 Acc: 0.5940 \n",
      "val Loss: 1.0239 Acc: 0.5535 \n",
      "Epoch 27/29\n",
      "----------\n",
      "train Loss: 0.9485 Acc: 0.5943 \n",
      "val Loss: 1.0237 Acc: 0.5545 \n",
      "Epoch 28/29\n",
      "----------\n",
      "train Loss: 0.9483 Acc: 0.5950 \n",
      "val Loss: 1.0234 Acc: 0.5555 \n",
      "Epoch 29/29\n",
      "----------\n",
      "train Loss: 0.9480 Acc: 0.5950 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-04-10 17:34:12,712 - pyswarms.single.global_best - INFO - Optimize for 50 iters with {'c1': 2, 'c2': 2, 'w': 0.4}\n",
      "pyswarms.single.global_best:   0%|                                                                                |0/50"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val Loss: 1.0232 Acc: 0.5550 \n",
      "Training complete in 0m 26s\n",
      "Best val Acc: 0.559000\n",
      "Epoch generations ( 4 /200) :"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "pyswarms.single.global_best: 100%|██████████████████████████████████████████████████████████████|50/50, best_cost=0.957\n",
      "2020-04-10 17:49:12,472 - pyswarms.single.global_best - INFO - Optimization finished | best cost: 0.957021900177002, best pos: [-2.13213038e+00 -3.51578331e+01 -1.95721471e+00 -3.09665799e-01\n",
      " -7.71389914e+00  3.51545715e+00  3.33792114e+00  2.55492926e+00\n",
      "  1.82147682e+00 -1.33757048e+01 -8.82764206e+01 -9.43575573e+00\n",
      "  1.51746142e+00  9.39273655e-01 -9.95839214e+00 -1.31123695e+01\n",
      " -5.35259962e-01 -2.10932449e+02  1.07021779e-01 -1.00068611e+02\n",
      " -1.49130177e+00 -1.05111332e+01 -3.09216714e+00 -1.95786785e-02\n",
      "  3.47180367e+00 -2.05442410e+01 -1.17791452e+01 -6.17329292e+01\n",
      " -2.92996120e+01  5.66720629e+00 -4.16204669e-02 -6.82390442e+01\n",
      " -3.33825260e-01 -1.21133887e+03 -6.20859909e+00 -2.52635980e+00\n",
      " -3.36674422e-01 -4.31899118e+00 -1.48933157e-01 -5.42010546e-01\n",
      " -8.58713984e-02 -5.31625710e+01 -3.52611184e+00 -3.71473541e+01\n",
      " -9.28924680e-01 -1.04144204e+00 -1.48800602e+01  2.43930936e+00\n",
      " -5.52578330e-01 -7.50518739e-01  1.79630542e+00  1.79769063e+00\n",
      " -2.40065560e-01 -9.28538859e-01  4.02670002e+00 -8.11080992e-01\n",
      " -7.33834565e-01 -1.18099575e+01 -1.71960850e+01 -3.40291667e+00\n",
      " -1.35859215e+00 -1.46913302e+00 -2.73112327e-01 -6.41802502e+00\n",
      "  8.24436724e-01 -4.56271896e+01 -4.84783554e+00 -2.33702445e+00\n",
      "  8.84206676e+00 -3.39011993e+01 -2.02330613e+00 -6.00741673e+00\n",
      " -4.22692680e+00 -1.01440783e+01 -5.30215073e+00 -2.05201343e-01\n",
      " -8.58545990e+01 -7.65135810e-02 -2.83265057e+01 -6.60972214e+00\n",
      " -1.22034292e+01  4.52957821e+00 -1.56056503e+02  1.17809403e+00\n",
      " -7.97973752e-01 -4.23443288e-01 -8.33190791e-03 -2.63014736e+01\n",
      "  1.94594920e+00  7.93608367e-01 -3.59847814e-01 -3.57233644e+00\n",
      " -1.01782703e+01 -5.46538949e-01 -4.08007050e+00 -3.03576738e-01\n",
      " -8.72782822e+01 -1.80893784e+01 -3.44476414e+00 -3.34968233e+00]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "keep prob 0.19\n",
      "Epoch 0/29\n",
      "----------\n",
      "train Loss: 0.9580 Acc: 0.5793 \n",
      "val Loss: 1.0296 Acc: 0.5580 \n",
      "Epoch 1/29\n",
      "----------\n",
      "train Loss: 0.9573 Acc: 0.5803 \n",
      "val Loss: 1.0295 Acc: 0.5575 \n",
      "Epoch 2/29\n",
      "----------\n",
      "train Loss: 0.9567 Acc: 0.5813 \n",
      "val Loss: 1.0293 Acc: 0.5560 \n",
      "Epoch 3/29\n",
      "----------\n",
      "train Loss: 0.9561 Acc: 0.5830 \n",
      "val Loss: 1.0291 Acc: 0.5550 \n",
      "Epoch 4/29\n",
      "----------\n",
      "train Loss: 0.9556 Acc: 0.5830 \n",
      "val Loss: 1.0289 Acc: 0.5535 \n",
      "Epoch 5/29\n",
      "----------\n",
      "train Loss: 0.9552 Acc: 0.5820 \n",
      "val Loss: 1.0287 Acc: 0.5530 \n",
      "Epoch 6/29\n",
      "----------\n",
      "train Loss: 0.9548 Acc: 0.5827 \n",
      "val Loss: 1.0285 Acc: 0.5535 \n",
      "Epoch 7/29\n",
      "----------\n",
      "train Loss: 0.9544 Acc: 0.5813 \n",
      "val Loss: 1.0282 Acc: 0.5545 \n",
      "Epoch 8/29\n",
      "----------\n",
      "train Loss: 0.9540 Acc: 0.5827 \n",
      "val Loss: 1.0280 Acc: 0.5530 \n",
      "Epoch 9/29\n",
      "----------\n",
      "train Loss: 0.9536 Acc: 0.5823 \n",
      "val Loss: 1.0277 Acc: 0.5525 \n",
      "Epoch 10/29\n",
      "----------\n",
      "train Loss: 0.9533 Acc: 0.5823 \n",
      "val Loss: 1.0275 Acc: 0.5525 \n",
      "Epoch 11/29\n",
      "----------\n",
      "train Loss: 0.9529 Acc: 0.5823 \n",
      "val Loss: 1.0273 Acc: 0.5525 \n",
      "Epoch 12/29\n",
      "----------\n",
      "train Loss: 0.9526 Acc: 0.5827 \n",
      "val Loss: 1.0270 Acc: 0.5530 \n",
      "Epoch 13/29\n",
      "----------\n",
      "train Loss: 0.9523 Acc: 0.5843 \n",
      "val Loss: 1.0268 Acc: 0.5520 \n",
      "Epoch 14/29\n",
      "----------\n",
      "train Loss: 0.9519 Acc: 0.5850 \n",
      "val Loss: 1.0265 Acc: 0.5510 \n",
      "Epoch 15/29\n",
      "----------\n",
      "train Loss: 0.9516 Acc: 0.5853 \n",
      "val Loss: 1.0263 Acc: 0.5505 \n",
      "Epoch 16/29\n",
      "----------\n",
      "train Loss: 0.9513 Acc: 0.5870 \n",
      "val Loss: 1.0261 Acc: 0.5505 \n",
      "Epoch 17/29\n",
      "----------\n",
      "train Loss: 0.9510 Acc: 0.5890 \n",
      "val Loss: 1.0258 Acc: 0.5510 \n",
      "Epoch 18/29\n",
      "----------\n",
      "train Loss: 0.9507 Acc: 0.5893 \n",
      "val Loss: 1.0256 Acc: 0.5510 \n",
      "Epoch 19/29\n",
      "----------\n",
      "train Loss: 0.9504 Acc: 0.5900 \n",
      "val Loss: 1.0253 Acc: 0.5515 \n",
      "Epoch 20/29\n",
      "----------\n",
      "train Loss: 0.9501 Acc: 0.5913 \n",
      "val Loss: 1.0251 Acc: 0.5510 \n",
      "Epoch 21/29\n",
      "----------\n",
      "train Loss: 0.9498 Acc: 0.5917 \n",
      "val Loss: 1.0249 Acc: 0.5510 \n",
      "Epoch 22/29\n",
      "----------\n",
      "train Loss: 0.9496 Acc: 0.5933 \n",
      "val Loss: 1.0246 Acc: 0.5520 \n",
      "Epoch 23/29\n",
      "----------\n",
      "train Loss: 0.9493 Acc: 0.5933 \n",
      "val Loss: 1.0243 Acc: 0.5525 \n",
      "Epoch 24/29\n",
      "----------\n",
      "train Loss: 0.9491 Acc: 0.5937 \n",
      "val Loss: 1.0241 Acc: 0.5535 \n",
      "Epoch 25/29\n",
      "----------\n",
      "train Loss: 0.9488 Acc: 0.5940 \n",
      "val Loss: 1.0239 Acc: 0.5535 \n",
      "Epoch 26/29\n",
      "----------\n",
      "train Loss: 0.9485 Acc: 0.5943 \n",
      "val Loss: 1.0237 Acc: 0.5545 \n",
      "Epoch 27/29\n",
      "----------\n",
      "train Loss: 0.9483 Acc: 0.5950 \n",
      "val Loss: 1.0234 Acc: 0.5550 \n",
      "Epoch 28/29\n",
      "----------\n",
      "train Loss: 0.9480 Acc: 0.5950 \n",
      "val Loss: 1.0232 Acc: 0.5550 \n",
      "Epoch 29/29\n",
      "----------\n",
      "train Loss: 0.9478 Acc: 0.5950 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-04-10 17:49:31,615 - pyswarms.single.global_best - INFO - Optimize for 50 iters with {'c1': 2, 'c2': 2, 'w': 0.4}\n",
      "pyswarms.single.global_best:   0%|                                                                                |0/50"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val Loss: 1.0230 Acc: 0.5555 \n",
      "Training complete in 0m 19s\n",
      "Best val Acc: 0.558000\n",
      "Epoch generations ( 5 /200) :"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "pyswarms.single.global_best: 100%|██████████████████████████████████████████████████████████████|50/50, best_cost=0.956\n",
      "2020-04-10 18:00:50,604 - pyswarms.single.global_best - INFO - Optimization finished | best cost: 0.9562754538853964, best pos: [-2.2019029e+00 -3.5360573e+01 -1.9213222e+00 -2.9737937e-01\n",
      " -7.7144065e+00  3.5147789e+00  3.3379710e+00  2.5549464e+00\n",
      "  1.7439076e+00 -1.3376075e+01 -9.0452332e+01 -9.4669352e+00\n",
      "  1.5155679e+00  9.3908262e-01 -1.0497993e+01 -1.3112148e+01\n",
      " -5.3418487e-01 -2.1406845e+02  1.1421968e-01 -9.6404434e+01\n",
      " -1.5120353e+00 -1.0514297e+01 -3.0936563e+00 -2.0656971e-02\n",
      "  5.9201818e+00 -2.0546545e+01 -1.1800880e+01 -6.0999973e+01\n",
      " -2.9332277e+01  2.3201470e+00 -4.0361233e-02 -6.8641266e+01\n",
      " -3.3372352e-01 -1.1781949e+03 -6.2170238e+00 -2.5269930e+00\n",
      " -3.3566758e-01 -4.2756610e+00 -1.4893176e-01 -5.4279947e-01\n",
      " -8.4715642e-02 -7.5608292e+01 -3.5275421e+00 -3.6346722e+01\n",
      " -8.7173057e-01 -1.0421133e+00 -1.5014142e+01  2.4399023e+00\n",
      " -5.5258292e-01 -7.5052267e-01  1.7953380e+00  1.8051212e+00\n",
      " -2.4231908e-01 -9.2849261e-01  3.9945548e+00 -8.0903518e-01\n",
      " -7.8005236e-01 -1.0060275e+01 -1.7223539e+01 -3.4777310e+00\n",
      " -1.2903007e+00 -1.4243319e+00 -2.6392612e-01 -6.4181046e+00\n",
      "  8.3886629e-01 -4.5300419e+01 -4.8481488e+00 -2.4733934e+00\n",
      "  9.3161364e+00 -3.4024197e+01 -2.0233085e+00 -5.9756651e+00\n",
      " -4.2194242e+00 -9.9726915e+00 -5.4558115e+00 -1.9718996e-01\n",
      " -8.5419983e+01 -6.4843126e-02 -2.8441620e+01 -6.6108799e+00\n",
      " -1.2200994e+01  4.5311790e+00 -1.4797034e+02  1.1788864e+00\n",
      " -1.0443276e+00 -4.9999556e-01 -9.2311660e-03 -2.6634972e+01\n",
      "  1.9456604e+00  7.9249567e-01 -3.6430177e-01 -3.5723066e+00\n",
      " -1.0175336e+01 -5.4405594e-01 -3.4850917e+00 -3.0357927e-01\n",
      " -8.4834824e+01 -1.8316919e+01 -3.4960611e+00 -3.4165380e+00]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "keep prob 0.19\n",
      "Epoch 0/29\n",
      "----------\n",
      "train Loss: 0.9573 Acc: 0.5803 \n",
      "val Loss: 1.0295 Acc: 0.5575 \n",
      "Epoch 1/29\n",
      "----------\n",
      "train Loss: 0.9567 Acc: 0.5813 \n",
      "val Loss: 1.0293 Acc: 0.5560 \n",
      "Epoch 2/29\n",
      "----------\n",
      "train Loss: 0.9561 Acc: 0.5830 \n",
      "val Loss: 1.0291 Acc: 0.5555 \n",
      "Epoch 3/29\n",
      "----------\n",
      "train Loss: 0.9556 Acc: 0.5830 \n",
      "val Loss: 1.0289 Acc: 0.5540 \n",
      "Epoch 4/29\n",
      "----------\n",
      "train Loss: 0.9552 Acc: 0.5820 \n",
      "val Loss: 1.0287 Acc: 0.5530 \n",
      "Epoch 5/29\n",
      "----------\n",
      "train Loss: 0.9548 Acc: 0.5827 \n",
      "val Loss: 1.0285 Acc: 0.5530 \n",
      "Epoch 6/29\n",
      "----------\n",
      "train Loss: 0.9544 Acc: 0.5813 \n",
      "val Loss: 1.0282 Acc: 0.5545 \n",
      "Epoch 7/29\n",
      "----------\n",
      "train Loss: 0.9540 Acc: 0.5827 \n",
      "val Loss: 1.0280 Acc: 0.5530 \n",
      "Epoch 8/29\n",
      "----------\n",
      "train Loss: 0.9536 Acc: 0.5823 \n",
      "val Loss: 1.0278 Acc: 0.5525 \n",
      "Epoch 9/29\n",
      "----------\n",
      "train Loss: 0.9533 Acc: 0.5823 \n",
      "val Loss: 1.0275 Acc: 0.5520 \n",
      "Epoch 10/29\n",
      "----------\n",
      "train Loss: 0.9529 Acc: 0.5827 \n",
      "val Loss: 1.0273 Acc: 0.5525 \n",
      "Epoch 11/29\n",
      "----------\n",
      "train Loss: 0.9526 Acc: 0.5827 \n",
      "val Loss: 1.0270 Acc: 0.5530 \n",
      "Epoch 12/29\n",
      "----------\n",
      "train Loss: 0.9523 Acc: 0.5840 \n",
      "val Loss: 1.0268 Acc: 0.5525 \n",
      "Epoch 13/29\n",
      "----------\n",
      "train Loss: 0.9519 Acc: 0.5850 \n",
      "val Loss: 1.0265 Acc: 0.5515 \n",
      "Epoch 14/29\n",
      "----------\n",
      "train Loss: 0.9516 Acc: 0.5853 \n",
      "val Loss: 1.0263 Acc: 0.5505 \n",
      "Epoch 15/29\n",
      "----------\n",
      "train Loss: 0.9513 Acc: 0.5863 \n",
      "val Loss: 1.0260 Acc: 0.5505 \n",
      "Epoch 16/29\n",
      "----------\n",
      "train Loss: 0.9510 Acc: 0.5890 \n",
      "val Loss: 1.0258 Acc: 0.5510 \n",
      "Epoch 17/29\n",
      "----------\n",
      "train Loss: 0.9507 Acc: 0.5893 \n",
      "val Loss: 1.0256 Acc: 0.5510 \n",
      "Epoch 18/29\n",
      "----------\n",
      "train Loss: 0.9504 Acc: 0.5903 \n",
      "val Loss: 1.0253 Acc: 0.5520 \n",
      "Epoch 19/29\n",
      "----------\n",
      "train Loss: 0.9501 Acc: 0.5913 \n",
      "val Loss: 1.0251 Acc: 0.5515 \n",
      "Epoch 20/29\n",
      "----------\n",
      "train Loss: 0.9499 Acc: 0.5917 \n",
      "val Loss: 1.0249 Acc: 0.5510 \n",
      "Epoch 21/29\n",
      "----------\n",
      "train Loss: 0.9496 Acc: 0.5933 \n",
      "val Loss: 1.0246 Acc: 0.5520 \n",
      "Epoch 22/29\n",
      "----------\n",
      "train Loss: 0.9493 Acc: 0.5930 \n",
      "val Loss: 1.0244 Acc: 0.5525 \n",
      "Epoch 23/29\n",
      "----------\n",
      "train Loss: 0.9491 Acc: 0.5937 \n",
      "val Loss: 1.0241 Acc: 0.5535 \n",
      "Epoch 24/29\n",
      "----------\n",
      "train Loss: 0.9488 Acc: 0.5940 \n",
      "val Loss: 1.0239 Acc: 0.5535 \n",
      "Epoch 25/29\n",
      "----------\n",
      "train Loss: 0.9485 Acc: 0.5943 \n",
      "val Loss: 1.0237 Acc: 0.5550 \n",
      "Epoch 26/29\n",
      "----------\n",
      "train Loss: 0.9483 Acc: 0.5950 \n",
      "val Loss: 1.0235 Acc: 0.5550 \n",
      "Epoch 27/29\n",
      "----------\n",
      "train Loss: 0.9480 Acc: 0.5950 \n",
      "val Loss: 1.0232 Acc: 0.5550 \n",
      "Epoch 28/29\n",
      "----------\n",
      "train Loss: 0.9478 Acc: 0.5950 \n",
      "val Loss: 1.0230 Acc: 0.5555 \n",
      "Epoch 29/29\n",
      "----------\n",
      "train Loss: 0.9475 Acc: 0.5950 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-04-10 18:01:09,906 - pyswarms.single.global_best - INFO - Optimize for 50 iters with {'c1': 2, 'c2': 2, 'w': 0.4}\n",
      "pyswarms.single.global_best:   0%|                                                                                |0/50"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val Loss: 1.0228 Acc: 0.5560 \n",
      "Training complete in 0m 19s\n",
      "Best val Acc: 0.557500\n",
      "Epoch generations ( 6 /200) :"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "pyswarms.single.global_best: 100%|██████████████████████████████████████████████████████████████|50/50, best_cost=0.956\n",
      "2020-04-10 18:12:28,043 - pyswarms.single.global_best - INFO - Optimization finished | best cost: 0.9556303482055664, best pos: [-2.20190287e+00 -3.53605652e+01 -1.92132223e+00 -2.97379375e-01\n",
      " -7.71440697e+00  3.51477885e+00  3.33797097e+00  2.55494642e+00\n",
      "  1.74390757e+00 -1.33760796e+01 -9.04526062e+01 -9.46693516e+00\n",
      "  1.51556790e+00  9.39082563e-01 -1.04979925e+01 -1.31121483e+01\n",
      " -5.34184992e-01 -2.13893936e+02  1.14219725e-01 -9.64043884e+01\n",
      " -1.51203525e+00 -1.05142965e+01 -3.09365630e+00 -2.06569713e-02\n",
      "  5.92018175e+00 -2.05465450e+01 -1.18008804e+01 -6.09999542e+01\n",
      " -2.93322773e+01  2.32017446e+00 -4.03612666e-02 -6.86412659e+01\n",
      " -3.33723515e-01 -1.17819385e+03 -6.21702385e+00 -2.52699304e+00\n",
      " -3.35665345e-01 -4.27566099e+00 -1.48931757e-01 -5.42799830e-01\n",
      " -8.47156420e-02 -7.54827881e+01 -3.52754211e+00 -3.63467751e+01\n",
      " -8.71730506e-01 -1.04211330e+00 -1.50141497e+01  2.43990231e+00\n",
      " -5.52582920e-01 -7.50522673e-01  1.79533803e+00  1.80512118e+00\n",
      " -2.42319077e-01 -9.28492606e-01  3.99455476e+00 -8.09035182e-01\n",
      " -7.80047238e-01 -1.00602751e+01 -1.72235394e+01 -3.47773075e+00\n",
      " -1.29030085e+00 -1.42433190e+00 -2.63926119e-01 -6.41810465e+00\n",
      "  8.38861704e-01 -4.53004189e+01 -4.84814882e+00 -2.47339344e+00\n",
      "  9.31613636e+00 -3.40241966e+01 -2.02330852e+00 -5.97578144e+00\n",
      " -4.21942425e+00 -9.97269154e+00 -5.45581150e+00 -1.97189942e-01\n",
      " -8.54199829e+01 -6.48663267e-02 -2.84416199e+01 -6.61087990e+00\n",
      " -1.22009935e+01  4.53118181e+00 -1.47970367e+02  1.17888653e+00\n",
      " -1.04428852e+00 -4.99989599e-01 -9.23117157e-03 -2.66349716e+01\n",
      "  1.94566035e+00  7.92495728e-01 -3.64301741e-01 -3.57230663e+00\n",
      " -1.01753359e+01 -5.44055939e-01 -3.48484278e+00 -3.03579271e-01\n",
      " -8.48349075e+01 -1.83169060e+01 -3.49606109e+00 -3.41653800e+00]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "keep prob 0.19\n",
      "Epoch 0/29\n",
      "----------\n",
      "train Loss: 0.9567 Acc: 0.5810 \n",
      "val Loss: 1.0293 Acc: 0.5560 \n",
      "Epoch 1/29\n",
      "----------\n",
      "train Loss: 0.9562 Acc: 0.5827 \n",
      "val Loss: 1.0291 Acc: 0.5550 \n",
      "Epoch 2/29\n",
      "----------\n",
      "train Loss: 0.9557 Acc: 0.5827 \n",
      "val Loss: 1.0289 Acc: 0.5540 \n",
      "Epoch 3/29\n",
      "----------\n",
      "train Loss: 0.9552 Acc: 0.5820 \n",
      "val Loss: 1.0287 Acc: 0.5530 \n",
      "Epoch 4/29\n",
      "----------\n",
      "train Loss: 0.9548 Acc: 0.5827 \n",
      "val Loss: 1.0285 Acc: 0.5535 \n",
      "Epoch 5/29\n",
      "----------\n",
      "train Loss: 0.9544 Acc: 0.5813 \n",
      "val Loss: 1.0282 Acc: 0.5540 \n",
      "Epoch 6/29\n",
      "----------\n",
      "train Loss: 0.9540 Acc: 0.5827 \n",
      "val Loss: 1.0280 Acc: 0.5530 \n",
      "Epoch 7/29\n",
      "----------\n",
      "train Loss: 0.9536 Acc: 0.5827 \n",
      "val Loss: 1.0278 Acc: 0.5525 \n",
      "Epoch 8/29\n",
      "----------\n",
      "train Loss: 0.9533 Acc: 0.5823 \n",
      "val Loss: 1.0275 Acc: 0.5525 \n",
      "Epoch 9/29\n",
      "----------\n",
      "train Loss: 0.9529 Acc: 0.5823 \n",
      "val Loss: 1.0273 Acc: 0.5530 \n",
      "Epoch 10/29\n",
      "----------\n",
      "train Loss: 0.9526 Acc: 0.5827 \n",
      "val Loss: 1.0270 Acc: 0.5530 \n",
      "Epoch 11/29\n",
      "----------\n",
      "train Loss: 0.9523 Acc: 0.5843 \n",
      "val Loss: 1.0268 Acc: 0.5530 \n",
      "Epoch 12/29\n",
      "----------\n",
      "train Loss: 0.9520 Acc: 0.5850 \n",
      "val Loss: 1.0265 Acc: 0.5515 \n",
      "Epoch 13/29\n",
      "----------\n",
      "train Loss: 0.9516 Acc: 0.5850 \n",
      "val Loss: 1.0263 Acc: 0.5510 \n",
      "Epoch 14/29\n",
      "----------\n",
      "train Loss: 0.9513 Acc: 0.5863 \n",
      "val Loss: 1.0261 Acc: 0.5510 \n",
      "Epoch 15/29\n",
      "----------\n",
      "train Loss: 0.9510 Acc: 0.5890 \n",
      "val Loss: 1.0258 Acc: 0.5510 \n",
      "Epoch 16/29\n",
      "----------\n",
      "train Loss: 0.9507 Acc: 0.5897 \n",
      "val Loss: 1.0256 Acc: 0.5510 \n",
      "Epoch 17/29\n",
      "----------\n",
      "train Loss: 0.9504 Acc: 0.5897 \n",
      "val Loss: 1.0253 Acc: 0.5520 \n",
      "Epoch 18/29\n",
      "----------\n",
      "train Loss: 0.9501 Acc: 0.5913 \n",
      "val Loss: 1.0251 Acc: 0.5510 \n",
      "Epoch 19/29\n",
      "----------\n",
      "train Loss: 0.9499 Acc: 0.5917 \n",
      "val Loss: 1.0249 Acc: 0.5510 \n",
      "Epoch 20/29\n",
      "----------\n",
      "train Loss: 0.9496 Acc: 0.5930 \n",
      "val Loss: 1.0246 Acc: 0.5520 \n",
      "Epoch 21/29\n",
      "----------\n",
      "train Loss: 0.9493 Acc: 0.5930 \n",
      "val Loss: 1.0244 Acc: 0.5525 \n",
      "Epoch 22/29\n",
      "----------\n",
      "train Loss: 0.9491 Acc: 0.5937 \n",
      "val Loss: 1.0241 Acc: 0.5535 \n",
      "Epoch 23/29\n",
      "----------\n",
      "train Loss: 0.9488 Acc: 0.5940 \n",
      "val Loss: 1.0239 Acc: 0.5535 \n",
      "Epoch 24/29\n",
      "----------\n",
      "train Loss: 0.9485 Acc: 0.5943 \n",
      "val Loss: 1.0237 Acc: 0.5545 \n",
      "Epoch 25/29\n",
      "----------\n",
      "train Loss: 0.9483 Acc: 0.5950 \n",
      "val Loss: 1.0234 Acc: 0.5550 \n",
      "Epoch 26/29\n",
      "----------\n",
      "train Loss: 0.9480 Acc: 0.5950 \n",
      "val Loss: 1.0232 Acc: 0.5550 \n",
      "Epoch 27/29\n",
      "----------\n",
      "train Loss: 0.9478 Acc: 0.5953 \n",
      "val Loss: 1.0230 Acc: 0.5555 \n",
      "Epoch 28/29\n",
      "----------\n",
      "train Loss: 0.9475 Acc: 0.5950 \n",
      "val Loss: 1.0228 Acc: 0.5555 \n",
      "Epoch 29/29\n",
      "----------\n",
      "train Loss: 0.9473 Acc: 0.5960 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-04-10 18:12:47,118 - pyswarms.single.global_best - INFO - Optimize for 50 iters with {'c1': 2, 'c2': 2, 'w': 0.4}\n",
      "pyswarms.single.global_best:   0%|                                                                                |0/50"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val Loss: 1.0226 Acc: 0.5560 \n",
      "Training complete in 0m 19s\n",
      "Best val Acc: 0.556000\n",
      "Epoch generations ( 7 /200) :"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "pyswarms.single.global_best: 100%|██████████████████████████████████████████████████████████████|50/50, best_cost=0.955\n",
      "2020-04-10 18:24:05,792 - pyswarms.single.global_best - INFO - Optimization finished | best cost: 0.9550539606412252, best pos: [-2.20190287e+00 -3.53605652e+01 -1.92132223e+00 -2.97379375e-01\n",
      " -7.71440697e+00  3.51477885e+00  3.33797097e+00  2.55494642e+00\n",
      "  1.74390757e+00 -1.33760796e+01 -9.04526062e+01 -9.46693516e+00\n",
      "  1.51556790e+00  9.39082563e-01 -1.04979925e+01 -1.31121483e+01\n",
      " -5.34184992e-01 -2.13893936e+02  1.14219725e-01 -9.64043884e+01\n",
      " -1.51203525e+00 -1.05142965e+01 -3.09365630e+00 -2.06569713e-02\n",
      "  5.92018175e+00 -2.05465450e+01 -1.18008804e+01 -6.09999542e+01\n",
      " -2.93322773e+01  2.32017446e+00 -4.03612666e-02 -6.86412659e+01\n",
      " -3.33723515e-01 -1.17819385e+03 -6.21702385e+00 -2.52699304e+00\n",
      " -3.35665345e-01 -4.27566099e+00 -1.48931757e-01 -5.42799830e-01\n",
      " -8.47156420e-02 -7.54827881e+01 -3.52754211e+00 -3.63467751e+01\n",
      " -8.71730506e-01 -1.04211330e+00 -1.50141497e+01  2.43990231e+00\n",
      " -5.52582920e-01 -7.50522673e-01  1.79533803e+00  1.80512118e+00\n",
      " -2.42319077e-01 -9.28492606e-01  3.99455476e+00 -8.09035182e-01\n",
      " -7.80047238e-01 -1.00602751e+01 -1.72235394e+01 -3.47773075e+00\n",
      " -1.29030085e+00 -1.42433190e+00 -2.63926119e-01 -6.41810465e+00\n",
      "  8.38861704e-01 -4.53004189e+01 -4.84814882e+00 -2.47339344e+00\n",
      "  9.31613636e+00 -3.40241966e+01 -2.02330852e+00 -5.97578144e+00\n",
      " -4.21942425e+00 -9.97269154e+00 -5.45581150e+00 -1.97189942e-01\n",
      " -8.54199829e+01 -6.48663267e-02 -2.84416199e+01 -6.61087990e+00\n",
      " -1.22009935e+01  4.53118181e+00 -1.47970367e+02  1.17888653e+00\n",
      " -1.04428852e+00 -4.99989599e-01 -9.23117157e-03 -2.66349716e+01\n",
      "  1.94566035e+00  7.92495728e-01 -3.64301741e-01 -3.57230663e+00\n",
      " -1.01753359e+01 -5.44055939e-01 -3.48484278e+00 -3.03579271e-01\n",
      " -8.48349075e+01 -1.83169060e+01 -3.49606109e+00 -3.41653800e+00]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "keep prob 0.19\n",
      "Epoch 0/29\n",
      "----------\n",
      "train Loss: 0.9562 Acc: 0.5823 \n",
      "val Loss: 1.0291 Acc: 0.5550 \n",
      "Epoch 1/29\n",
      "----------\n",
      "train Loss: 0.9557 Acc: 0.5823 \n",
      "val Loss: 1.0289 Acc: 0.5540 \n",
      "Epoch 2/29\n",
      "----------\n",
      "train Loss: 0.9552 Acc: 0.5817 \n",
      "val Loss: 1.0287 Acc: 0.5535 \n",
      "Epoch 3/29\n",
      "----------\n",
      "train Loss: 0.9548 Acc: 0.5823 \n",
      "val Loss: 1.0285 Acc: 0.5535 \n",
      "Epoch 4/29\n",
      "----------\n",
      "train Loss: 0.9544 Acc: 0.5820 \n",
      "val Loss: 1.0282 Acc: 0.5540 \n",
      "Epoch 5/29\n",
      "----------\n",
      "train Loss: 0.9540 Acc: 0.5823 \n",
      "val Loss: 1.0280 Acc: 0.5530 \n",
      "Epoch 6/29\n",
      "----------\n",
      "train Loss: 0.9536 Acc: 0.5823 \n",
      "val Loss: 1.0277 Acc: 0.5525 \n",
      "Epoch 7/29\n",
      "----------\n",
      "train Loss: 0.9533 Acc: 0.5823 \n",
      "val Loss: 1.0275 Acc: 0.5525 \n",
      "Epoch 8/29\n",
      "----------\n",
      "train Loss: 0.9529 Acc: 0.5823 \n",
      "val Loss: 1.0273 Acc: 0.5530 \n",
      "Epoch 9/29\n",
      "----------\n",
      "train Loss: 0.9526 Acc: 0.5823 \n",
      "val Loss: 1.0270 Acc: 0.5530 \n",
      "Epoch 10/29\n",
      "----------\n",
      "train Loss: 0.9523 Acc: 0.5843 \n",
      "val Loss: 1.0268 Acc: 0.5530 \n",
      "Epoch 11/29\n",
      "----------\n",
      "train Loss: 0.9519 Acc: 0.5850 \n",
      "val Loss: 1.0265 Acc: 0.5515 \n",
      "Epoch 12/29\n",
      "----------\n",
      "train Loss: 0.9516 Acc: 0.5853 \n",
      "val Loss: 1.0263 Acc: 0.5510 \n",
      "Epoch 13/29\n",
      "----------\n",
      "train Loss: 0.9513 Acc: 0.5863 \n",
      "val Loss: 1.0261 Acc: 0.5510 \n",
      "Epoch 14/29\n",
      "----------\n",
      "train Loss: 0.9510 Acc: 0.5887 \n",
      "val Loss: 1.0258 Acc: 0.5510 \n",
      "Epoch 15/29\n",
      "----------\n",
      "train Loss: 0.9507 Acc: 0.5893 \n",
      "val Loss: 1.0256 Acc: 0.5520 \n",
      "Epoch 16/29\n",
      "----------\n",
      "train Loss: 0.9504 Acc: 0.5897 \n",
      "val Loss: 1.0253 Acc: 0.5520 \n",
      "Epoch 17/29\n",
      "----------\n",
      "train Loss: 0.9501 Acc: 0.5917 \n",
      "val Loss: 1.0251 Acc: 0.5520 \n",
      "Epoch 18/29\n",
      "----------\n",
      "train Loss: 0.9499 Acc: 0.5913 \n",
      "val Loss: 1.0249 Acc: 0.5510 \n",
      "Epoch 19/29\n",
      "----------\n",
      "train Loss: 0.9496 Acc: 0.5930 \n",
      "val Loss: 1.0246 Acc: 0.5520 \n",
      "Epoch 20/29\n",
      "----------\n",
      "train Loss: 0.9493 Acc: 0.5930 \n",
      "val Loss: 1.0244 Acc: 0.5525 \n",
      "Epoch 21/29\n",
      "----------\n",
      "train Loss: 0.9491 Acc: 0.5937 \n",
      "val Loss: 1.0241 Acc: 0.5535 \n",
      "Epoch 22/29\n",
      "----------\n",
      "train Loss: 0.9488 Acc: 0.5940 \n",
      "val Loss: 1.0239 Acc: 0.5535 \n",
      "Epoch 23/29\n",
      "----------\n",
      "train Loss: 0.9485 Acc: 0.5943 \n",
      "val Loss: 1.0237 Acc: 0.5545 \n",
      "Epoch 24/29\n",
      "----------\n",
      "train Loss: 0.9483 Acc: 0.5950 \n",
      "val Loss: 1.0234 Acc: 0.5555 \n",
      "Epoch 25/29\n",
      "----------\n",
      "train Loss: 0.9480 Acc: 0.5950 \n",
      "val Loss: 1.0232 Acc: 0.5550 \n",
      "Epoch 26/29\n",
      "----------\n",
      "train Loss: 0.9478 Acc: 0.5950 \n",
      "val Loss: 1.0230 Acc: 0.5555 \n",
      "Epoch 27/29\n",
      "----------\n",
      "train Loss: 0.9475 Acc: 0.5950 \n",
      "val Loss: 1.0228 Acc: 0.5550 \n",
      "Epoch 28/29\n",
      "----------\n",
      "train Loss: 0.9473 Acc: 0.5960 \n",
      "val Loss: 1.0226 Acc: 0.5565 \n",
      "Epoch 29/29\n",
      "----------\n",
      "train Loss: 0.9471 Acc: 0.5967 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-04-10 18:24:25,362 - pyswarms.single.global_best - INFO - Optimize for 50 iters with {'c1': 2, 'c2': 2, 'w': 0.4}\n",
      "pyswarms.single.global_best:   0%|                                                                                |0/50"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val Loss: 1.0224 Acc: 0.5560 \n",
      "Training complete in 0m 20s\n",
      "Best val Acc: 0.556500\n",
      "Epoch generations ( 8 /200) :"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "pyswarms.single.global_best: 100%|██████████████████████████████████████████████████████████████|50/50, best_cost=0.946\n",
      "2020-04-10 18:36:20,554 - pyswarms.single.global_best - INFO - Optimization finished | best cost: 0.9457404451370239, best pos: [-2.20190287e+00 -3.53605652e+01 -1.92132223e+00 -2.97379375e-01\n",
      " -7.71440697e+00  3.51477885e+00  3.33797097e+00  2.55494642e+00\n",
      "  1.74390757e+00 -1.33760796e+01 -9.04526062e+01 -9.46693516e+00\n",
      "  1.51556790e+00  9.39082563e-01 -1.04979925e+01 -1.31121483e+01\n",
      " -5.34184992e-01 -2.13893936e+02  1.14219725e-01 -9.64043884e+01\n",
      " -1.51203525e+00 -1.05142965e+01 -3.09365630e+00 -2.06569713e-02\n",
      "  5.92018175e+00 -2.05465450e+01 -1.18008804e+01 -6.09999542e+01\n",
      " -2.93322773e+01  2.32017446e+00 -4.03612666e-02 -6.86412659e+01\n",
      " -3.33723515e-01 -1.17819385e+03 -6.21702385e+00 -2.52699304e+00\n",
      " -3.35665345e-01 -4.27566099e+00 -1.48931757e-01 -5.42799830e-01\n",
      " -8.47156420e-02 -7.54827881e+01 -3.52754211e+00 -3.63467751e+01\n",
      " -8.71730506e-01 -1.04211330e+00 -1.50141497e+01  2.43990231e+00\n",
      " -5.52582920e-01 -7.50522673e-01  1.79533803e+00  1.80512118e+00\n",
      " -2.42319077e-01 -9.28492606e-01  3.99455476e+00 -8.09035182e-01\n",
      " -7.80047238e-01 -1.00602751e+01 -1.72235394e+01 -3.47773075e+00\n",
      " -1.29030085e+00 -1.42433190e+00 -2.63926119e-01 -6.41810465e+00\n",
      "  8.38861704e-01 -4.53004189e+01 -4.84814882e+00 -2.47339344e+00\n",
      "  9.31613636e+00 -3.40241966e+01 -2.02330852e+00 -5.97578144e+00\n",
      " -4.21942425e+00 -9.97269154e+00 -5.45581150e+00 -1.97189942e-01\n",
      " -8.54199829e+01 -6.48663267e-02 -2.84416199e+01 -6.61087990e+00\n",
      " -1.22009935e+01  4.53118181e+00 -1.47970367e+02  1.17888653e+00\n",
      " -1.04428852e+00 -4.99989599e-01 -9.23117157e-03 -2.66349716e+01\n",
      "  1.94566035e+00  7.92495728e-01 -3.64301741e-01 -3.57230663e+00\n",
      " -1.01753359e+01 -5.44055939e-01 -3.48484278e+00 -3.03579271e-01\n",
      " -8.48349075e+01 -1.83169060e+01 -3.49606109e+00 -3.41653800e+00]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "keep prob 0.19\n",
      "Epoch 0/29\n",
      "----------\n",
      "train Loss: 0.9471 Acc: 0.5967 \n",
      "val Loss: 1.0224 Acc: 0.5560 \n",
      "Epoch 1/29\n",
      "----------\n",
      "train Loss: 0.9469 Acc: 0.5963 \n",
      "val Loss: 1.0222 Acc: 0.5555 \n",
      "Epoch 2/29\n",
      "----------\n",
      "train Loss: 0.9466 Acc: 0.5963 \n",
      "val Loss: 1.0220 Acc: 0.5555 \n",
      "Epoch 3/29\n",
      "----------\n",
      "train Loss: 0.9464 Acc: 0.5973 \n",
      "val Loss: 1.0218 Acc: 0.5550 \n",
      "Epoch 4/29\n",
      "----------\n",
      "train Loss: 0.9462 Acc: 0.5980 \n",
      "val Loss: 1.0217 Acc: 0.5565 \n",
      "Epoch 5/29\n",
      "----------\n",
      "train Loss: 0.9460 Acc: 0.5983 \n",
      "val Loss: 1.0215 Acc: 0.5555 \n",
      "Epoch 6/29\n",
      "----------\n",
      "train Loss: 0.9458 Acc: 0.5987 \n",
      "val Loss: 1.0213 Acc: 0.5560 \n",
      "Epoch 7/29\n",
      "----------\n",
      "train Loss: 0.9456 Acc: 0.5997 \n",
      "val Loss: 1.0212 Acc: 0.5560 \n",
      "Epoch 8/29\n",
      "----------\n",
      "train Loss: 0.9454 Acc: 0.6000 \n",
      "val Loss: 1.0210 Acc: 0.5565 \n",
      "Epoch 9/29\n",
      "----------\n",
      "train Loss: 0.9452 Acc: 0.6000 \n",
      "val Loss: 1.0208 Acc: 0.5570 \n",
      "Epoch 10/29\n",
      "----------\n",
      "train Loss: 0.9450 Acc: 0.6010 \n",
      "val Loss: 1.0207 Acc: 0.5565 \n",
      "Epoch 11/29\n",
      "----------\n",
      "train Loss: 0.9448 Acc: 0.6007 \n",
      "val Loss: 1.0205 Acc: 0.5565 \n",
      "Epoch 12/29\n",
      "----------\n",
      "train Loss: 0.9446 Acc: 0.6010 \n",
      "val Loss: 1.0204 Acc: 0.5565 \n",
      "Epoch 13/29\n",
      "----------\n",
      "train Loss: 0.9443 Acc: 0.6010 \n",
      "val Loss: 1.0202 Acc: 0.5565 \n",
      "Epoch 14/29\n",
      "----------\n",
      "train Loss: 0.9441 Acc: 0.6013 \n",
      "val Loss: 1.0201 Acc: 0.5570 \n",
      "Epoch 15/29\n",
      "----------\n",
      "train Loss: 0.9439 Acc: 0.6013 \n",
      "val Loss: 1.0199 Acc: 0.5565 \n",
      "Epoch 16/29\n",
      "----------\n",
      "train Loss: 0.9437 Acc: 0.6017 \n",
      "val Loss: 1.0198 Acc: 0.5565 \n",
      "Epoch 17/29\n",
      "----------\n",
      "train Loss: 0.9435 Acc: 0.6017 \n",
      "val Loss: 1.0196 Acc: 0.5565 \n",
      "Epoch 18/29\n",
      "----------\n",
      "train Loss: 0.9433 Acc: 0.6020 \n",
      "val Loss: 1.0195 Acc: 0.5560 \n",
      "Epoch 19/29\n",
      "----------\n",
      "train Loss: 0.9431 Acc: 0.6017 \n",
      "val Loss: 1.0194 Acc: 0.5555 \n",
      "Epoch 20/29\n",
      "----------\n",
      "train Loss: 0.9430 Acc: 0.6017 \n",
      "val Loss: 1.0192 Acc: 0.5550 \n",
      "Epoch 21/29\n",
      "----------\n",
      "train Loss: 0.9428 Acc: 0.6027 \n",
      "val Loss: 1.0191 Acc: 0.5560 \n",
      "Epoch 22/29\n",
      "----------\n",
      "train Loss: 0.9426 Acc: 0.6037 \n",
      "val Loss: 1.0190 Acc: 0.5560 \n",
      "Epoch 23/29\n",
      "----------\n",
      "train Loss: 0.9424 Acc: 0.6043 \n",
      "val Loss: 1.0188 Acc: 0.5560 \n",
      "Epoch 24/29\n",
      "----------\n",
      "train Loss: 0.9422 Acc: 0.6037 \n",
      "val Loss: 1.0187 Acc: 0.5560 \n",
      "Epoch 25/29\n",
      "----------\n",
      "train Loss: 0.9420 Acc: 0.6040 \n",
      "val Loss: 1.0186 Acc: 0.5560 \n",
      "Epoch 26/29\n",
      "----------\n",
      "train Loss: 0.9418 Acc: 0.6040 \n",
      "val Loss: 1.0184 Acc: 0.5565 \n",
      "Epoch 27/29\n",
      "----------\n",
      "train Loss: 0.9416 Acc: 0.6037 \n",
      "val Loss: 1.0183 Acc: 0.5560 \n",
      "Epoch 28/29\n",
      "----------\n",
      "train Loss: 0.9414 Acc: 0.6043 \n",
      "val Loss: 1.0182 Acc: 0.5555 \n",
      "Epoch 29/29\n",
      "----------\n",
      "train Loss: 0.9412 Acc: 0.6043 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-04-10 18:36:39,871 - pyswarms.single.global_best - INFO - Optimize for 50 iters with {'c1': 2, 'c2': 2, 'w': 0.4}\n",
      "pyswarms.single.global_best:   0%|                                                                                |0/50"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val Loss: 1.0181 Acc: 0.5550 \n",
      "Training complete in 0m 19s\n",
      "Best val Acc: 0.557000\n",
      "Epoch generations ( 9 /200) :"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "pyswarms.single.global_best: 100%|██████████████████████████████████████████████████████████████|50/50, best_cost=0.944\n",
      "2020-04-10 18:48:39,271 - pyswarms.single.global_best - INFO - Optimization finished | best cost: 0.9435989699363708, best pos: [-2.20190287e+00 -3.53605652e+01 -1.92132223e+00 -2.97379375e-01\n",
      " -7.71440697e+00  3.51477885e+00  3.33797097e+00  2.55494642e+00\n",
      "  1.74390757e+00 -1.33760796e+01 -9.04526062e+01 -9.46693516e+00\n",
      "  1.51556790e+00  9.39082563e-01 -1.04979925e+01 -1.31121483e+01\n",
      " -5.34184992e-01 -2.13893936e+02  1.14219725e-01 -9.64043884e+01\n",
      " -1.51203525e+00 -1.05142965e+01 -3.09365630e+00 -2.06569713e-02\n",
      "  5.92018175e+00 -2.05465450e+01 -1.18008804e+01 -6.09999542e+01\n",
      " -2.93322773e+01  2.32017446e+00 -4.03612666e-02 -6.86412659e+01\n",
      " -3.33723515e-01 -1.17819385e+03 -6.21702385e+00 -2.52699304e+00\n",
      " -3.35665345e-01 -4.27566099e+00 -1.48931757e-01 -5.42799830e-01\n",
      " -8.47156420e-02 -7.54827881e+01 -3.52754211e+00 -3.63467751e+01\n",
      " -8.71730506e-01 -1.04211330e+00 -1.50141497e+01  2.43990231e+00\n",
      " -5.52582920e-01 -7.50522673e-01  1.79533803e+00  1.80512118e+00\n",
      " -2.42319077e-01 -9.28492606e-01  3.99455476e+00 -8.09035182e-01\n",
      " -7.80047238e-01 -1.00602751e+01 -1.72235394e+01 -3.47773075e+00\n",
      " -1.29030085e+00 -1.42433190e+00 -2.63926119e-01 -6.41810465e+00\n",
      "  8.38861704e-01 -4.53004189e+01 -4.84814882e+00 -2.47339344e+00\n",
      "  9.31613636e+00 -3.40241966e+01 -2.02330852e+00 -5.97578144e+00\n",
      " -4.21942425e+00 -9.97269154e+00 -5.45581150e+00 -1.97189942e-01\n",
      " -8.54199829e+01 -6.48663267e-02 -2.84416199e+01 -6.61087990e+00\n",
      " -1.22009935e+01  4.53118181e+00 -1.47970367e+02  1.17888653e+00\n",
      " -1.04428852e+00 -4.99989599e-01 -9.23117157e-03 -2.66349716e+01\n",
      "  1.94566035e+00  7.92495728e-01 -3.64301741e-01 -3.57230663e+00\n",
      " -1.01753359e+01 -5.44055939e-01 -3.48484278e+00 -3.03579271e-01\n",
      " -8.48349075e+01 -1.83169060e+01 -3.49606109e+00 -3.41653800e+00]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "keep prob 0.19\n",
      "Epoch 0/29\n",
      "----------\n",
      "train Loss: 0.9450 Acc: 0.6010 \n",
      "val Loss: 1.0207 Acc: 0.5565 \n",
      "Epoch 1/29\n",
      "----------\n",
      "train Loss: 0.9448 Acc: 0.6007 \n",
      "val Loss: 1.0205 Acc: 0.5565 \n",
      "Epoch 2/29\n",
      "----------\n",
      "train Loss: 0.9446 Acc: 0.6007 \n",
      "val Loss: 1.0204 Acc: 0.5565 \n",
      "Epoch 3/29\n",
      "----------\n",
      "train Loss: 0.9444 Acc: 0.6013 \n",
      "val Loss: 1.0202 Acc: 0.5565 \n",
      "Epoch 4/29\n",
      "----------\n",
      "train Loss: 0.9441 Acc: 0.6013 \n",
      "val Loss: 1.0201 Acc: 0.5575 \n",
      "Epoch 5/29\n",
      "----------\n",
      "train Loss: 0.9439 Acc: 0.6013 \n",
      "val Loss: 1.0199 Acc: 0.5565 \n",
      "Epoch 6/29\n",
      "----------\n",
      "train Loss: 0.9437 Acc: 0.6017 \n",
      "val Loss: 1.0198 Acc: 0.5565 \n",
      "Epoch 7/29\n",
      "----------\n",
      "train Loss: 0.9435 Acc: 0.6017 \n",
      "val Loss: 1.0197 Acc: 0.5560 \n",
      "Epoch 8/29\n",
      "----------\n",
      "train Loss: 0.9433 Acc: 0.6020 \n",
      "val Loss: 1.0195 Acc: 0.5560 \n",
      "Epoch 9/29\n",
      "----------\n",
      "train Loss: 0.9432 Acc: 0.6017 \n",
      "val Loss: 1.0194 Acc: 0.5555 \n",
      "Epoch 10/29\n",
      "----------\n",
      "train Loss: 0.9430 Acc: 0.6017 \n",
      "val Loss: 1.0192 Acc: 0.5550 \n",
      "Epoch 11/29\n",
      "----------\n",
      "train Loss: 0.9428 Acc: 0.6027 \n",
      "val Loss: 1.0191 Acc: 0.5560 \n",
      "Epoch 12/29\n",
      "----------\n",
      "train Loss: 0.9426 Acc: 0.6033 \n",
      "val Loss: 1.0190 Acc: 0.5560 \n",
      "Epoch 13/29\n",
      "----------\n",
      "train Loss: 0.9424 Acc: 0.6043 \n",
      "val Loss: 1.0188 Acc: 0.5560 \n",
      "Epoch 14/29\n",
      "----------\n",
      "train Loss: 0.9422 Acc: 0.6043 \n",
      "val Loss: 1.0187 Acc: 0.5560 \n",
      "Epoch 15/29\n",
      "----------\n",
      "train Loss: 0.9420 Acc: 0.6040 \n",
      "val Loss: 1.0186 Acc: 0.5565 \n",
      "Epoch 16/29\n",
      "----------\n",
      "train Loss: 0.9418 Acc: 0.6040 \n",
      "val Loss: 1.0184 Acc: 0.5560 \n",
      "Epoch 17/29\n",
      "----------\n",
      "train Loss: 0.9416 Acc: 0.6040 \n",
      "val Loss: 1.0183 Acc: 0.5560 \n",
      "Epoch 18/29\n",
      "----------\n",
      "train Loss: 0.9414 Acc: 0.6043 \n",
      "val Loss: 1.0182 Acc: 0.5560 \n",
      "Epoch 19/29\n",
      "----------\n",
      "train Loss: 0.9412 Acc: 0.6043 \n",
      "val Loss: 1.0181 Acc: 0.5540 \n",
      "Epoch 20/29\n",
      "----------\n",
      "train Loss: 0.9410 Acc: 0.6043 \n",
      "val Loss: 1.0179 Acc: 0.5540 \n",
      "Epoch 21/29\n",
      "----------\n",
      "train Loss: 0.9409 Acc: 0.6040 \n",
      "val Loss: 1.0178 Acc: 0.5540 \n",
      "Epoch 22/29\n",
      "----------\n",
      "train Loss: 0.9407 Acc: 0.6043 \n",
      "val Loss: 1.0178 Acc: 0.5535 \n",
      "Epoch 23/29\n",
      "----------\n",
      "train Loss: 0.9405 Acc: 0.6037 \n",
      "val Loss: 1.0177 Acc: 0.5540 \n",
      "Epoch 24/29\n",
      "----------\n",
      "train Loss: 0.9403 Acc: 0.6040 \n",
      "val Loss: 1.0176 Acc: 0.5535 \n",
      "Epoch 25/29\n",
      "----------\n",
      "train Loss: 0.9401 Acc: 0.6040 \n",
      "val Loss: 1.0175 Acc: 0.5535 \n",
      "Epoch 26/29\n",
      "----------\n",
      "train Loss: 0.9399 Acc: 0.6037 \n",
      "val Loss: 1.0174 Acc: 0.5530 \n",
      "Epoch 27/29\n",
      "----------\n",
      "train Loss: 0.9397 Acc: 0.6033 \n",
      "val Loss: 1.0173 Acc: 0.5535 \n",
      "Epoch 28/29\n",
      "----------\n",
      "train Loss: 0.9395 Acc: 0.6037 \n",
      "val Loss: 1.0173 Acc: 0.5535 \n",
      "Epoch 29/29\n",
      "----------\n",
      "train Loss: 0.9393 Acc: 0.6040 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-04-10 18:49:00,473 - pyswarms.single.global_best - INFO - Optimize for 50 iters with {'c1': 2, 'c2': 2, 'w': 0.4}\n",
      "pyswarms.single.global_best:   0%|                                                                                |0/50"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val Loss: 1.0172 Acc: 0.5535 \n",
      "Training complete in 0m 21s\n",
      "Best val Acc: 0.557500\n",
      "Epoch generations ( 10 /200) :"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "pyswarms.single.global_best: 100%|██████████████████████████████████████████████████████████████|50/50, best_cost=0.943\n",
      "2020-04-10 19:01:31,867 - pyswarms.single.global_best - INFO - Optimization finished | best cost: 0.9425469096501669, best pos: [-2.20190287e+00 -3.53605652e+01 -1.92132223e+00 -2.97379375e-01\n",
      " -7.71440697e+00  3.51477885e+00  3.33797097e+00  2.55494642e+00\n",
      "  1.74390757e+00 -1.33760796e+01 -9.04526062e+01 -9.46693516e+00\n",
      "  1.51556790e+00  9.39082563e-01 -1.04979925e+01 -1.31121483e+01\n",
      " -5.34184992e-01 -2.13893936e+02  1.14219725e-01 -9.64043884e+01\n",
      " -1.51203525e+00 -1.05142965e+01 -3.09365630e+00 -2.06569713e-02\n",
      "  5.92018175e+00 -2.05465450e+01 -1.18008804e+01 -6.09999542e+01\n",
      " -2.93322773e+01  2.32017446e+00 -4.03612666e-02 -6.86412659e+01\n",
      " -3.33723515e-01 -1.17819385e+03 -6.21702385e+00 -2.52699304e+00\n",
      " -3.35665345e-01 -4.27566099e+00 -1.48931757e-01 -5.42799830e-01\n",
      " -8.47156420e-02 -7.54827881e+01 -3.52754211e+00 -3.63467751e+01\n",
      " -8.71730506e-01 -1.04211330e+00 -1.50141497e+01  2.43990231e+00\n",
      " -5.52582920e-01 -7.50522673e-01  1.79533803e+00  1.80512118e+00\n",
      " -2.42319077e-01 -9.28492606e-01  3.99455476e+00 -8.09035182e-01\n",
      " -7.80047238e-01 -1.00602751e+01 -1.72235394e+01 -3.47773075e+00\n",
      " -1.29030085e+00 -1.42433190e+00 -2.63926119e-01 -6.41810465e+00\n",
      "  8.38861704e-01 -4.53004189e+01 -4.84814882e+00 -2.47339344e+00\n",
      "  9.31613636e+00 -3.40241966e+01 -2.02330852e+00 -5.97578144e+00\n",
      " -4.21942425e+00 -9.97269154e+00 -5.45581150e+00 -1.97189942e-01\n",
      " -8.54199829e+01 -6.48663267e-02 -2.84416199e+01 -6.61087990e+00\n",
      " -1.22009935e+01  4.53118181e+00 -1.47970367e+02  1.17888653e+00\n",
      " -1.04428852e+00 -4.99989599e-01 -9.23117157e-03 -2.66349716e+01\n",
      "  1.94566035e+00  7.92495728e-01 -3.64301741e-01 -3.57230663e+00\n",
      " -1.01753359e+01 -5.44055939e-01 -3.48484278e+00 -3.03579271e-01\n",
      " -8.48349075e+01 -1.83169060e+01 -3.49606109e+00 -3.41653800e+00]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "keep prob 0.19\n",
      "Epoch 0/29\n",
      "----------\n",
      "train Loss: 0.9439 Acc: 0.6013 \n",
      "val Loss: 1.0199 Acc: 0.5560 \n",
      "Epoch 1/29\n",
      "----------\n",
      "train Loss: 0.9437 Acc: 0.6017 \n",
      "val Loss: 1.0198 Acc: 0.5560 \n",
      "Epoch 2/29\n",
      "----------\n",
      "train Loss: 0.9435 Acc: 0.6017 \n",
      "val Loss: 1.0197 Acc: 0.5560 \n",
      "Epoch 3/29\n",
      "----------\n",
      "train Loss: 0.9433 Acc: 0.6020 \n",
      "val Loss: 1.0195 Acc: 0.5560 \n",
      "Epoch 4/29\n",
      "----------\n",
      "train Loss: 0.9432 Acc: 0.6017 \n",
      "val Loss: 1.0194 Acc: 0.5555 \n",
      "Epoch 5/29\n",
      "----------\n",
      "train Loss: 0.9430 Acc: 0.6017 \n",
      "val Loss: 1.0192 Acc: 0.5550 \n",
      "Epoch 6/29\n",
      "----------\n",
      "train Loss: 0.9428 Acc: 0.6027 \n",
      "val Loss: 1.0191 Acc: 0.5555 \n",
      "Epoch 7/29\n",
      "----------\n",
      "train Loss: 0.9426 Acc: 0.6037 \n",
      "val Loss: 1.0190 Acc: 0.5560 \n",
      "Epoch 8/29\n",
      "----------\n",
      "train Loss: 0.9424 Acc: 0.6040 \n",
      "val Loss: 1.0188 Acc: 0.5560 \n",
      "Epoch 9/29\n",
      "----------\n",
      "train Loss: 0.9422 Acc: 0.6043 \n",
      "val Loss: 1.0187 Acc: 0.5555 \n",
      "Epoch 10/29\n",
      "----------\n",
      "train Loss: 0.9420 Acc: 0.6040 \n",
      "val Loss: 1.0186 Acc: 0.5560 \n",
      "Epoch 11/29\n",
      "----------\n",
      "train Loss: 0.9418 Acc: 0.6040 \n",
      "val Loss: 1.0184 Acc: 0.5555 \n",
      "Epoch 12/29\n",
      "----------\n",
      "train Loss: 0.9416 Acc: 0.6037 \n",
      "val Loss: 1.0183 Acc: 0.5545 \n",
      "Epoch 13/29\n",
      "----------\n",
      "train Loss: 0.9414 Acc: 0.6047 \n",
      "val Loss: 1.0182 Acc: 0.5545 \n",
      "Epoch 14/29\n",
      "----------\n",
      "train Loss: 0.9412 Acc: 0.6043 \n",
      "val Loss: 1.0181 Acc: 0.5540 \n",
      "Epoch 15/29\n",
      "----------\n",
      "train Loss: 0.9410 Acc: 0.6040 \n",
      "val Loss: 1.0180 Acc: 0.5540 \n",
      "Epoch 16/29\n",
      "----------\n",
      "train Loss: 0.9408 Acc: 0.6040 \n",
      "val Loss: 1.0179 Acc: 0.5535 \n",
      "Epoch 17/29\n",
      "----------\n",
      "train Loss: 0.9407 Acc: 0.6040 \n",
      "val Loss: 1.0177 Acc: 0.5535 \n",
      "Epoch 18/29\n",
      "----------\n",
      "train Loss: 0.9405 Acc: 0.6037 \n",
      "val Loss: 1.0177 Acc: 0.5540 \n",
      "Epoch 19/29\n",
      "----------\n",
      "train Loss: 0.9403 Acc: 0.6040 \n",
      "val Loss: 1.0176 Acc: 0.5535 \n",
      "Epoch 20/29\n",
      "----------\n",
      "train Loss: 0.9401 Acc: 0.6040 \n",
      "val Loss: 1.0175 Acc: 0.5535 \n",
      "Epoch 21/29\n",
      "----------\n",
      "train Loss: 0.9399 Acc: 0.6033 \n",
      "val Loss: 1.0174 Acc: 0.5525 \n",
      "Epoch 22/29\n",
      "----------\n",
      "train Loss: 0.9397 Acc: 0.6033 \n",
      "val Loss: 1.0173 Acc: 0.5530 \n",
      "Epoch 23/29\n",
      "----------\n",
      "train Loss: 0.9395 Acc: 0.6043 \n",
      "val Loss: 1.0173 Acc: 0.5535 \n",
      "Epoch 24/29\n",
      "----------\n",
      "train Loss: 0.9393 Acc: 0.6040 \n",
      "val Loss: 1.0172 Acc: 0.5540 \n",
      "Epoch 25/29\n",
      "----------\n",
      "train Loss: 0.9390 Acc: 0.6043 \n",
      "val Loss: 1.0171 Acc: 0.5540 \n",
      "Epoch 26/29\n",
      "----------\n",
      "train Loss: 0.9389 Acc: 0.6043 \n",
      "val Loss: 1.0170 Acc: 0.5550 \n",
      "Epoch 27/29\n",
      "----------\n",
      "train Loss: 0.9386 Acc: 0.6047 \n",
      "val Loss: 1.0170 Acc: 0.5545 \n",
      "Epoch 28/29\n",
      "----------\n",
      "train Loss: 0.9384 Acc: 0.6047 \n",
      "val Loss: 1.0169 Acc: 0.5540 \n",
      "Epoch 29/29\n",
      "----------\n",
      "train Loss: 0.9382 Acc: 0.6047 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-04-10 19:01:52,792 - pyswarms.single.global_best - INFO - Optimize for 50 iters with {'c1': 2, 'c2': 2, 'w': 0.4}\n",
      "pyswarms.single.global_best:   0%|                                                                                |0/50"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val Loss: 1.0168 Acc: 0.5535 \n",
      "Training complete in 0m 21s\n",
      "Best val Acc: 0.556000\n",
      "Epoch generations ( 11 /200) :"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "pyswarms.single.global_best: 100%|██████████████████████████████████████████████████████████████|50/50, best_cost=0.942\n",
      "2020-04-10 19:14:29,110 - pyswarms.single.global_best - INFO - Optimization finished | best cost: 0.942344843864441, best pos: [-2.20190287e+00 -3.53605652e+01 -1.92132223e+00 -2.97379375e-01\n",
      " -7.71440697e+00  3.51477885e+00  3.33797097e+00  2.55494642e+00\n",
      "  1.74390757e+00 -1.33760796e+01 -9.04526062e+01 -9.46693516e+00\n",
      "  1.51556790e+00  9.39082563e-01 -1.04979925e+01 -1.31121483e+01\n",
      " -5.34184992e-01 -2.13893936e+02  1.14219725e-01 -9.64043884e+01\n",
      " -1.51203525e+00 -1.05142965e+01 -3.09365630e+00 -2.06569713e-02\n",
      "  5.92018175e+00 -2.05465450e+01 -1.18008804e+01 -6.09999542e+01\n",
      " -2.93322773e+01  2.32017446e+00 -4.03612666e-02 -6.86412659e+01\n",
      " -3.33723515e-01 -1.17819385e+03 -6.21702385e+00 -2.52699304e+00\n",
      " -3.35665345e-01 -4.27566099e+00 -1.48931757e-01 -5.42799830e-01\n",
      " -8.47156420e-02 -7.54827881e+01 -3.52754211e+00 -3.63467751e+01\n",
      " -8.71730506e-01 -1.04211330e+00 -1.50141497e+01  2.43990231e+00\n",
      " -5.52582920e-01 -7.50522673e-01  1.79533803e+00  1.80512118e+00\n",
      " -2.42319077e-01 -9.28492606e-01  3.99455476e+00 -8.09035182e-01\n",
      " -7.80047238e-01 -1.00602751e+01 -1.72235394e+01 -3.47773075e+00\n",
      " -1.29030085e+00 -1.42433190e+00 -2.63926119e-01 -6.41810465e+00\n",
      "  8.38861704e-01 -4.53004189e+01 -4.84814882e+00 -2.47339344e+00\n",
      "  9.31613636e+00 -3.40241966e+01 -2.02330852e+00 -5.97578144e+00\n",
      " -4.21942425e+00 -9.97269154e+00 -5.45581150e+00 -1.97189942e-01\n",
      " -8.54199829e+01 -6.48663267e-02 -2.84416199e+01 -6.61087990e+00\n",
      " -1.22009935e+01  4.53118181e+00 -1.47970367e+02  1.17888653e+00\n",
      " -1.04428852e+00 -4.99989599e-01 -9.23117157e-03 -2.66349716e+01\n",
      "  1.94566035e+00  7.92495728e-01 -3.64301741e-01 -3.57230663e+00\n",
      " -1.01753359e+01 -5.44055939e-01 -3.48484278e+00 -3.03579271e-01\n",
      " -8.48349075e+01 -1.83169060e+01 -3.49606109e+00 -3.41653800e+00]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "keep prob 0.19\n",
      "Epoch 0/29\n",
      "----------\n",
      "train Loss: 0.9437 Acc: 0.6017 \n",
      "val Loss: 1.0198 Acc: 0.5550 \n",
      "Epoch 1/29\n",
      "----------\n",
      "train Loss: 0.9435 Acc: 0.6017 \n",
      "val Loss: 1.0197 Acc: 0.5560 \n",
      "Epoch 2/29\n",
      "----------\n",
      "train Loss: 0.9433 Acc: 0.6017 \n",
      "val Loss: 1.0195 Acc: 0.5555 \n",
      "Epoch 3/29\n",
      "----------\n",
      "train Loss: 0.9432 Acc: 0.6017 \n",
      "val Loss: 1.0194 Acc: 0.5555 \n",
      "Epoch 4/29\n",
      "----------\n",
      "train Loss: 0.9430 Acc: 0.6020 \n",
      "val Loss: 1.0192 Acc: 0.5550 \n",
      "Epoch 5/29\n",
      "----------\n",
      "train Loss: 0.9428 Acc: 0.6023 \n",
      "val Loss: 1.0191 Acc: 0.5550 \n",
      "Epoch 6/29\n",
      "----------\n",
      "train Loss: 0.9426 Acc: 0.6033 \n",
      "val Loss: 1.0190 Acc: 0.5560 \n",
      "Epoch 7/29\n",
      "----------\n",
      "train Loss: 0.9424 Acc: 0.6040 \n",
      "val Loss: 1.0189 Acc: 0.5560 \n",
      "Epoch 8/29\n",
      "----------\n",
      "train Loss: 0.9422 Acc: 0.6040 \n",
      "val Loss: 1.0187 Acc: 0.5555 \n",
      "Epoch 9/29\n",
      "----------\n",
      "train Loss: 0.9420 Acc: 0.6040 \n",
      "val Loss: 1.0186 Acc: 0.5560 \n",
      "Epoch 10/29\n",
      "----------\n",
      "train Loss: 0.9418 Acc: 0.6040 \n",
      "val Loss: 1.0184 Acc: 0.5560 \n",
      "Epoch 11/29\n",
      "----------\n",
      "train Loss: 0.9416 Acc: 0.6037 \n",
      "val Loss: 1.0183 Acc: 0.5555 \n",
      "Epoch 12/29\n",
      "----------\n",
      "train Loss: 0.9414 Acc: 0.6047 \n",
      "val Loss: 1.0182 Acc: 0.5535 \n",
      "Epoch 13/29\n",
      "----------\n",
      "train Loss: 0.9412 Acc: 0.6043 \n",
      "val Loss: 1.0181 Acc: 0.5540 \n",
      "Epoch 14/29\n",
      "----------\n",
      "train Loss: 0.9410 Acc: 0.6040 \n",
      "val Loss: 1.0180 Acc: 0.5540 \n",
      "Epoch 15/29\n",
      "----------\n",
      "train Loss: 0.9408 Acc: 0.6040 \n",
      "val Loss: 1.0179 Acc: 0.5540 \n",
      "Epoch 16/29\n",
      "----------\n",
      "train Loss: 0.9407 Acc: 0.6040 \n",
      "val Loss: 1.0178 Acc: 0.5535 \n",
      "Epoch 17/29\n",
      "----------\n",
      "train Loss: 0.9405 Acc: 0.6040 \n",
      "val Loss: 1.0177 Acc: 0.5535 \n",
      "Epoch 18/29\n",
      "----------\n",
      "train Loss: 0.9403 Acc: 0.6040 \n",
      "val Loss: 1.0176 Acc: 0.5535 \n",
      "Epoch 19/29\n",
      "----------\n",
      "train Loss: 0.9401 Acc: 0.6040 \n",
      "val Loss: 1.0175 Acc: 0.5525 \n",
      "Epoch 20/29\n",
      "----------\n",
      "train Loss: 0.9399 Acc: 0.6037 \n",
      "val Loss: 1.0174 Acc: 0.5530 \n",
      "Epoch 21/29\n",
      "----------\n",
      "train Loss: 0.9397 Acc: 0.6030 \n",
      "val Loss: 1.0173 Acc: 0.5535 \n",
      "Epoch 22/29\n",
      "----------\n",
      "train Loss: 0.9395 Acc: 0.6033 \n",
      "val Loss: 1.0173 Acc: 0.5535 \n",
      "Epoch 23/29\n",
      "----------\n",
      "train Loss: 0.9393 Acc: 0.6037 \n",
      "val Loss: 1.0172 Acc: 0.5535 \n",
      "Epoch 24/29\n",
      "----------\n",
      "train Loss: 0.9391 Acc: 0.6040 \n",
      "val Loss: 1.0171 Acc: 0.5545 \n",
      "Epoch 25/29\n",
      "----------\n",
      "train Loss: 0.9388 Acc: 0.6043 \n",
      "val Loss: 1.0170 Acc: 0.5545 \n",
      "Epoch 26/29\n",
      "----------\n",
      "train Loss: 0.9386 Acc: 0.6047 \n",
      "val Loss: 1.0170 Acc: 0.5545 \n",
      "Epoch 27/29\n",
      "----------\n",
      "train Loss: 0.9384 Acc: 0.6047 \n",
      "val Loss: 1.0169 Acc: 0.5540 \n",
      "Epoch 28/29\n",
      "----------\n",
      "train Loss: 0.9382 Acc: 0.6050 \n",
      "val Loss: 1.0168 Acc: 0.5540 \n",
      "Epoch 29/29\n",
      "----------\n",
      "train Loss: 0.9380 Acc: 0.6053 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-04-10 19:14:50,651 - pyswarms.single.global_best - INFO - Optimize for 50 iters with {'c1': 2, 'c2': 2, 'w': 0.4}\n",
      "pyswarms.single.global_best:   0%|                                                                                |0/50"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val Loss: 1.0168 Acc: 0.5545 \n",
      "Training complete in 0m 22s\n",
      "Best val Acc: 0.556000\n",
      "Epoch generations ( 12 /200) :"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "pyswarms.single.global_best: 100%|██████████████████████████████████████████████████████████████|50/50, best_cost=0.942\n",
      "2020-04-10 19:26:19,385 - pyswarms.single.global_best - INFO - Optimization finished | best cost: 0.9419519465764363, best pos: [-2.20190287e+00 -3.53605652e+01 -1.92132223e+00 -2.97379375e-01\n",
      " -7.71440697e+00  3.51477885e+00  3.33797097e+00  2.55494642e+00\n",
      "  1.74390757e+00 -1.33760796e+01 -9.04526062e+01 -9.46693516e+00\n",
      "  1.51556790e+00  9.39082563e-01 -1.04979925e+01 -1.31121483e+01\n",
      " -5.34184992e-01 -2.13893936e+02  1.14219725e-01 -9.64043884e+01\n",
      " -1.51203525e+00 -1.05142965e+01 -3.09365630e+00 -2.06569713e-02\n",
      "  5.92018175e+00 -2.05465450e+01 -1.18008804e+01 -6.09999542e+01\n",
      " -2.93322773e+01  2.32017446e+00 -4.03612666e-02 -6.86412659e+01\n",
      " -3.33723515e-01 -1.17819385e+03 -6.21702385e+00 -2.52699304e+00\n",
      " -3.35665345e-01 -4.27566099e+00 -1.48931757e-01 -5.42799830e-01\n",
      " -8.47156420e-02 -7.54827881e+01 -3.52754211e+00 -3.63467751e+01\n",
      " -8.71730506e-01 -1.04211330e+00 -1.50141497e+01  2.43990231e+00\n",
      " -5.52582920e-01 -7.50522673e-01  1.79533803e+00  1.80512118e+00\n",
      " -2.42319077e-01 -9.28492606e-01  3.99455476e+00 -8.09035182e-01\n",
      " -7.80047238e-01 -1.00602751e+01 -1.72235394e+01 -3.47773075e+00\n",
      " -1.29030085e+00 -1.42433190e+00 -2.63926119e-01 -6.41810465e+00\n",
      "  8.38861704e-01 -4.53004189e+01 -4.84814882e+00 -2.47339344e+00\n",
      "  9.31613636e+00 -3.40241966e+01 -2.02330852e+00 -5.97578144e+00\n",
      " -4.21942425e+00 -9.97269154e+00 -5.45581150e+00 -1.97189942e-01\n",
      " -8.54199829e+01 -6.48663267e-02 -2.84416199e+01 -6.61087990e+00\n",
      " -1.22009935e+01  4.53118181e+00 -1.47970367e+02  1.17888653e+00\n",
      " -1.04428852e+00 -4.99989599e-01 -9.23117157e-03 -2.66349716e+01\n",
      "  1.94566035e+00  7.92495728e-01 -3.64301741e-01 -3.57230663e+00\n",
      " -1.01753359e+01 -5.44055939e-01 -3.48484278e+00 -3.03579271e-01\n",
      " -8.48349075e+01 -1.83169060e+01 -3.49606109e+00 -3.41653800e+00]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "keep prob 0.19\n",
      "Epoch 0/29\n",
      "----------\n",
      "train Loss: 0.9433 Acc: 0.6017 \n",
      "val Loss: 1.0195 Acc: 0.5555 \n",
      "Epoch 1/29\n",
      "----------\n",
      "train Loss: 0.9431 Acc: 0.6017 \n",
      "val Loss: 1.0194 Acc: 0.5550 \n",
      "Epoch 2/29\n",
      "----------\n",
      "train Loss: 0.9430 Acc: 0.6017 \n",
      "val Loss: 1.0193 Acc: 0.5545 \n",
      "Epoch 3/29\n",
      "----------\n",
      "train Loss: 0.9428 Acc: 0.6023 \n",
      "val Loss: 1.0191 Acc: 0.5550 \n",
      "Epoch 4/29\n",
      "----------\n",
      "train Loss: 0.9426 Acc: 0.6037 \n",
      "val Loss: 1.0190 Acc: 0.5560 \n",
      "Epoch 5/29\n",
      "----------\n",
      "train Loss: 0.9424 Acc: 0.6040 \n",
      "val Loss: 1.0188 Acc: 0.5555 \n",
      "Epoch 6/29\n",
      "----------\n",
      "train Loss: 0.9422 Acc: 0.6040 \n",
      "val Loss: 1.0187 Acc: 0.5555 \n",
      "Epoch 7/29\n",
      "----------\n",
      "train Loss: 0.9420 Acc: 0.6040 \n",
      "val Loss: 1.0186 Acc: 0.5560 \n",
      "Epoch 8/29\n",
      "----------\n",
      "train Loss: 0.9418 Acc: 0.6040 \n",
      "val Loss: 1.0185 Acc: 0.5560 \n",
      "Epoch 9/29\n",
      "----------\n",
      "train Loss: 0.9416 Acc: 0.6040 \n",
      "val Loss: 1.0183 Acc: 0.5550 \n",
      "Epoch 10/29\n",
      "----------\n",
      "train Loss: 0.9414 Acc: 0.6040 \n",
      "val Loss: 1.0182 Acc: 0.5555 \n",
      "Epoch 11/29\n",
      "----------\n",
      "train Loss: 0.9412 Acc: 0.6043 \n",
      "val Loss: 1.0181 Acc: 0.5540 \n",
      "Epoch 12/29\n",
      "----------\n",
      "train Loss: 0.9410 Acc: 0.6040 \n",
      "val Loss: 1.0180 Acc: 0.5540 \n",
      "Epoch 13/29\n",
      "----------\n",
      "train Loss: 0.9408 Acc: 0.6040 \n",
      "val Loss: 1.0179 Acc: 0.5540 \n",
      "Epoch 14/29\n",
      "----------\n",
      "train Loss: 0.9407 Acc: 0.6040 \n",
      "val Loss: 1.0178 Acc: 0.5535 \n",
      "Epoch 15/29\n",
      "----------\n",
      "train Loss: 0.9405 Acc: 0.6037 \n",
      "val Loss: 1.0177 Acc: 0.5535 \n",
      "Epoch 16/29\n",
      "----------\n",
      "train Loss: 0.9403 Acc: 0.6040 \n",
      "val Loss: 1.0176 Acc: 0.5535 \n",
      "Epoch 17/29\n",
      "----------\n",
      "train Loss: 0.9401 Acc: 0.6040 \n",
      "val Loss: 1.0175 Acc: 0.5530 \n",
      "Epoch 18/29\n",
      "----------\n",
      "train Loss: 0.9399 Acc: 0.6037 \n",
      "val Loss: 1.0174 Acc: 0.5525 \n",
      "Epoch 19/29\n",
      "----------\n",
      "train Loss: 0.9397 Acc: 0.6037 \n",
      "val Loss: 1.0173 Acc: 0.5530 \n",
      "Epoch 20/29\n",
      "----------\n",
      "train Loss: 0.9395 Acc: 0.6037 \n",
      "val Loss: 1.0173 Acc: 0.5535 \n",
      "Epoch 21/29\n",
      "----------\n",
      "train Loss: 0.9393 Acc: 0.6040 \n",
      "val Loss: 1.0172 Acc: 0.5540 \n",
      "Epoch 22/29\n",
      "----------\n",
      "train Loss: 0.9391 Acc: 0.6037 \n",
      "val Loss: 1.0171 Acc: 0.5540 \n",
      "Epoch 23/29\n",
      "----------\n",
      "train Loss: 0.9389 Acc: 0.6040 \n",
      "val Loss: 1.0170 Acc: 0.5550 \n",
      "Epoch 24/29\n",
      "----------\n",
      "train Loss: 0.9386 Acc: 0.6040 \n",
      "val Loss: 1.0169 Acc: 0.5545 \n",
      "Epoch 25/29\n",
      "----------\n",
      "train Loss: 0.9384 Acc: 0.6043 \n",
      "val Loss: 1.0169 Acc: 0.5540 \n",
      "Epoch 26/29\n",
      "----------\n",
      "train Loss: 0.9382 Acc: 0.6043 \n",
      "val Loss: 1.0168 Acc: 0.5540 \n",
      "Epoch 27/29\n",
      "----------\n",
      "train Loss: 0.9380 Acc: 0.6047 \n",
      "val Loss: 1.0168 Acc: 0.5545 \n",
      "Epoch 28/29\n",
      "----------\n",
      "train Loss: 0.9378 Acc: 0.6053 \n",
      "val Loss: 1.0167 Acc: 0.5545 \n",
      "Epoch 29/29\n",
      "----------\n",
      "train Loss: 0.9376 Acc: 0.6063 \n",
      "val Loss: 1.0166 Acc: 0.5540 "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-04-10 19:26:38,128 - pyswarms.single.global_best - INFO - Optimize for 50 iters with {'c1': 2, 'c2': 2, 'w': 0.4}\n",
      "pyswarms.single.global_best:   0%|                                                                                |0/50"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training complete in 0m 19s\n",
      "Best val Acc: 0.556000\n",
      "Epoch generations ( 13 /200) :"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "pyswarms.single.global_best: 100%|██████████████████████████████████████████████████████████████|50/50, best_cost=0.941\n",
      "2020-04-10 19:38:58,719 - pyswarms.single.global_best - INFO - Optimization finished | best cost: 0.9409691653251648, best pos: [-2.20190287e+00 -3.53605652e+01 -1.92132223e+00 -2.97379375e-01\n",
      " -7.71440697e+00  3.51477885e+00  3.33797097e+00  2.55494642e+00\n",
      "  1.74390757e+00 -1.33760796e+01 -9.04526062e+01 -9.46693516e+00\n",
      "  1.51556790e+00  9.39082563e-01 -1.04979925e+01 -1.31121483e+01\n",
      " -5.34184992e-01 -2.13893936e+02  1.14219725e-01 -9.64043884e+01\n",
      " -1.51203525e+00 -1.05142965e+01 -3.09365630e+00 -2.06569713e-02\n",
      "  5.92018175e+00 -2.05465450e+01 -1.18008804e+01 -6.09999542e+01\n",
      " -2.93322773e+01  2.32017446e+00 -4.03612666e-02 -6.86412659e+01\n",
      " -3.33723515e-01 -1.17819385e+03 -6.21702385e+00 -2.52699304e+00\n",
      " -3.35665345e-01 -4.27566099e+00 -1.48931757e-01 -5.42799830e-01\n",
      " -8.47156420e-02 -7.54827881e+01 -3.52754211e+00 -3.63467751e+01\n",
      " -8.71730506e-01 -1.04211330e+00 -1.50141497e+01  2.43990231e+00\n",
      " -5.52582920e-01 -7.50522673e-01  1.79533803e+00  1.80512118e+00\n",
      " -2.42319077e-01 -9.28492606e-01  3.99455476e+00 -8.09035182e-01\n",
      " -7.80047238e-01 -1.00602751e+01 -1.72235394e+01 -3.47773075e+00\n",
      " -1.29030085e+00 -1.42433190e+00 -2.63926119e-01 -6.41810465e+00\n",
      "  8.38861704e-01 -4.53004189e+01 -4.84814882e+00 -2.47339344e+00\n",
      "  9.31613636e+00 -3.40241966e+01 -2.02330852e+00 -5.97578144e+00\n",
      " -4.21942425e+00 -9.97269154e+00 -5.45581150e+00 -1.97189942e-01\n",
      " -8.54199829e+01 -6.48663267e-02 -2.84416199e+01 -6.61087990e+00\n",
      " -1.22009935e+01  4.53118181e+00 -1.47970367e+02  1.17888653e+00\n",
      " -1.04428852e+00 -4.99989599e-01 -9.23117157e-03 -2.66349716e+01\n",
      "  1.94566035e+00  7.92495728e-01 -3.64301741e-01 -3.57230663e+00\n",
      " -1.01753359e+01 -5.44055939e-01 -3.48484278e+00 -3.03579271e-01\n",
      " -8.48349075e+01 -1.83169060e+01 -3.49606109e+00 -3.41653800e+00]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "keep prob 0.19\n",
      "Epoch 0/29\n",
      "----------\n",
      "train Loss: 0.9424 Acc: 0.6040 \n",
      "val Loss: 1.0188 Acc: 0.5550 \n",
      "Epoch 1/29\n",
      "----------\n",
      "train Loss: 0.9422 Acc: 0.6040 \n",
      "val Loss: 1.0187 Acc: 0.5555 \n",
      "Epoch 2/29\n",
      "----------\n",
      "train Loss: 0.9420 Acc: 0.6043 \n",
      "val Loss: 1.0186 Acc: 0.5560 \n",
      "Epoch 3/29\n",
      "----------\n",
      "train Loss: 0.9418 Acc: 0.6040 \n",
      "val Loss: 1.0185 Acc: 0.5560 \n",
      "Epoch 4/29\n",
      "----------\n",
      "train Loss: 0.9416 Acc: 0.6040 \n",
      "val Loss: 1.0183 Acc: 0.5555 \n",
      "Epoch 5/29\n",
      "----------\n",
      "train Loss: 0.9414 Acc: 0.6043 \n",
      "val Loss: 1.0182 Acc: 0.5555 \n",
      "Epoch 6/29\n",
      "----------\n",
      "train Loss: 0.9412 Acc: 0.6040 \n",
      "val Loss: 1.0181 Acc: 0.5540 \n",
      "Epoch 7/29\n",
      "----------\n",
      "train Loss: 0.9410 Acc: 0.6040 \n",
      "val Loss: 1.0180 Acc: 0.5540 \n",
      "Epoch 8/29\n",
      "----------\n",
      "train Loss: 0.9409 Acc: 0.6040 \n",
      "val Loss: 1.0178 Acc: 0.5535 \n",
      "Epoch 9/29\n",
      "----------\n",
      "train Loss: 0.9407 Acc: 0.6043 \n",
      "val Loss: 1.0178 Acc: 0.5535 \n",
      "Epoch 10/29\n",
      "----------\n",
      "train Loss: 0.9405 Acc: 0.6033 \n",
      "val Loss: 1.0177 Acc: 0.5540 \n",
      "Epoch 11/29\n",
      "----------\n",
      "train Loss: 0.9403 Acc: 0.6040 \n",
      "val Loss: 1.0176 Acc: 0.5535 \n",
      "Epoch 12/29\n",
      "----------\n",
      "train Loss: 0.9401 Acc: 0.6040 \n",
      "val Loss: 1.0175 Acc: 0.5535 \n",
      "Epoch 13/29\n",
      "----------\n",
      "train Loss: 0.9399 Acc: 0.6033 \n",
      "val Loss: 1.0174 Acc: 0.5530 \n",
      "Epoch 14/29\n",
      "----------\n",
      "train Loss: 0.9397 Acc: 0.6037 \n",
      "val Loss: 1.0173 Acc: 0.5535 \n",
      "Epoch 15/29\n",
      "----------\n",
      "train Loss: 0.9395 Acc: 0.6030 \n",
      "val Loss: 1.0173 Acc: 0.5535 \n",
      "Epoch 16/29\n",
      "----------\n",
      "train Loss: 0.9393 Acc: 0.6037 \n",
      "val Loss: 1.0172 Acc: 0.5540 \n",
      "Epoch 17/29\n",
      "----------\n",
      "train Loss: 0.9391 Acc: 0.6037 \n",
      "val Loss: 1.0171 Acc: 0.5540 \n",
      "Epoch 18/29\n",
      "----------\n",
      "train Loss: 0.9388 Acc: 0.6040 \n",
      "val Loss: 1.0170 Acc: 0.5550 \n",
      "Epoch 19/29\n",
      "----------\n",
      "train Loss: 0.9386 Acc: 0.6043 \n",
      "val Loss: 1.0169 Acc: 0.5545 \n",
      "Epoch 20/29\n",
      "----------\n",
      "train Loss: 0.9384 Acc: 0.6040 \n",
      "val Loss: 1.0169 Acc: 0.5540 \n",
      "Epoch 21/29\n",
      "----------\n",
      "train Loss: 0.9382 Acc: 0.6047 \n",
      "val Loss: 1.0168 Acc: 0.5540 \n",
      "Epoch 22/29\n",
      "----------\n",
      "train Loss: 0.9380 Acc: 0.6050 \n",
      "val Loss: 1.0168 Acc: 0.5545 \n",
      "Epoch 23/29\n",
      "----------\n",
      "train Loss: 0.9378 Acc: 0.6057 \n",
      "val Loss: 1.0167 Acc: 0.5545 \n",
      "Epoch 24/29\n",
      "----------\n",
      "train Loss: 0.9376 Acc: 0.6060 \n",
      "val Loss: 1.0166 Acc: 0.5540 \n",
      "Epoch 25/29\n",
      "----------\n",
      "train Loss: 0.9373 Acc: 0.6063 \n",
      "val Loss: 1.0166 Acc: 0.5545 \n",
      "Epoch 26/29\n",
      "----------\n",
      "train Loss: 0.9371 Acc: 0.6063 \n",
      "val Loss: 1.0165 Acc: 0.5550 \n",
      "Epoch 27/29\n",
      "----------\n",
      "train Loss: 0.9369 Acc: 0.6067 \n",
      "val Loss: 1.0164 Acc: 0.5550 \n",
      "Epoch 28/29\n",
      "----------\n",
      "train Loss: 0.9367 Acc: 0.6077 \n",
      "val Loss: 1.0164 Acc: 0.5555 \n",
      "Epoch 29/29\n",
      "----------\n",
      "train Loss: 0.9365 Acc: 0.6083 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-04-10 19:39:20,792 - pyswarms.single.global_best - INFO - Optimize for 50 iters with {'c1': 2, 'c2': 2, 'w': 0.4}\n",
      "pyswarms.single.global_best:   0%|                                                                                |0/50"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val Loss: 1.0163 Acc: 0.5560 \n",
      "Training complete in 0m 22s\n",
      "Best val Acc: 0.556000\n",
      "Epoch generations ( 14 /200) :"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "pyswarms.single.global_best:  28%|█████████████████▋                                             |14/50, best_cost=0.94"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-7-102888c9465f>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[1;32mwhile\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mepochgens\u001b[0m\u001b[1;33m<=\u001b[0m\u001b[1;36m20\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m     \u001b[0mprint\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;34m'Epoch generations ('\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mepochgens\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m'/200)'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mend\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m' :'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 8\u001b[1;33m     \u001b[0mcost\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mmask\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0moptim\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moptimize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mf\u001b[0m\u001b[1;33m,\u001b[0m  \u001b[0miters\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m50\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      9\u001b[0m     \u001b[0mmask\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msigmoid_new\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmask\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     10\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'keep prob'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mkeep_prob\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmask\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\ana\\envs\\packt_torch\\lib\\site-packages\\pyswarms\\single\\global_best.py\u001b[0m in \u001b[0;36moptimize\u001b[1;34m(self, objective_func, iters, n_processes, **kwargs)\u001b[0m\n\u001b[0;32m    182\u001b[0m             \u001b[1;31m# Compute cost for current position and personal best\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    183\u001b[0m             \u001b[1;31m# fmt: off\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 184\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mswarm\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcurrent_cost\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcompute_objective_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mswarm\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mobjective_func\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpool\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mpool\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    185\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mswarm\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpbest_pos\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mswarm\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpbest_cost\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcompute_pbest\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mswarm\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    186\u001b[0m             \u001b[1;31m# Set best_cost_yet_found for ftol\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\ana\\envs\\packt_torch\\lib\\site-packages\\pyswarms\\backend\\operators.py\u001b[0m in \u001b[0;36mcompute_objective_function\u001b[1;34m(swarm, objective_func, pool, **kwargs)\u001b[0m\n\u001b[0;32m    237\u001b[0m     \"\"\"\n\u001b[0;32m    238\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mpool\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 239\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mobjective_func\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mswarm\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mposition\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    240\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    241\u001b[0m         results = pool.map(\n",
      "\u001b[1;32m<ipython-input-5-8ff8bf029087>\u001b[0m in \u001b[0;36mf\u001b[1;34m(x)\u001b[0m\n\u001b[0;32m     28\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     29\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 30\u001b[1;33m     \u001b[0mj\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mfitness\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnum_particles\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     31\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mj\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     32\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-5-8ff8bf029087>\u001b[0m in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m     28\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     29\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 30\u001b[1;33m     \u001b[0mj\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mfitness\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnum_particles\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     31\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mj\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     32\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-5-8ff8bf029087>\u001b[0m in \u001b[0;36mfitness\u001b[1;34m(mask)\u001b[0m\n\u001b[0;32m     11\u001b[0m             \u001b[0minputs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     12\u001b[0m             \u001b[0mlabels\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mlabels\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdevice\u001b[0m \u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 13\u001b[1;33m             \u001b[0moutputs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mForward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mmask\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mkeep_prob\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmask\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     14\u001b[0m             \u001b[0m_\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mpreds\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmax\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     15\u001b[0m             \u001b[0mloss\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcriterion\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mlabels\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-3-6fc22768d845>\u001b[0m in \u001b[0;36mForward\u001b[1;34m(self, x, mask, p)\u001b[0m\n\u001b[0;32m      7\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mForward\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mmask\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mp\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m         \u001b[1;31m#feed forward function\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 9\u001b[1;33m         \u001b[0mx\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mview\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m40\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     10\u001b[0m         \u001b[0mact1\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mrelu\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlinear1\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     11\u001b[0m         \u001b[0mact1_masked\u001b[0m \u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmasking\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mact1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mmask\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mp\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "epochgens=0\n",
    "total_loss=[]\n",
    "total_acc=[]\n",
    "mask_par=torch.bernoulli(torch.empty(num_particles,maskLength).uniform_(0,1)).numpy()\n",
    "optim = ps.single.GlobalBestPSO(n_particles=num_particles, dimensions=maskLength, options=options,init_pos=mask_par)\n",
    "while (epochgens<=20):\n",
    "    print ('Epoch generations (',epochgens,'/200)',end=' :')\n",
    "    cost,mask = optim.optimize(f,  iters=50)\n",
    "    mask=sigmoid_new(mask)\n",
    "    print('keep prob',keep_prob(mask))\n",
    "    model,losses,accuracies=train_model(model,criterion,optimizer,mask,keep_prob(mask),30)\n",
    "    total_loss=total_loss+losses\n",
    "    total_acc=total_acc+accuracies\n",
    "    epochgens+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt \n",
    "plt.plot(range(630),total_loss,label='loss')\n",
    "plt.plot(range(630),total_acc,label='accuracy')\n",
    "plt.labelegend()\n",
    "plt.xlabel('number of epochs ')\n",
    "plt.title('drop_pso')\n",
    "plt.savefig('wave 100/pso_drop.jpg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mask=torch.ones((100,)).numpy().astype('float32')\n",
    "mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0/299\n",
      "----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\ana\\envs\\packt_torch\\lib\\site-packages\\ipykernel_launcher.py:12: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  if sys.path[0] == '':\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train Loss: 1.0042 Acc: 0.5950 \n",
      "val Loss: 1.0140 Acc: 0.5660 \n",
      "Epoch 1/299\n",
      "----------\n",
      "train Loss: 1.0001 Acc: 0.5973 \n",
      "val Loss: 1.0110 Acc: 0.5690 \n",
      "Epoch 2/299\n",
      "----------\n",
      "train Loss: 0.9972 Acc: 0.5970 \n",
      "val Loss: 1.0085 Acc: 0.5695 \n",
      "Epoch 3/299\n",
      "----------\n",
      "train Loss: 0.9947 Acc: 0.5967 \n",
      "val Loss: 1.0063 Acc: 0.5725 \n",
      "Epoch 4/299\n",
      "----------\n",
      "train Loss: 0.9925 Acc: 0.5947 \n",
      "val Loss: 1.0043 Acc: 0.5740 \n",
      "Epoch 5/299\n",
      "----------\n",
      "train Loss: 0.9904 Acc: 0.5933 \n",
      "val Loss: 1.0024 Acc: 0.5730 \n",
      "Epoch 6/299\n",
      "----------\n",
      "train Loss: 0.9885 Acc: 0.5940 \n",
      "val Loss: 1.0007 Acc: 0.5735 \n",
      "Epoch 7/299\n",
      "----------\n",
      "train Loss: 0.9867 Acc: 0.5937 \n",
      "val Loss: 0.9990 Acc: 0.5740 \n",
      "Epoch 8/299\n",
      "----------\n",
      "train Loss: 0.9850 Acc: 0.5930 \n",
      "val Loss: 0.9975 Acc: 0.5730 \n",
      "Epoch 9/299\n",
      "----------\n",
      "train Loss: 0.9834 Acc: 0.5933 \n",
      "val Loss: 0.9960 Acc: 0.5730 \n",
      "Epoch 10/299\n",
      "----------\n",
      "train Loss: 0.9819 Acc: 0.5923 \n",
      "val Loss: 0.9946 Acc: 0.5740 \n",
      "Epoch 11/299\n",
      "----------\n",
      "train Loss: 0.9805 Acc: 0.5917 \n",
      "val Loss: 0.9933 Acc: 0.5755 \n",
      "Epoch 12/299\n",
      "----------\n",
      "train Loss: 0.9792 Acc: 0.5910 \n",
      "val Loss: 0.9921 Acc: 0.5765 \n",
      "Epoch 13/299\n",
      "----------\n",
      "train Loss: 0.9780 Acc: 0.5900 \n",
      "val Loss: 0.9910 Acc: 0.5775 \n",
      "Epoch 14/299\n",
      "----------\n",
      "train Loss: 0.9768 Acc: 0.5897 \n",
      "val Loss: 0.9899 Acc: 0.5755 \n",
      "Epoch 15/299\n",
      "----------\n",
      "train Loss: 0.9757 Acc: 0.5890 \n",
      "val Loss: 0.9888 Acc: 0.5765 \n",
      "Epoch 16/299\n",
      "----------\n",
      "train Loss: 0.9747 Acc: 0.5890 \n",
      "val Loss: 0.9879 Acc: 0.5765 \n",
      "Epoch 17/299\n",
      "----------\n",
      "train Loss: 0.9737 Acc: 0.5900 \n",
      "val Loss: 0.9869 Acc: 0.5765 \n",
      "Epoch 18/299\n",
      "----------\n",
      "train Loss: 0.9728 Acc: 0.5893 \n",
      "val Loss: 0.9861 Acc: 0.5760 \n",
      "Epoch 19/299\n",
      "----------\n",
      "train Loss: 0.9719 Acc: 0.5890 \n",
      "val Loss: 0.9853 Acc: 0.5770 \n",
      "Epoch 20/299\n",
      "----------\n",
      "train Loss: 0.9711 Acc: 0.5887 \n",
      "val Loss: 0.9845 Acc: 0.5770 \n",
      "Epoch 21/299\n",
      "----------\n",
      "train Loss: 0.9703 Acc: 0.5883 \n",
      "val Loss: 0.9837 Acc: 0.5770 \n",
      "Epoch 22/299\n",
      "----------\n",
      "train Loss: 0.9695 Acc: 0.5880 \n",
      "val Loss: 0.9830 Acc: 0.5770 \n",
      "Epoch 23/299\n",
      "----------\n",
      "train Loss: 0.9688 Acc: 0.5883 \n",
      "val Loss: 0.9824 Acc: 0.5775 \n",
      "Epoch 24/299\n",
      "----------\n",
      "train Loss: 0.9682 Acc: 0.5880 \n",
      "val Loss: 0.9817 Acc: 0.5775 \n",
      "Epoch 25/299\n",
      "----------\n",
      "train Loss: 0.9675 Acc: 0.5877 \n",
      "val Loss: 0.9811 Acc: 0.5785 \n",
      "Epoch 26/299\n",
      "----------\n",
      "train Loss: 0.9669 Acc: 0.5877 \n",
      "val Loss: 0.9806 Acc: 0.5785 \n",
      "Epoch 27/299\n",
      "----------\n",
      "train Loss: 0.9663 Acc: 0.5877 \n",
      "val Loss: 0.9800 Acc: 0.5785 \n",
      "Epoch 28/299\n",
      "----------\n",
      "train Loss: 0.9657 Acc: 0.5880 \n",
      "val Loss: 0.9795 Acc: 0.5790 \n",
      "Epoch 29/299\n",
      "----------\n",
      "train Loss: 0.9652 Acc: 0.5880 \n",
      "val Loss: 0.9790 Acc: 0.5790 \n",
      "Epoch 30/299\n",
      "----------\n",
      "train Loss: 0.9647 Acc: 0.5880 \n",
      "val Loss: 0.9785 Acc: 0.5790 \n",
      "Epoch 31/299\n",
      "----------\n",
      "train Loss: 0.9642 Acc: 0.5880 \n",
      "val Loss: 0.9781 Acc: 0.5790 \n",
      "Epoch 32/299\n",
      "----------\n",
      "train Loss: 0.9637 Acc: 0.5880 \n",
      "val Loss: 0.9777 Acc: 0.5790 \n",
      "Epoch 33/299\n",
      "----------\n",
      "train Loss: 0.9633 Acc: 0.5883 \n",
      "val Loss: 0.9773 Acc: 0.5785 \n",
      "Epoch 34/299\n",
      "----------\n",
      "train Loss: 0.9629 Acc: 0.5883 \n",
      "val Loss: 0.9769 Acc: 0.5785 \n",
      "Epoch 35/299\n",
      "----------\n",
      "train Loss: 0.9624 Acc: 0.5880 \n",
      "val Loss: 0.9765 Acc: 0.5785 \n",
      "Epoch 36/299\n",
      "----------\n",
      "train Loss: 0.9620 Acc: 0.5883 \n",
      "val Loss: 0.9761 Acc: 0.5785 \n",
      "Epoch 37/299\n",
      "----------\n",
      "train Loss: 0.9616 Acc: 0.5887 \n",
      "val Loss: 0.9758 Acc: 0.5785 \n",
      "Epoch 38/299\n",
      "----------\n",
      "train Loss: 0.9613 Acc: 0.5887 \n",
      "val Loss: 0.9755 Acc: 0.5785 \n",
      "Epoch 39/299\n",
      "----------\n",
      "train Loss: 0.9609 Acc: 0.5887 \n",
      "val Loss: 0.9752 Acc: 0.5785 \n",
      "Epoch 40/299\n",
      "----------\n",
      "train Loss: 0.9606 Acc: 0.5887 \n",
      "val Loss: 0.9749 Acc: 0.5785 \n",
      "Epoch 41/299\n",
      "----------\n",
      "train Loss: 0.9602 Acc: 0.5887 \n",
      "val Loss: 0.9746 Acc: 0.5780 \n",
      "Epoch 42/299\n",
      "----------\n",
      "train Loss: 0.9599 Acc: 0.5890 \n",
      "val Loss: 0.9743 Acc: 0.5780 \n",
      "Epoch 43/299\n",
      "----------\n",
      "train Loss: 0.9596 Acc: 0.5890 \n",
      "val Loss: 0.9741 Acc: 0.5785 \n",
      "Epoch 44/299\n",
      "----------\n",
      "train Loss: 0.9593 Acc: 0.5890 \n",
      "val Loss: 0.9738 Acc: 0.5790 \n",
      "Epoch 45/299\n",
      "----------\n",
      "train Loss: 0.9590 Acc: 0.5890 \n",
      "val Loss: 0.9736 Acc: 0.5790 \n",
      "Epoch 46/299\n",
      "----------\n",
      "train Loss: 0.9587 Acc: 0.5890 \n",
      "val Loss: 0.9734 Acc: 0.5790 \n",
      "Epoch 47/299\n",
      "----------\n",
      "train Loss: 0.9584 Acc: 0.5887 \n",
      "val Loss: 0.9732 Acc: 0.5785 \n",
      "Epoch 48/299\n",
      "----------\n",
      "train Loss: 0.9582 Acc: 0.5887 \n",
      "val Loss: 0.9730 Acc: 0.5785 \n",
      "Epoch 49/299\n",
      "----------\n",
      "train Loss: 0.9579 Acc: 0.5887 \n",
      "val Loss: 0.9728 Acc: 0.5785 \n",
      "Epoch 50/299\n",
      "----------\n",
      "train Loss: 0.9577 Acc: 0.5883 \n",
      "val Loss: 0.9726 Acc: 0.5785 \n",
      "Epoch 51/299\n",
      "----------\n",
      "train Loss: 0.9574 Acc: 0.5887 \n",
      "val Loss: 0.9724 Acc: 0.5790 \n",
      "Epoch 52/299\n",
      "----------\n",
      "train Loss: 0.9572 Acc: 0.5883 \n",
      "val Loss: 0.9722 Acc: 0.5790 \n",
      "Epoch 53/299\n",
      "----------\n",
      "train Loss: 0.9570 Acc: 0.5887 \n",
      "val Loss: 0.9720 Acc: 0.5790 \n",
      "Epoch 54/299\n",
      "----------\n",
      "train Loss: 0.9567 Acc: 0.5890 \n",
      "val Loss: 0.9719 Acc: 0.5790 \n",
      "Epoch 55/299\n",
      "----------\n",
      "train Loss: 0.9565 Acc: 0.5890 \n",
      "val Loss: 0.9717 Acc: 0.5785 \n",
      "Epoch 56/299\n",
      "----------\n",
      "train Loss: 0.9563 Acc: 0.5890 \n",
      "val Loss: 0.9716 Acc: 0.5775 \n",
      "Epoch 57/299\n",
      "----------\n",
      "train Loss: 0.9561 Acc: 0.5890 \n",
      "val Loss: 0.9715 Acc: 0.5775 \n",
      "Epoch 58/299\n",
      "----------\n",
      "train Loss: 0.9559 Acc: 0.5890 \n",
      "val Loss: 0.9713 Acc: 0.5765 \n",
      "Epoch 59/299\n",
      "----------\n",
      "train Loss: 0.9557 Acc: 0.5893 \n",
      "val Loss: 0.9712 Acc: 0.5765 \n",
      "Epoch 60/299\n",
      "----------\n",
      "train Loss: 0.9555 Acc: 0.5897 \n",
      "val Loss: 0.9711 Acc: 0.5765 \n",
      "Epoch 61/299\n",
      "----------\n",
      "train Loss: 0.9553 Acc: 0.5897 \n",
      "val Loss: 0.9709 Acc: 0.5765 \n",
      "Epoch 62/299\n",
      "----------\n",
      "train Loss: 0.9551 Acc: 0.5897 \n",
      "val Loss: 0.9708 Acc: 0.5765 \n",
      "Epoch 63/299\n",
      "----------\n",
      "train Loss: 0.9549 Acc: 0.5897 \n",
      "val Loss: 0.9707 Acc: 0.5765 \n",
      "Epoch 64/299\n",
      "----------\n",
      "train Loss: 0.9547 Acc: 0.5897 \n",
      "val Loss: 0.9706 Acc: 0.5765 \n",
      "Epoch 65/299\n",
      "----------\n",
      "train Loss: 0.9546 Acc: 0.5900 \n",
      "val Loss: 0.9705 Acc: 0.5765 \n",
      "Epoch 66/299\n",
      "----------\n",
      "train Loss: 0.9544 Acc: 0.5903 \n",
      "val Loss: 0.9704 Acc: 0.5765 \n",
      "Epoch 67/299\n",
      "----------\n",
      "train Loss: 0.9542 Acc: 0.5910 \n",
      "val Loss: 0.9703 Acc: 0.5760 \n",
      "Epoch 68/299\n",
      "----------\n",
      "train Loss: 0.9541 Acc: 0.5910 \n",
      "val Loss: 0.9702 Acc: 0.5760 \n",
      "Epoch 69/299\n",
      "----------\n",
      "train Loss: 0.9539 Acc: 0.5917 \n",
      "val Loss: 0.9702 Acc: 0.5760 \n",
      "Epoch 70/299\n",
      "----------\n",
      "train Loss: 0.9537 Acc: 0.5923 \n",
      "val Loss: 0.9701 Acc: 0.5760 \n",
      "Epoch 71/299\n",
      "----------\n",
      "train Loss: 0.9536 Acc: 0.5923 \n",
      "val Loss: 0.9700 Acc: 0.5760 \n",
      "Epoch 72/299\n",
      "----------\n",
      "train Loss: 0.9534 Acc: 0.5923 \n",
      "val Loss: 0.9699 Acc: 0.5770 \n",
      "Epoch 73/299\n",
      "----------\n",
      "train Loss: 0.9533 Acc: 0.5923 \n",
      "val Loss: 0.9698 Acc: 0.5770 \n",
      "Epoch 74/299\n",
      "----------\n",
      "train Loss: 0.9531 Acc: 0.5923 \n",
      "val Loss: 0.9698 Acc: 0.5770 \n",
      "Epoch 75/299\n",
      "----------\n",
      "train Loss: 0.9530 Acc: 0.5927 \n",
      "val Loss: 0.9697 Acc: 0.5770 \n",
      "Epoch 76/299\n",
      "----------\n",
      "train Loss: 0.9528 Acc: 0.5930 \n",
      "val Loss: 0.9696 Acc: 0.5770 \n",
      "Epoch 77/299\n",
      "----------\n",
      "train Loss: 0.9527 Acc: 0.5930 \n",
      "val Loss: 0.9696 Acc: 0.5770 \n",
      "Epoch 78/299\n",
      "----------\n",
      "train Loss: 0.9526 Acc: 0.5930 \n",
      "val Loss: 0.9695 Acc: 0.5760 \n",
      "Epoch 79/299\n",
      "----------\n",
      "train Loss: 0.9524 Acc: 0.5930 \n",
      "val Loss: 0.9695 Acc: 0.5765 \n",
      "Epoch 80/299\n",
      "----------\n",
      "train Loss: 0.9523 Acc: 0.5927 \n",
      "val Loss: 0.9694 Acc: 0.5765 \n",
      "Epoch 81/299\n",
      "----------\n",
      "train Loss: 0.9522 Acc: 0.5933 \n",
      "val Loss: 0.9694 Acc: 0.5765 \n",
      "Epoch 82/299\n",
      "----------\n",
      "train Loss: 0.9520 Acc: 0.5933 \n",
      "val Loss: 0.9693 Acc: 0.5765 \n",
      "Epoch 83/299\n",
      "----------\n",
      "train Loss: 0.9519 Acc: 0.5933 \n",
      "val Loss: 0.9693 Acc: 0.5765 \n",
      "Epoch 84/299\n",
      "----------\n",
      "train Loss: 0.9518 Acc: 0.5933 \n",
      "val Loss: 0.9692 Acc: 0.5765 \n",
      "Epoch 85/299\n",
      "----------\n",
      "train Loss: 0.9516 Acc: 0.5933 \n",
      "val Loss: 0.9692 Acc: 0.5760 \n",
      "Epoch 86/299\n",
      "----------\n",
      "train Loss: 0.9515 Acc: 0.5937 \n",
      "val Loss: 0.9692 Acc: 0.5760 \n",
      "Epoch 87/299\n",
      "----------\n",
      "train Loss: 0.9514 Acc: 0.5933 \n",
      "val Loss: 0.9691 Acc: 0.5760 \n",
      "Epoch 88/299\n",
      "----------\n",
      "train Loss: 0.9513 Acc: 0.5930 \n",
      "val Loss: 0.9691 Acc: 0.5765 \n",
      "Epoch 89/299\n",
      "----------\n",
      "train Loss: 0.9512 Acc: 0.5930 \n",
      "val Loss: 0.9690 Acc: 0.5760 \n",
      "Epoch 90/299\n",
      "----------\n",
      "train Loss: 0.9510 Acc: 0.5930 \n",
      "val Loss: 0.9690 Acc: 0.5760 \n",
      "Epoch 91/299\n",
      "----------\n",
      "train Loss: 0.9509 Acc: 0.5933 \n",
      "val Loss: 0.9690 Acc: 0.5760 \n",
      "Epoch 92/299\n",
      "----------\n",
      "train Loss: 0.9508 Acc: 0.5937 \n",
      "val Loss: 0.9689 Acc: 0.5765 \n",
      "Epoch 93/299\n",
      "----------\n",
      "train Loss: 0.9507 Acc: 0.5940 \n",
      "val Loss: 0.9689 Acc: 0.5765 \n",
      "Epoch 94/299\n",
      "----------\n",
      "train Loss: 0.9506 Acc: 0.5943 \n",
      "val Loss: 0.9689 Acc: 0.5765 \n",
      "Epoch 95/299\n",
      "----------\n",
      "train Loss: 0.9505 Acc: 0.5943 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val Loss: 0.9688 Acc: 0.5775 \n",
      "Epoch 96/299\n",
      "----------\n",
      "train Loss: 0.9504 Acc: 0.5943 \n",
      "val Loss: 0.9688 Acc: 0.5770 \n",
      "Epoch 97/299\n",
      "----------\n",
      "train Loss: 0.9502 Acc: 0.5943 \n",
      "val Loss: 0.9688 Acc: 0.5775 \n",
      "Epoch 98/299\n",
      "----------\n",
      "train Loss: 0.9501 Acc: 0.5943 \n",
      "val Loss: 0.9688 Acc: 0.5770 \n",
      "Epoch 99/299\n",
      "----------\n",
      "train Loss: 0.9500 Acc: 0.5947 \n",
      "val Loss: 0.9687 Acc: 0.5770 \n",
      "Epoch 100/299\n",
      "----------\n",
      "train Loss: 0.9499 Acc: 0.5947 \n",
      "val Loss: 0.9687 Acc: 0.5770 \n",
      "Epoch 101/299\n",
      "----------\n",
      "train Loss: 0.9498 Acc: 0.5947 \n",
      "val Loss: 0.9687 Acc: 0.5765 \n",
      "Epoch 102/299\n",
      "----------\n",
      "train Loss: 0.9497 Acc: 0.5950 \n",
      "val Loss: 0.9687 Acc: 0.5750 \n",
      "Epoch 103/299\n",
      "----------\n",
      "train Loss: 0.9496 Acc: 0.5950 \n",
      "val Loss: 0.9687 Acc: 0.5750 \n",
      "Epoch 104/299\n",
      "----------\n",
      "train Loss: 0.9495 Acc: 0.5950 \n",
      "val Loss: 0.9686 Acc: 0.5750 \n",
      "Epoch 105/299\n",
      "----------\n",
      "train Loss: 0.9494 Acc: 0.5950 \n",
      "val Loss: 0.9686 Acc: 0.5750 \n",
      "Epoch 106/299\n",
      "----------\n",
      "train Loss: 0.9493 Acc: 0.5947 \n",
      "val Loss: 0.9686 Acc: 0.5745 \n",
      "Epoch 107/299\n",
      "----------\n",
      "train Loss: 0.9492 Acc: 0.5943 \n",
      "val Loss: 0.9686 Acc: 0.5745 \n",
      "Epoch 108/299\n",
      "----------\n",
      "train Loss: 0.9491 Acc: 0.5943 \n",
      "val Loss: 0.9686 Acc: 0.5740 \n",
      "Epoch 109/299\n",
      "----------\n",
      "train Loss: 0.9490 Acc: 0.5937 \n",
      "val Loss: 0.9686 Acc: 0.5735 \n",
      "Epoch 110/299\n",
      "----------\n",
      "train Loss: 0.9489 Acc: 0.5937 \n",
      "val Loss: 0.9685 Acc: 0.5735 \n",
      "Epoch 111/299\n",
      "----------\n",
      "train Loss: 0.9488 Acc: 0.5937 \n",
      "val Loss: 0.9685 Acc: 0.5735 \n",
      "Epoch 112/299\n",
      "----------\n",
      "train Loss: 0.9487 Acc: 0.5940 \n",
      "val Loss: 0.9685 Acc: 0.5735 \n",
      "Epoch 113/299\n",
      "----------\n",
      "train Loss: 0.9487 Acc: 0.5947 \n",
      "val Loss: 0.9685 Acc: 0.5730 \n",
      "Epoch 114/299\n",
      "----------\n",
      "train Loss: 0.9486 Acc: 0.5943 \n",
      "val Loss: 0.9685 Acc: 0.5725 \n",
      "Epoch 115/299\n",
      "----------\n",
      "train Loss: 0.9485 Acc: 0.5940 \n",
      "val Loss: 0.9685 Acc: 0.5725 \n",
      "Epoch 116/299\n",
      "----------\n",
      "train Loss: 0.9484 Acc: 0.5940 \n",
      "val Loss: 0.9685 Acc: 0.5725 \n",
      "Epoch 117/299\n",
      "----------\n",
      "train Loss: 0.9483 Acc: 0.5940 \n",
      "val Loss: 0.9685 Acc: 0.5730 \n",
      "Epoch 118/299\n",
      "----------\n",
      "train Loss: 0.9482 Acc: 0.5943 \n",
      "val Loss: 0.9684 Acc: 0.5735 \n",
      "Epoch 119/299\n",
      "----------\n",
      "train Loss: 0.9481 Acc: 0.5947 \n",
      "val Loss: 0.9684 Acc: 0.5735 \n",
      "Epoch 120/299\n",
      "----------\n",
      "train Loss: 0.9480 Acc: 0.5943 \n",
      "val Loss: 0.9684 Acc: 0.5735 \n",
      "Epoch 121/299\n",
      "----------\n",
      "train Loss: 0.9479 Acc: 0.5943 \n",
      "val Loss: 0.9684 Acc: 0.5735 \n",
      "Epoch 122/299\n",
      "----------\n",
      "train Loss: 0.9478 Acc: 0.5943 \n",
      "val Loss: 0.9684 Acc: 0.5735 \n",
      "Epoch 123/299\n",
      "----------\n",
      "train Loss: 0.9478 Acc: 0.5943 \n",
      "val Loss: 0.9684 Acc: 0.5730 \n",
      "Epoch 124/299\n",
      "----------\n",
      "train Loss: 0.9477 Acc: 0.5940 \n",
      "val Loss: 0.9684 Acc: 0.5730 \n",
      "Epoch 125/299\n",
      "----------\n",
      "train Loss: 0.9476 Acc: 0.5940 \n",
      "val Loss: 0.9684 Acc: 0.5730 \n",
      "Epoch 126/299\n",
      "----------\n",
      "train Loss: 0.9475 Acc: 0.5943 \n",
      "val Loss: 0.9684 Acc: 0.5730 \n",
      "Epoch 127/299\n",
      "----------\n",
      "train Loss: 0.9474 Acc: 0.5943 \n",
      "val Loss: 0.9684 Acc: 0.5730 \n",
      "Epoch 128/299\n",
      "----------\n",
      "train Loss: 0.9473 Acc: 0.5943 \n",
      "val Loss: 0.9684 Acc: 0.5730 \n",
      "Epoch 129/299\n",
      "----------\n",
      "train Loss: 0.9473 Acc: 0.5947 \n",
      "val Loss: 0.9684 Acc: 0.5730 \n",
      "Epoch 130/299\n",
      "----------\n",
      "train Loss: 0.9472 Acc: 0.5947 \n",
      "val Loss: 0.9684 Acc: 0.5730 \n",
      "Epoch 131/299\n",
      "----------\n",
      "train Loss: 0.9471 Acc: 0.5950 \n",
      "val Loss: 0.9684 Acc: 0.5730 \n",
      "Epoch 132/299\n",
      "----------\n",
      "train Loss: 0.9470 Acc: 0.5950 \n",
      "val Loss: 0.9684 Acc: 0.5730 \n",
      "Epoch 133/299\n",
      "----------\n",
      "train Loss: 0.9469 Acc: 0.5950 \n",
      "val Loss: 0.9684 Acc: 0.5725 \n",
      "Epoch 134/299\n",
      "----------\n",
      "train Loss: 0.9469 Acc: 0.5947 \n",
      "val Loss: 0.9683 Acc: 0.5725 \n",
      "Epoch 135/299\n",
      "----------\n",
      "train Loss: 0.9468 Acc: 0.5947 \n",
      "val Loss: 0.9683 Acc: 0.5730 \n",
      "Epoch 136/299\n",
      "----------\n",
      "train Loss: 0.9467 Acc: 0.5947 \n",
      "val Loss: 0.9683 Acc: 0.5730 \n",
      "Epoch 137/299\n",
      "----------\n",
      "train Loss: 0.9466 Acc: 0.5947 \n",
      "val Loss: 0.9683 Acc: 0.5730 \n",
      "Epoch 138/299\n",
      "----------\n",
      "train Loss: 0.9465 Acc: 0.5947 \n",
      "val Loss: 0.9683 Acc: 0.5735 \n",
      "Epoch 139/299\n",
      "----------\n",
      "train Loss: 0.9465 Acc: 0.5947 \n",
      "val Loss: 0.9683 Acc: 0.5740 \n",
      "Epoch 140/299\n",
      "----------\n",
      "train Loss: 0.9464 Acc: 0.5950 \n",
      "val Loss: 0.9683 Acc: 0.5745 \n",
      "Epoch 141/299\n",
      "----------\n",
      "train Loss: 0.9463 Acc: 0.5953 \n",
      "val Loss: 0.9683 Acc: 0.5745 \n",
      "Epoch 142/299\n",
      "----------\n",
      "train Loss: 0.9462 Acc: 0.5953 \n",
      "val Loss: 0.9683 Acc: 0.5745 \n",
      "Epoch 143/299\n",
      "----------\n",
      "train Loss: 0.9462 Acc: 0.5957 \n",
      "val Loss: 0.9683 Acc: 0.5745 \n",
      "Epoch 144/299\n",
      "----------\n",
      "train Loss: 0.9461 Acc: 0.5960 \n",
      "val Loss: 0.9683 Acc: 0.5745 \n",
      "Epoch 145/299\n",
      "----------\n",
      "train Loss: 0.9460 Acc: 0.5960 \n",
      "val Loss: 0.9683 Acc: 0.5745 \n",
      "Epoch 146/299\n",
      "----------\n",
      "train Loss: 0.9459 Acc: 0.5967 \n",
      "val Loss: 0.9683 Acc: 0.5735 \n",
      "Epoch 147/299\n",
      "----------\n",
      "train Loss: 0.9459 Acc: 0.5970 \n",
      "val Loss: 0.9683 Acc: 0.5735 \n",
      "Epoch 148/299\n",
      "----------\n",
      "train Loss: 0.9458 Acc: 0.5970 \n",
      "val Loss: 0.9683 Acc: 0.5735 \n",
      "Epoch 149/299\n",
      "----------\n",
      "train Loss: 0.9457 Acc: 0.5970 \n",
      "val Loss: 0.9683 Acc: 0.5735 \n",
      "Epoch 150/299\n",
      "----------\n",
      "train Loss: 0.9456 Acc: 0.5973 \n",
      "val Loss: 0.9683 Acc: 0.5735 \n",
      "Epoch 151/299\n",
      "----------\n",
      "train Loss: 0.9456 Acc: 0.5973 \n",
      "val Loss: 0.9683 Acc: 0.5740 \n",
      "Epoch 152/299\n",
      "----------\n",
      "train Loss: 0.9455 Acc: 0.5973 \n",
      "val Loss: 0.9683 Acc: 0.5740 \n",
      "Epoch 153/299\n",
      "----------\n",
      "train Loss: 0.9454 Acc: 0.5973 \n",
      "val Loss: 0.9683 Acc: 0.5740 \n",
      "Epoch 154/299\n",
      "----------\n",
      "train Loss: 0.9454 Acc: 0.5973 \n",
      "val Loss: 0.9683 Acc: 0.5740 \n",
      "Epoch 155/299\n",
      "----------\n",
      "train Loss: 0.9453 Acc: 0.5977 \n",
      "val Loss: 0.9683 Acc: 0.5740 \n",
      "Epoch 156/299\n",
      "----------\n",
      "train Loss: 0.9452 Acc: 0.5977 \n",
      "val Loss: 0.9683 Acc: 0.5735 \n",
      "Epoch 157/299\n",
      "----------\n",
      "train Loss: 0.9451 Acc: 0.5977 \n",
      "val Loss: 0.9683 Acc: 0.5735 \n",
      "Epoch 158/299\n",
      "----------\n",
      "train Loss: 0.9451 Acc: 0.5980 \n",
      "val Loss: 0.9683 Acc: 0.5730 \n",
      "Epoch 159/299\n",
      "----------\n",
      "train Loss: 0.9450 Acc: 0.5980 \n",
      "val Loss: 0.9683 Acc: 0.5735 \n",
      "Epoch 160/299\n",
      "----------\n",
      "train Loss: 0.9449 Acc: 0.5980 \n",
      "val Loss: 0.9683 Acc: 0.5735 \n",
      "Epoch 161/299\n",
      "----------\n",
      "train Loss: 0.9449 Acc: 0.5980 \n",
      "val Loss: 0.9683 Acc: 0.5735 \n",
      "Epoch 162/299\n",
      "----------\n",
      "train Loss: 0.9448 Acc: 0.5980 \n",
      "val Loss: 0.9683 Acc: 0.5735 \n",
      "Epoch 163/299\n",
      "----------\n",
      "train Loss: 0.9447 Acc: 0.5980 \n",
      "val Loss: 0.9683 Acc: 0.5740 \n",
      "Epoch 164/299\n",
      "----------\n",
      "train Loss: 0.9446 Acc: 0.5983 \n",
      "val Loss: 0.9683 Acc: 0.5740 \n",
      "Epoch 165/299\n",
      "----------\n",
      "train Loss: 0.9446 Acc: 0.5983 \n",
      "val Loss: 0.9683 Acc: 0.5740 \n",
      "Epoch 166/299\n",
      "----------\n",
      "train Loss: 0.9445 Acc: 0.5983 \n",
      "val Loss: 0.9683 Acc: 0.5740 \n",
      "Epoch 167/299\n",
      "----------\n",
      "train Loss: 0.9444 Acc: 0.5983 \n",
      "val Loss: 0.9683 Acc: 0.5740 \n",
      "Epoch 168/299\n",
      "----------\n",
      "train Loss: 0.9444 Acc: 0.5983 \n",
      "val Loss: 0.9684 Acc: 0.5740 \n",
      "Epoch 169/299\n",
      "----------\n",
      "train Loss: 0.9443 Acc: 0.5987 \n",
      "val Loss: 0.9684 Acc: 0.5740 \n",
      "Epoch 170/299\n",
      "----------\n",
      "train Loss: 0.9442 Acc: 0.5987 \n",
      "val Loss: 0.9684 Acc: 0.5735 \n",
      "Epoch 171/299\n",
      "----------\n",
      "train Loss: 0.9441 Acc: 0.5990 \n",
      "val Loss: 0.9684 Acc: 0.5730 \n",
      "Epoch 172/299\n",
      "----------\n",
      "train Loss: 0.9441 Acc: 0.5990 \n",
      "val Loss: 0.9684 Acc: 0.5730 \n",
      "Epoch 173/299\n",
      "----------\n",
      "train Loss: 0.9440 Acc: 0.5990 \n",
      "val Loss: 0.9684 Acc: 0.5725 \n",
      "Epoch 174/299\n",
      "----------\n",
      "train Loss: 0.9439 Acc: 0.5990 \n",
      "val Loss: 0.9684 Acc: 0.5725 \n",
      "Epoch 175/299\n",
      "----------\n",
      "train Loss: 0.9439 Acc: 0.5990 \n",
      "val Loss: 0.9684 Acc: 0.5725 \n",
      "Epoch 176/299\n",
      "----------\n",
      "train Loss: 0.9438 Acc: 0.5987 \n",
      "val Loss: 0.9684 Acc: 0.5725 \n",
      "Epoch 177/299\n",
      "----------\n",
      "train Loss: 0.9437 Acc: 0.5987 \n",
      "val Loss: 0.9684 Acc: 0.5725 \n",
      "Epoch 178/299\n",
      "----------\n",
      "train Loss: 0.9437 Acc: 0.5990 \n",
      "val Loss: 0.9684 Acc: 0.5730 \n",
      "Epoch 179/299\n",
      "----------\n",
      "train Loss: 0.9436 Acc: 0.5990 \n",
      "val Loss: 0.9684 Acc: 0.5730 \n",
      "Epoch 180/299\n",
      "----------\n",
      "train Loss: 0.9435 Acc: 0.5993 \n",
      "val Loss: 0.9684 Acc: 0.5725 \n",
      "Epoch 181/299\n",
      "----------\n",
      "train Loss: 0.9435 Acc: 0.5993 \n",
      "val Loss: 0.9684 Acc: 0.5725 \n",
      "Epoch 182/299\n",
      "----------\n",
      "train Loss: 0.9434 Acc: 0.5993 \n",
      "val Loss: 0.9684 Acc: 0.5725 \n",
      "Epoch 183/299\n",
      "----------\n",
      "train Loss: 0.9433 Acc: 0.5993 \n",
      "val Loss: 0.9684 Acc: 0.5725 \n",
      "Epoch 184/299\n",
      "----------\n",
      "train Loss: 0.9433 Acc: 0.5997 \n",
      "val Loss: 0.9684 Acc: 0.5730 \n",
      "Epoch 185/299\n",
      "----------\n",
      "train Loss: 0.9432 Acc: 0.5997 \n",
      "val Loss: 0.9684 Acc: 0.5730 \n",
      "Epoch 186/299\n",
      "----------\n",
      "train Loss: 0.9431 Acc: 0.5997 \n",
      "val Loss: 0.9684 Acc: 0.5725 \n",
      "Epoch 187/299\n",
      "----------\n",
      "train Loss: 0.9431 Acc: 0.5997 \n",
      "val Loss: 0.9684 Acc: 0.5725 \n",
      "Epoch 188/299\n",
      "----------\n",
      "train Loss: 0.9430 Acc: 0.5997 \n",
      "val Loss: 0.9684 Acc: 0.5720 \n",
      "Epoch 189/299\n",
      "----------\n",
      "train Loss: 0.9429 Acc: 0.5997 \n",
      "val Loss: 0.9684 Acc: 0.5720 \n",
      "Epoch 190/299\n",
      "----------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train Loss: 0.9429 Acc: 0.5997 \n",
      "val Loss: 0.9684 Acc: 0.5720 \n",
      "Epoch 191/299\n",
      "----------\n",
      "train Loss: 0.9428 Acc: 0.6000 \n",
      "val Loss: 0.9684 Acc: 0.5720 \n",
      "Epoch 192/299\n",
      "----------\n",
      "train Loss: 0.9427 Acc: 0.6000 \n",
      "val Loss: 0.9684 Acc: 0.5720 \n",
      "Epoch 193/299\n",
      "----------\n",
      "train Loss: 0.9427 Acc: 0.5997 \n",
      "val Loss: 0.9684 Acc: 0.5720 \n",
      "Epoch 194/299\n",
      "----------\n",
      "train Loss: 0.9426 Acc: 0.5997 \n",
      "val Loss: 0.9684 Acc: 0.5725 \n",
      "Epoch 195/299\n",
      "----------\n",
      "train Loss: 0.9425 Acc: 0.5997 \n",
      "val Loss: 0.9684 Acc: 0.5725 \n",
      "Epoch 196/299\n",
      "----------\n",
      "train Loss: 0.9425 Acc: 0.6000 \n",
      "val Loss: 0.9685 Acc: 0.5720 \n",
      "Epoch 197/299\n",
      "----------\n",
      "train Loss: 0.9424 Acc: 0.6000 \n",
      "val Loss: 0.9685 Acc: 0.5725 \n",
      "Epoch 198/299\n",
      "----------\n",
      "train Loss: 0.9423 Acc: 0.6000 \n",
      "val Loss: 0.9685 Acc: 0.5730 \n",
      "Epoch 199/299\n",
      "----------\n",
      "train Loss: 0.9423 Acc: 0.6000 \n",
      "val Loss: 0.9685 Acc: 0.5730 \n",
      "Epoch 200/299\n",
      "----------\n",
      "train Loss: 0.9422 Acc: 0.6007 \n",
      "val Loss: 0.9685 Acc: 0.5730 \n",
      "Epoch 201/299\n",
      "----------\n",
      "train Loss: 0.9421 Acc: 0.6007 \n",
      "val Loss: 0.9685 Acc: 0.5740 \n",
      "Epoch 202/299\n",
      "----------\n",
      "train Loss: 0.9421 Acc: 0.6007 \n",
      "val Loss: 0.9685 Acc: 0.5740 \n",
      "Epoch 203/299\n",
      "----------\n",
      "train Loss: 0.9420 Acc: 0.6010 \n",
      "val Loss: 0.9685 Acc: 0.5735 \n",
      "Epoch 204/299\n",
      "----------\n",
      "train Loss: 0.9419 Acc: 0.6010 \n",
      "val Loss: 0.9685 Acc: 0.5740 \n",
      "Epoch 205/299\n",
      "----------\n",
      "train Loss: 0.9419 Acc: 0.6010 \n",
      "val Loss: 0.9685 Acc: 0.5740 \n",
      "Epoch 206/299\n",
      "----------\n",
      "train Loss: 0.9418 Acc: 0.6007 \n",
      "val Loss: 0.9685 Acc: 0.5740 \n",
      "Epoch 207/299\n",
      "----------\n",
      "train Loss: 0.9417 Acc: 0.6007 \n",
      "val Loss: 0.9685 Acc: 0.5740 \n",
      "Epoch 208/299\n",
      "----------\n",
      "train Loss: 0.9417 Acc: 0.6007 \n",
      "val Loss: 0.9685 Acc: 0.5740 \n",
      "Epoch 209/299\n",
      "----------\n",
      "train Loss: 0.9416 Acc: 0.6003 \n",
      "val Loss: 0.9686 Acc: 0.5740 \n",
      "Epoch 210/299\n",
      "----------\n",
      "train Loss: 0.9415 Acc: 0.6000 \n",
      "val Loss: 0.9686 Acc: 0.5730 \n",
      "Epoch 211/299\n",
      "----------\n",
      "train Loss: 0.9415 Acc: 0.6000 \n",
      "val Loss: 0.9686 Acc: 0.5730 \n",
      "Epoch 212/299\n",
      "----------\n",
      "train Loss: 0.9414 Acc: 0.6003 \n",
      "val Loss: 0.9686 Acc: 0.5730 \n",
      "Epoch 213/299\n",
      "----------\n",
      "train Loss: 0.9414 Acc: 0.6003 \n",
      "val Loss: 0.9686 Acc: 0.5730 \n",
      "Epoch 214/299\n",
      "----------\n",
      "train Loss: 0.9413 Acc: 0.6003 \n",
      "val Loss: 0.9686 Acc: 0.5730 \n",
      "Epoch 215/299\n",
      "----------\n",
      "train Loss: 0.9412 Acc: 0.6010 \n",
      "val Loss: 0.9686 Acc: 0.5725 \n",
      "Epoch 216/299\n",
      "----------\n",
      "train Loss: 0.9412 Acc: 0.6010 \n",
      "val Loss: 0.9686 Acc: 0.5730 \n",
      "Epoch 217/299\n",
      "----------\n",
      "train Loss: 0.9411 Acc: 0.6013 \n",
      "val Loss: 0.9686 Acc: 0.5730 \n",
      "Epoch 218/299\n",
      "----------\n",
      "train Loss: 0.9410 Acc: 0.6013 \n",
      "val Loss: 0.9686 Acc: 0.5725 \n",
      "Epoch 219/299\n",
      "----------\n",
      "train Loss: 0.9410 Acc: 0.6013 \n",
      "val Loss: 0.9686 Acc: 0.5725 \n",
      "Epoch 220/299\n",
      "----------\n",
      "train Loss: 0.9409 Acc: 0.6017 \n",
      "val Loss: 0.9686 Acc: 0.5730 \n",
      "Epoch 221/299\n",
      "----------\n",
      "train Loss: 0.9408 Acc: 0.6017 \n",
      "val Loss: 0.9687 Acc: 0.5730 \n",
      "Epoch 222/299\n",
      "----------\n",
      "train Loss: 0.9408 Acc: 0.6013 \n",
      "val Loss: 0.9687 Acc: 0.5725 \n",
      "Epoch 223/299\n",
      "----------\n",
      "train Loss: 0.9407 Acc: 0.6013 \n",
      "val Loss: 0.9687 Acc: 0.5725 \n",
      "Epoch 224/299\n",
      "----------\n",
      "train Loss: 0.9407 Acc: 0.6013 \n",
      "val Loss: 0.9687 Acc: 0.5720 \n",
      "Epoch 225/299\n",
      "----------\n",
      "train Loss: 0.9406 Acc: 0.6013 \n",
      "val Loss: 0.9687 Acc: 0.5715 \n",
      "Epoch 226/299\n",
      "----------\n",
      "train Loss: 0.9405 Acc: 0.6013 \n",
      "val Loss: 0.9687 Acc: 0.5715 \n",
      "Epoch 227/299\n",
      "----------\n",
      "train Loss: 0.9405 Acc: 0.6013 \n",
      "val Loss: 0.9687 Acc: 0.5715 \n",
      "Epoch 228/299\n",
      "----------\n",
      "train Loss: 0.9404 Acc: 0.6013 \n",
      "val Loss: 0.9687 Acc: 0.5715 \n",
      "Epoch 229/299\n",
      "----------\n",
      "train Loss: 0.9404 Acc: 0.6017 \n",
      "val Loss: 0.9688 Acc: 0.5715 \n",
      "Epoch 230/299\n",
      "----------\n",
      "train Loss: 0.9403 Acc: 0.6017 \n",
      "val Loss: 0.9688 Acc: 0.5710 \n",
      "Epoch 231/299\n",
      "----------\n",
      "train Loss: 0.9402 Acc: 0.6013 \n",
      "val Loss: 0.9688 Acc: 0.5710 \n",
      "Epoch 232/299\n",
      "----------\n",
      "train Loss: 0.9402 Acc: 0.6013 \n",
      "val Loss: 0.9688 Acc: 0.5710 \n",
      "Epoch 233/299\n",
      "----------\n",
      "train Loss: 0.9401 Acc: 0.6010 \n",
      "val Loss: 0.9688 Acc: 0.5710 \n",
      "Epoch 234/299\n",
      "----------\n",
      "train Loss: 0.9400 Acc: 0.6010 \n",
      "val Loss: 0.9688 Acc: 0.5705 \n",
      "Epoch 235/299\n",
      "----------\n",
      "train Loss: 0.9400 Acc: 0.6013 \n",
      "val Loss: 0.9688 Acc: 0.5705 \n",
      "Epoch 236/299\n",
      "----------\n",
      "train Loss: 0.9399 Acc: 0.6010 \n",
      "val Loss: 0.9689 Acc: 0.5705 \n",
      "Epoch 237/299\n",
      "----------\n",
      "train Loss: 0.9399 Acc: 0.6010 \n",
      "val Loss: 0.9689 Acc: 0.5705 \n",
      "Epoch 238/299\n",
      "----------\n",
      "train Loss: 0.9398 Acc: 0.6013 \n",
      "val Loss: 0.9689 Acc: 0.5705 \n",
      "Epoch 239/299\n",
      "----------\n",
      "train Loss: 0.9397 Acc: 0.6013 \n",
      "val Loss: 0.9689 Acc: 0.5710 \n",
      "Epoch 240/299\n",
      "----------\n",
      "train Loss: 0.9397 Acc: 0.6013 \n",
      "val Loss: 0.9689 Acc: 0.5710 \n",
      "Epoch 241/299\n",
      "----------\n",
      "train Loss: 0.9396 Acc: 0.6013 \n",
      "val Loss: 0.9689 Acc: 0.5710 \n",
      "Epoch 242/299\n",
      "----------\n",
      "train Loss: 0.9396 Acc: 0.6013 \n",
      "val Loss: 0.9689 Acc: 0.5705 \n",
      "Epoch 243/299\n",
      "----------\n",
      "train Loss: 0.9395 Acc: 0.6017 \n",
      "val Loss: 0.9690 Acc: 0.5705 \n",
      "Epoch 244/299\n",
      "----------\n",
      "train Loss: 0.9394 Acc: 0.6020 \n",
      "val Loss: 0.9690 Acc: 0.5705 \n",
      "Epoch 245/299\n",
      "----------\n",
      "train Loss: 0.9394 Acc: 0.6020 \n",
      "val Loss: 0.9690 Acc: 0.5705 \n",
      "Epoch 246/299\n",
      "----------\n",
      "train Loss: 0.9393 Acc: 0.6023 \n",
      "val Loss: 0.9690 Acc: 0.5695 \n",
      "Epoch 247/299\n",
      "----------\n",
      "train Loss: 0.9392 Acc: 0.6023 \n",
      "val Loss: 0.9690 Acc: 0.5690 \n",
      "Epoch 248/299\n",
      "----------\n",
      "train Loss: 0.9392 Acc: 0.6020 \n",
      "val Loss: 0.9690 Acc: 0.5690 \n",
      "Epoch 249/299\n",
      "----------\n",
      "train Loss: 0.9391 Acc: 0.6020 \n",
      "val Loss: 0.9691 Acc: 0.5690 \n",
      "Epoch 250/299\n",
      "----------\n",
      "train Loss: 0.9391 Acc: 0.6023 \n",
      "val Loss: 0.9691 Acc: 0.5690 \n",
      "Epoch 251/299\n",
      "----------\n",
      "train Loss: 0.9390 Acc: 0.6023 \n",
      "val Loss: 0.9691 Acc: 0.5690 \n",
      "Epoch 252/299\n",
      "----------\n",
      "train Loss: 0.9389 Acc: 0.6023 \n",
      "val Loss: 0.9691 Acc: 0.5685 \n",
      "Epoch 253/299\n",
      "----------\n",
      "train Loss: 0.9389 Acc: 0.6027 \n",
      "val Loss: 0.9691 Acc: 0.5685 \n",
      "Epoch 254/299\n",
      "----------\n",
      "train Loss: 0.9388 Acc: 0.6027 \n",
      "val Loss: 0.9691 Acc: 0.5690 \n",
      "Epoch 255/299\n",
      "----------\n",
      "train Loss: 0.9388 Acc: 0.6027 \n",
      "val Loss: 0.9692 Acc: 0.5690 \n",
      "Epoch 256/299\n",
      "----------\n",
      "train Loss: 0.9387 Acc: 0.6027 \n",
      "val Loss: 0.9692 Acc: 0.5690 \n",
      "Epoch 257/299\n",
      "----------\n",
      "train Loss: 0.9386 Acc: 0.6027 \n",
      "val Loss: 0.9692 Acc: 0.5685 \n",
      "Epoch 258/299\n",
      "----------\n",
      "train Loss: 0.9386 Acc: 0.6027 \n",
      "val Loss: 0.9692 Acc: 0.5685 \n",
      "Epoch 259/299\n",
      "----------\n",
      "train Loss: 0.9385 Acc: 0.6027 \n",
      "val Loss: 0.9692 Acc: 0.5685 \n",
      "Epoch 260/299\n",
      "----------\n",
      "train Loss: 0.9384 Acc: 0.6027 \n",
      "val Loss: 0.9693 Acc: 0.5685 \n",
      "Epoch 261/299\n",
      "----------\n",
      "train Loss: 0.9384 Acc: 0.6023 \n",
      "val Loss: 0.9693 Acc: 0.5685 \n",
      "Epoch 262/299\n",
      "----------\n",
      "train Loss: 0.9383 Acc: 0.6027 \n",
      "val Loss: 0.9693 Acc: 0.5680 \n",
      "Epoch 263/299\n",
      "----------\n",
      "train Loss: 0.9383 Acc: 0.6027 \n",
      "val Loss: 0.9693 Acc: 0.5680 \n",
      "Epoch 264/299\n",
      "----------\n",
      "train Loss: 0.9382 Acc: 0.6027 \n",
      "val Loss: 0.9693 Acc: 0.5685 \n",
      "Epoch 265/299\n",
      "----------\n",
      "train Loss: 0.9381 Acc: 0.6027 \n",
      "val Loss: 0.9694 Acc: 0.5680 \n",
      "Epoch 266/299\n",
      "----------\n",
      "train Loss: 0.9381 Acc: 0.6030 \n",
      "val Loss: 0.9694 Acc: 0.5680 \n",
      "Epoch 267/299\n",
      "----------\n",
      "train Loss: 0.9380 Acc: 0.6033 \n",
      "val Loss: 0.9694 Acc: 0.5680 \n",
      "Epoch 268/299\n",
      "----------\n",
      "train Loss: 0.9379 Acc: 0.6033 \n",
      "val Loss: 0.9694 Acc: 0.5675 \n",
      "Epoch 269/299\n",
      "----------\n",
      "train Loss: 0.9379 Acc: 0.6033 \n",
      "val Loss: 0.9694 Acc: 0.5670 \n",
      "Epoch 270/299\n",
      "----------\n",
      "train Loss: 0.9378 Acc: 0.6037 \n",
      "val Loss: 0.9694 Acc: 0.5670 \n",
      "Epoch 271/299\n",
      "----------\n",
      "train Loss: 0.9378 Acc: 0.6037 \n",
      "val Loss: 0.9695 Acc: 0.5670 \n",
      "Epoch 272/299\n",
      "----------\n",
      "train Loss: 0.9377 Acc: 0.6037 \n",
      "val Loss: 0.9695 Acc: 0.5665 \n",
      "Epoch 273/299\n",
      "----------\n",
      "train Loss: 0.9376 Acc: 0.6037 \n",
      "val Loss: 0.9695 Acc: 0.5665 \n",
      "Epoch 274/299\n",
      "----------\n",
      "train Loss: 0.9376 Acc: 0.6037 \n",
      "val Loss: 0.9695 Acc: 0.5665 \n",
      "Epoch 275/299\n",
      "----------\n",
      "train Loss: 0.9375 Acc: 0.6037 \n",
      "val Loss: 0.9695 Acc: 0.5665 \n",
      "Epoch 276/299\n",
      "----------\n",
      "train Loss: 0.9374 Acc: 0.6037 \n",
      "val Loss: 0.9696 Acc: 0.5670 \n",
      "Epoch 277/299\n",
      "----------\n",
      "train Loss: 0.9374 Acc: 0.6037 \n",
      "val Loss: 0.9696 Acc: 0.5675 \n",
      "Epoch 278/299\n",
      "----------\n",
      "train Loss: 0.9373 Acc: 0.6037 \n",
      "val Loss: 0.9696 Acc: 0.5675 \n",
      "Epoch 279/299\n",
      "----------\n",
      "train Loss: 0.9373 Acc: 0.6037 \n",
      "val Loss: 0.9696 Acc: 0.5675 \n",
      "Epoch 280/299\n",
      "----------\n",
      "train Loss: 0.9372 Acc: 0.6037 \n",
      "val Loss: 0.9696 Acc: 0.5675 \n",
      "Epoch 281/299\n",
      "----------\n",
      "train Loss: 0.9371 Acc: 0.6040 \n",
      "val Loss: 0.9696 Acc: 0.5675 \n",
      "Epoch 282/299\n",
      "----------\n",
      "train Loss: 0.9371 Acc: 0.6037 \n",
      "val Loss: 0.9697 Acc: 0.5675 \n",
      "Epoch 283/299\n",
      "----------\n",
      "train Loss: 0.9370 Acc: 0.6037 \n",
      "val Loss: 0.9697 Acc: 0.5675 \n",
      "Epoch 284/299\n",
      "----------\n",
      "train Loss: 0.9369 Acc: 0.6037 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val Loss: 0.9697 Acc: 0.5675 \n",
      "Epoch 285/299\n",
      "----------\n",
      "train Loss: 0.9369 Acc: 0.6037 \n",
      "val Loss: 0.9697 Acc: 0.5670 \n",
      "Epoch 286/299\n",
      "----------\n",
      "train Loss: 0.9368 Acc: 0.6037 \n",
      "val Loss: 0.9697 Acc: 0.5670 \n",
      "Epoch 287/299\n",
      "----------\n",
      "train Loss: 0.9367 Acc: 0.6037 \n",
      "val Loss: 0.9697 Acc: 0.5670 \n",
      "Epoch 288/299\n",
      "----------\n",
      "train Loss: 0.9367 Acc: 0.6037 \n",
      "val Loss: 0.9698 Acc: 0.5670 \n",
      "Epoch 289/299\n",
      "----------\n",
      "train Loss: 0.9366 Acc: 0.6037 \n",
      "val Loss: 0.9698 Acc: 0.5670 \n",
      "Epoch 290/299\n",
      "----------\n",
      "train Loss: 0.9366 Acc: 0.6037 \n",
      "val Loss: 0.9698 Acc: 0.5670 \n",
      "Epoch 291/299\n",
      "----------\n",
      "train Loss: 0.9365 Acc: 0.6033 \n",
      "val Loss: 0.9698 Acc: 0.5670 \n",
      "Epoch 292/299\n",
      "----------\n",
      "train Loss: 0.9364 Acc: 0.6033 \n",
      "val Loss: 0.9698 Acc: 0.5665 \n",
      "Epoch 293/299\n",
      "----------\n",
      "train Loss: 0.9364 Acc: 0.6037 \n",
      "val Loss: 0.9698 Acc: 0.5655 \n",
      "Epoch 294/299\n",
      "----------\n",
      "train Loss: 0.9363 Acc: 0.6043 \n",
      "val Loss: 0.9698 Acc: 0.5655 \n",
      "Epoch 295/299\n",
      "----------\n",
      "train Loss: 0.9362 Acc: 0.6043 \n",
      "val Loss: 0.9699 Acc: 0.5655 \n",
      "Epoch 296/299\n",
      "----------\n",
      "train Loss: 0.9362 Acc: 0.6047 \n",
      "val Loss: 0.9699 Acc: 0.5655 \n",
      "Epoch 297/299\n",
      "----------\n",
      "train Loss: 0.9361 Acc: 0.6053 \n",
      "val Loss: 0.9699 Acc: 0.5655 \n",
      "Epoch 298/299\n",
      "----------\n",
      "train Loss: 0.9360 Acc: 0.6053 \n",
      "val Loss: 0.9699 Acc: 0.5655 \n",
      "Epoch 299/299\n",
      "----------\n",
      "train Loss: 0.9360 Acc: 0.6053 \n",
      "val Loss: 0.9699 Acc: 0.5655 \n",
      "Training complete in 3m 15s\n",
      "Best val Acc: 0.579000\n"
     ]
    }
   ],
   "source": [
    "model,losses,accuracies=train_model(model,criterion,optimizer,mask,1.0,300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
