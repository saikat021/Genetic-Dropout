{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "fsqWgtdkweov"
   },
   "outputs": [],
   "source": [
    "import math\n",
    "import random\n",
    "from torchvision import datasets ,transforms\n",
    "from torchvision.datasets import ImageFolder\n",
    "from torch.utils.data import DataLoader,Dataset\n",
    "from torch import nn \n",
    "from torch.nn.functional import relu ,softmax \n",
    "import copy \n",
    "from torch.utils.data import Subset\n",
    "import pandas as pd \n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import time\n",
    "import torch\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import torch.nn.functional as F\n",
    "device=\"cpu\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 256
    },
    "colab_type": "code",
    "id": "Bzm5sf5zwiV_",
    "outputId": "927911e5-363d-4f8b-abb5-a23101a2361a"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>att_1</th>\n",
       "      <th>att_2</th>\n",
       "      <th>att_3</th>\n",
       "      <th>att_4</th>\n",
       "      <th>att_5</th>\n",
       "      <th>att_6</th>\n",
       "      <th>att_7</th>\n",
       "      <th>att_8</th>\n",
       "      <th>att_9</th>\n",
       "      <th>att_10</th>\n",
       "      <th>...</th>\n",
       "      <th>att_136</th>\n",
       "      <th>att_137</th>\n",
       "      <th>att_138</th>\n",
       "      <th>att_139</th>\n",
       "      <th>att_140</th>\n",
       "      <th>att_141</th>\n",
       "      <th>att_142</th>\n",
       "      <th>att_143</th>\n",
       "      <th>att_144</th>\n",
       "      <th>class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>279</td>\n",
       "      <td>60</td>\n",
       "      <td>39</td>\n",
       "      <td>36</td>\n",
       "      <td>24</td>\n",
       "      <td>21</td>\n",
       "      <td>20</td>\n",
       "      <td>19</td>\n",
       "      <td>20</td>\n",
       "      <td>22</td>\n",
       "      <td>...</td>\n",
       "      <td>3993</td>\n",
       "      <td>2419</td>\n",
       "      <td>3320</td>\n",
       "      <td>2660</td>\n",
       "      <td>2850</td>\n",
       "      <td>2591</td>\n",
       "      <td>2594</td>\n",
       "      <td>2444</td>\n",
       "      <td>2546</td>\n",
       "      <td>A</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>272</td>\n",
       "      <td>62</td>\n",
       "      <td>43</td>\n",
       "      <td>27</td>\n",
       "      <td>24</td>\n",
       "      <td>20</td>\n",
       "      <td>20</td>\n",
       "      <td>20</td>\n",
       "      <td>25</td>\n",
       "      <td>18</td>\n",
       "      <td>...</td>\n",
       "      <td>2687</td>\n",
       "      <td>2887</td>\n",
       "      <td>3494</td>\n",
       "      <td>2496</td>\n",
       "      <td>2591</td>\n",
       "      <td>2745</td>\n",
       "      <td>2384</td>\n",
       "      <td>2351</td>\n",
       "      <td>2411</td>\n",
       "      <td>A</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>177</td>\n",
       "      <td>82</td>\n",
       "      <td>46</td>\n",
       "      <td>28</td>\n",
       "      <td>23</td>\n",
       "      <td>22</td>\n",
       "      <td>19</td>\n",
       "      <td>16</td>\n",
       "      <td>17</td>\n",
       "      <td>16</td>\n",
       "      <td>...</td>\n",
       "      <td>3071</td>\n",
       "      <td>3717</td>\n",
       "      <td>3073</td>\n",
       "      <td>2835</td>\n",
       "      <td>3094</td>\n",
       "      <td>2499</td>\n",
       "      <td>2480</td>\n",
       "      <td>2483</td>\n",
       "      <td>2548</td>\n",
       "      <td>A</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>205</td>\n",
       "      <td>84</td>\n",
       "      <td>44</td>\n",
       "      <td>26</td>\n",
       "      <td>25</td>\n",
       "      <td>23</td>\n",
       "      <td>20</td>\n",
       "      <td>20</td>\n",
       "      <td>20</td>\n",
       "      <td>28</td>\n",
       "      <td>...</td>\n",
       "      <td>3818</td>\n",
       "      <td>2340</td>\n",
       "      <td>2372</td>\n",
       "      <td>2372</td>\n",
       "      <td>2370</td>\n",
       "      <td>2537</td>\n",
       "      <td>2526</td>\n",
       "      <td>2521</td>\n",
       "      <td>2418</td>\n",
       "      <td>A</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>166</td>\n",
       "      <td>91</td>\n",
       "      <td>38</td>\n",
       "      <td>26</td>\n",
       "      <td>25</td>\n",
       "      <td>21</td>\n",
       "      <td>19</td>\n",
       "      <td>19</td>\n",
       "      <td>19</td>\n",
       "      <td>17</td>\n",
       "      <td>...</td>\n",
       "      <td>2388</td>\n",
       "      <td>2561</td>\n",
       "      <td>2558</td>\n",
       "      <td>2543</td>\n",
       "      <td>3033</td>\n",
       "      <td>2928</td>\n",
       "      <td>2825</td>\n",
       "      <td>2530</td>\n",
       "      <td>2424</td>\n",
       "      <td>A</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 145 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   att_1  att_2  att_3  att_4  att_5  att_6  att_7  att_8  att_9  att_10  ...  \\\n",
       "0    279     60     39     36     24     21     20     19     20      22  ...   \n",
       "1    272     62     43     27     24     20     20     20     25      18  ...   \n",
       "2    177     82     46     28     23     22     19     16     17      16  ...   \n",
       "3    205     84     44     26     25     23     20     20     20      28  ...   \n",
       "4    166     91     38     26     25     21     19     19     19      17  ...   \n",
       "\n",
       "   att_136  att_137  att_138  att_139  att_140  att_141  att_142  att_143  \\\n",
       "0     3993     2419     3320     2660     2850     2591     2594     2444   \n",
       "1     2687     2887     3494     2496     2591     2745     2384     2351   \n",
       "2     3071     3717     3073     2835     3094     2499     2480     2483   \n",
       "3     3818     2340     2372     2372     2370     2537     2526     2521   \n",
       "4     2388     2561     2558     2543     3033     2928     2825     2530   \n",
       "\n",
       "   att_144  class  \n",
       "0     2546      A  \n",
       "1     2411      A  \n",
       "2     2548      A  \n",
       "3     2418      A  \n",
       "4     2424      A  \n",
       "\n",
       "[5 rows x 145 columns]"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "path ='Hand_Written_script_classification data/Line-level-Features/DHT_Algorithm_144_Features.csv'\n",
    "#Write the path of the dataset\n",
    "df = pd.read_csv(path) #header=None\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "id": "q9lhL3iVwyc6",
    "outputId": "1e35db16-a4c7-4523-d023-9e9d6feb633d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3600, 145)\n"
     ]
    }
   ],
   "source": [
    "print(df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 127
    },
    "colab_type": "code",
    "id": "S4K9LYdmw4iW",
    "outputId": "6a2c8931-b341-4de4-b864-c9d9771e1595"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3600, 145)\n",
      "<class 'numpy.ndarray'>\n",
      "(3600, 144)\n",
      "\n",
      "['A' 'B' 'C' 'D' 'E' 'F' 'G' 'H' 'I' 'J' 'K' 'L']\n",
      "[0 0 0 0 0 0 0 0 0 0]\n"
     ]
    }
   ],
   "source": [
    "print(df.shape)\n",
    "#X=df.loc[:,df.columns != 'Classifier'].values\n",
    "X=df.loc[:,df.columns != df.columns[-1]].values\n",
    "print(type(X))\n",
    "print(X.shape)\n",
    "#y_label=df['Classifier'].values \n",
    "y_label =df.iloc[:,-1].values\n",
    "#print(type(y_label))\n",
    "#print(X[0])\n",
    "print()\n",
    "from sklearn import preprocessing\n",
    "le = preprocessing.LabelEncoder()\n",
    "le.fit(y_label)\n",
    "print(le.classes_)\n",
    "y=le.transform(y_label)\n",
    "print(y[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 54
    },
    "colab_type": "code",
    "id": "ASaktl2pxAXk",
    "outputId": "95a10aee-01f1-4229-ffb4-fbd5825700d3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11} {0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11}\n",
      "(3600, 144) (3600,)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler #for normalization\n",
    "sc = StandardScaler()\n",
    "X = sc.fit_transform(X)\n",
    "# from sklearn.preprocessing import MinMaxScaler\n",
    "# scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "# X = scaler.fit_transform(X)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42, shuffle=True)\n",
    "# print(y_train)\n",
    "# print(y_test)\n",
    "#print(len(X_train), len(y_train), len(X_test), len(y_test))\n",
    "# # X=X_train\n",
    "# # y=y_train\n",
    "print(set(y_train), set(y_test))\n",
    "print(X.shape, y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "IU4XOA5fy_4d"
   },
   "outputs": [],
   "source": [
    "X_train = torch.tensor(X_train, dtype=torch.float)\n",
    "y_train = torch.tensor(y_train, dtype=torch.long)\n",
    "X_test = torch.tensor(X_test, dtype=torch.float)\n",
    "y_test = torch.tensor(y_test, dtype=torch.long)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "AeGznOD8xEPs"
   },
   "outputs": [],
   "source": [
    "#n_inputs = X_train.shape[1]\n",
    "#n_hidden = 180\n",
    "#n_classes = len(le.classes_)\n",
    "#N = X_train.shape[0] # Number of samples\n",
    "#print(N)\n",
    "#hyperparameters\n",
    "\n",
    "input_size = X_train.shape[1]    #The image size = 28 x 28 = 784\n",
    "hidden_size = X_train.shape[1]    #The number of nodes at the hidden layer\n",
    "num_classes = len(le.classes_)       #The number of output classes. In this case, from 0 to 9\n",
    "num_epochs = 100      # The number of times entire dataset is trained\n",
    "batch_size = X_train.shape[0]    # The size of input data took for one iteration\n",
    "lr = 0.001  # The speed of convergence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "fatO2VKDxILM"
   },
   "outputs": [],
   "source": [
    "\n",
    "#build model\n",
    "class Model(nn.Module):\n",
    "    def __init__(self,input_size, hidden_size, num_classes):\n",
    "        super(Model, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_size , hidden_size)\n",
    "        self.fc2 = nn.Linear(hidden_size, num_classes)\n",
    "        self.drop = nn.Dropout (p=0.4)\n",
    "\n",
    "    def masking (self,act1,mask,p):\n",
    "        if (self.training ==True ):\n",
    "            return ((act1*mask)/p)\n",
    "        else :\n",
    "            return (act1)\n",
    "        \n",
    "\n",
    "    def Forward(self, x,mask,p):\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.masking (x,mask,p)\n",
    "        x = self.fc2(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "xnf3Kd39xosT"
   },
   "outputs": [],
   "source": [
    "#the mask has to be specified \n",
    "def train_model(model, criterion, optimizer,mask,p, num_epochs=25):\n",
    "  \n",
    "    since = time.time()\n",
    "    best_model_wts = copy.deepcopy(model.state_dict())\n",
    "    best_acc = 0.0\n",
    "    losses=[]\n",
    "    accuracies=[]\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        print('Epoch {}/{}'.format(epoch, num_epochs - 1))\n",
    "        print('-' * 10)\n",
    "\n",
    "        # Each epoch has a training and validation phase\n",
    "        for phase in ['train', 'val']:\n",
    "              if phase == 'train':\n",
    "                model.train()  # Set model to training mode\n",
    "                optimizer.zero_grad()\n",
    "                with torch.set_grad_enabled(phase == 'train'):\n",
    "                    outputs= model.Forward(X_train,mask,p) #inputs\n",
    "                    train_loss = criterion(outputs, y_train)# backward + optimize only if in training phase,labels\n",
    "                    train_loss.backward()\n",
    "                    optimizer.step()\n",
    "                train_acc = 100 * torch.sum(y_train==torch.max(outputs.data, 1)[1]).double() / len(y_train)\n",
    "                print('********************{}*************'.format(phase))\n",
    "                print ('Epoch [%d/%d] Loss: %.4f   Acc: %.4f'%(epoch+1, num_epochs, train_loss.item(), train_acc.item()))\n",
    "                losses.append(train_loss)\n",
    "                accuracies.append(train_acc)\n",
    "            else:\n",
    "                model.eval()   # Set model to evaluate mode\n",
    "                outputs= model.Forward(X_test,mask,p) #inputs\n",
    "                _, predicted = torch.max(outputs, 1)\n",
    "                test_loss = criterion(outputs, y_test)\n",
    "                test_acc = 100 * torch.sum(y_test==predicted).double() / len(y_test)\n",
    "                print('********************{}*************'.format(phase))\n",
    "                print ('Epoch [%d/%d] Loss: %.4f   Acc: %.4f'%(epoch+1, num_epochs, test_loss.item(), test_acc.item()))\n",
    "                if test_acc > best_acc:\n",
    "                    best_acc = test_acc\n",
    "                    best_model_wts = copy.deepcopy(model.state_dict())\n",
    "\n",
    "              \n",
    "\n",
    "\n",
    "\n",
    "    time_elapsed = time.time() - since\n",
    "    print('Training complete in {:.0f}m {:.0f}s'.format(\n",
    "          time_elapsed // 60, time_elapsed % 60))\n",
    "    print('Best val Acc: {:4f}'.format(best_acc))\n",
    "\n",
    "    #load best model weights\n",
    "    model.load_state_dict(best_model_wts)\n",
    "    return model,losses,accuracies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "_rFn0uEUxvQt"
   },
   "outputs": [],
   "source": [
    "class Population:   \n",
    "    #constructor for initialising the population list\n",
    "    #list of DNA objects\n",
    "    def __init__(self,m,num,maskLength):\n",
    "        self.population=[]\n",
    "        self.mutation_rate=m #mutation rate for mutation\n",
    "        self.popmax=num #maximum number of entities in the population\n",
    "        self.maskLength=maskLength\n",
    "        for i in range (num):\n",
    "            #creating a dna object\n",
    "            #an initial random population created \n",
    "            dna =DNA(self.maskLength)\n",
    "            self.population.append (dna)\n",
    "      \n",
    "        self.matingPool=[]\n",
    "    \n",
    "    #going through all the entities of population \n",
    "    #finding fitness of all population entities \n",
    "    def calcFitness (self,model):\n",
    "        \n",
    "        for i in range(0,self.popmax):\n",
    "            self.population[i].fitness (model)\n",
    "\n",
    "    def naturalSelection(self):\n",
    "        self.matingPool=[]\n",
    "        maxFitness=0\n",
    "        for i in range (self.popmax):\n",
    "            # moving throught the entire population \n",
    "            if (self.population[i].fit>maxFitness):\n",
    "                maxFitness=self.population[i].fit\n",
    "       \n",
    "        #max Fitness has the maximum loss score of the entire population  \n",
    "        #iterating through the all inviduals of the population\n",
    "        for i in range (self.popmax ):\n",
    "        \n",
    "            n=self.Mymap(self.population[i].fit,0,maxFitness,0,1)\n",
    "        \n",
    "            n=math.floor(n*100)\n",
    "            \n",
    "            for j in range (n):\n",
    "                self.matingPool.append (self.population[i])#creating mating pool\n",
    "\n",
    "    def Mymap(self,num,prevlow,prevhigh,nextlow,nexthigh):\n",
    "        \n",
    "        prevrange =float((num-prevlow)/(prevhigh-prevlow))\n",
    "        return nextlow+(nexthigh-nextlow)*prevrange\n",
    "\n",
    "    def   generate (self):\n",
    "        for i in range (self.popmax ):\n",
    "            index_1=math.floor(random.randint  (0,len(self.matingPool)-1))\n",
    "            index_2=math.floor (random.randint (0,len(self.matingPool)-1))\n",
    "            parent1=self.matingPool[index_1]\n",
    "            parent2=self.matingPool[index_2]\n",
    "            child=parent1.crossover(parent2)\n",
    "            child.mutate(self.mutation_rate)\n",
    "            self.population[i]=child\n",
    "            \n",
    "    #returns the fiitest individual mask of the population \n",
    "    #also returns the keeping probability of the fittest mask \n",
    "    def fittest(self):\n",
    "        fittest=self.population[0]\n",
    "        for i  in range (self.popmax):\n",
    "            if (fittest.fit<self.population[i].fit):\n",
    "                fittest=self.population[i]\n",
    "        return fittest,fittest.keep_prob()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "dcIsijbQxxbM"
   },
   "outputs": [],
   "source": [
    "class DNA: \n",
    "    #constructor for the creation of the mask as a gene object \n",
    "    def __init__(self,maskLength):\n",
    "        self.maskLength=maskLength\n",
    "        self.gene=torch.bernoulli(torch.empty(1,maskLength).uniform_(0,1))#creation of mask \n",
    "        self.fit=0\n",
    "\n",
    "    def keep_prob (self):\n",
    "        num_one =0\n",
    "        for i in range (self.maskLength):\n",
    "            if (self.gene[0,i]==1):\n",
    "                num_one=num_one+1\n",
    "        return float(num_one/self.maskLength)\n",
    "    \n",
    "    \n",
    "    #finding the fitness of a particular mask\n",
    "    #accuracy of all training set is the fitness in one epoch\n",
    "    #putting model in train mode\n",
    "    def fitness(self,model):\n",
    " \n",
    "        running_loss=0\n",
    "        running_corrects=0\n",
    "        model.train()\n",
    "        outputs=model.Forward(X_train,self.gene,self.keep_prob())\n",
    "        _,preds=torch.max(outputs,1)\n",
    "        loss=criterion (outputs,y_train)\n",
    "        acc = 100*torch.sum(y_train==torch.max(outputs.data, 1)[1]).double() / len(y_train)\n",
    "        self.fit=acc\n",
    "        return acc\n",
    "        \n",
    "    #one parent is the passed in the argument \n",
    "    #another parent is the one from which this function is called \n",
    "    #another parent is self.gene\n",
    "    def crossover (self,parent2):\n",
    "        \n",
    "        child =DNA(self.maskLength)\n",
    "        midpoint =random .randint (0,self.maskLength-1)\n",
    "        for i in range (0,self.maskLength):\n",
    "            if (i>midpoint):\n",
    "                child.gene [0,i]=self.gene[0,i]\n",
    "            else :\n",
    "                child.gene [0,i]=parent2.gene[0,i]\n",
    "        \n",
    "        return child\n",
    "    \n",
    "    \n",
    "    #randomly activate some of the nodes  \n",
    "    #mutate some of the genes\n",
    "    def mutate(self,mutation_rate):\n",
    "         \n",
    "        for i in range (self.maskLength):\n",
    "            if (random.randint (0,99)<=mutation_rate*100):\n",
    "                self.gene[0,i]=1 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "X3LuAn4Oxz6q"
   },
   "outputs": [],
   "source": [
    "mutation_rate =0\n",
    "max_population=30\n",
    "maskLength=X_train.shape[1]\n",
    "model=Model(input_size, hidden_size, num_classes)#creating the object of the class\n",
    "model.to(device)\n",
    "criterion=nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "apf1SKKNx9Q5",
    "outputId": "b002f9b0-bcc6-429c-afd8-789fb968c68d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch generations ( 0 /100) :accuracy(fittest mask) tensor(10.9921, dtype=torch.float64) keep_prob 0.4861111111111111\n",
      "Epoch 0/29\n",
      "----------\n",
      "********************train*************\n",
      "Epoch [1/30] Loss: 2.5472   Acc: 10.9921\n",
      "********************val*************\n",
      "Epoch [1/30] Loss: 2.4893   Acc: 6.6667\n",
      "Epoch 1/29\n",
      "----------\n",
      "********************train*************\n",
      "Epoch [2/30] Loss: 2.4503   Acc: 14.1667\n",
      "********************val*************\n",
      "Epoch [2/30] Loss: 2.4435   Acc: 10.0926\n",
      "Epoch 2/29\n",
      "----------\n",
      "********************train*************\n",
      "Epoch [3/30] Loss: 2.3618   Acc: 19.1667\n",
      "********************val*************\n",
      "Epoch [3/30] Loss: 2.3999   Acc: 15.0926\n",
      "Epoch 3/29\n",
      "----------\n",
      "********************train*************\n",
      "Epoch [4/30] Loss: 2.2812   Acc: 26.0317\n",
      "********************val*************\n",
      "Epoch [4/30] Loss: 2.3583   Acc: 19.4444\n",
      "Epoch 4/29\n",
      "----------\n",
      "********************train*************\n",
      "Epoch [5/30] Loss: 2.2081   Acc: 30.7540\n",
      "********************val*************\n",
      "Epoch [5/30] Loss: 2.3185   Acc: 24.2593\n",
      "Epoch 5/29\n",
      "----------\n",
      "********************train*************\n",
      "Epoch [6/30] Loss: 2.1422   Acc: 33.1349\n",
      "********************val*************\n",
      "Epoch [6/30] Loss: 2.2801   Acc: 27.8704\n",
      "Epoch 6/29\n",
      "----------\n",
      "********************train*************\n",
      "Epoch [7/30] Loss: 2.0821   Acc: 35.1587\n",
      "********************val*************\n",
      "Epoch [7/30] Loss: 2.2431   Acc: 29.9074\n",
      "Epoch 7/29\n",
      "----------\n",
      "********************train*************\n",
      "Epoch [8/30] Loss: 2.0272   Acc: 36.5476\n",
      "********************val*************\n",
      "Epoch [8/30] Loss: 2.2075   Acc: 31.8519\n",
      "Epoch 8/29\n",
      "----------\n",
      "********************train*************\n",
      "Epoch [9/30] Loss: 1.9768   Acc: 38.8095\n",
      "********************val*************\n",
      "Epoch [9/30] Loss: 2.1738   Acc: 33.6111\n",
      "Epoch 9/29\n",
      "----------\n",
      "********************train*************\n",
      "Epoch [10/30] Loss: 1.9302   Acc: 40.2778\n",
      "********************val*************\n",
      "Epoch [10/30] Loss: 2.1418   Acc: 35.2778\n",
      "Epoch 10/29\n",
      "----------\n",
      "********************train*************\n",
      "Epoch [11/30] Loss: 1.8864   Acc: 41.7857\n",
      "********************val*************\n",
      "Epoch [11/30] Loss: 2.1115   Acc: 36.2963\n",
      "Epoch 11/29\n",
      "----------\n",
      "********************train*************\n",
      "Epoch [12/30] Loss: 1.8448   Acc: 43.9286\n",
      "********************val*************\n",
      "Epoch [12/30] Loss: 2.0830   Acc: 37.4074\n",
      "Epoch 12/29\n",
      "----------\n",
      "********************train*************\n",
      "Epoch [13/30] Loss: 1.8048   Acc: 44.8810\n",
      "********************val*************\n",
      "Epoch [13/30] Loss: 2.0560   Acc: 39.1667\n",
      "Epoch 13/29\n",
      "----------\n",
      "********************train*************\n",
      "Epoch [14/30] Loss: 1.7662   Acc: 46.3889\n",
      "********************val*************\n",
      "Epoch [14/30] Loss: 2.0305   Acc: 41.2037\n",
      "Epoch 14/29\n",
      "----------\n",
      "********************train*************\n",
      "Epoch [15/30] Loss: 1.7289   Acc: 48.0159\n",
      "********************val*************\n",
      "Epoch [15/30] Loss: 2.0063   Acc: 42.8704\n",
      "Epoch 15/29\n",
      "----------\n",
      "********************train*************\n",
      "Epoch [16/30] Loss: 1.6928   Acc: 48.3730\n",
      "********************val*************\n",
      "Epoch [16/30] Loss: 1.9833   Acc: 43.7963\n",
      "Epoch 16/29\n",
      "----------\n",
      "********************train*************\n",
      "Epoch [17/30] Loss: 1.6579   Acc: 49.4841\n",
      "********************val*************\n",
      "Epoch [17/30] Loss: 1.9613   Acc: 45.4630\n",
      "Epoch 17/29\n",
      "----------\n",
      "********************train*************\n",
      "Epoch [18/30] Loss: 1.6244   Acc: 50.9524\n",
      "********************val*************\n",
      "Epoch [18/30] Loss: 1.9401   Acc: 47.5000\n",
      "Epoch 18/29\n",
      "----------\n",
      "********************train*************\n",
      "Epoch [19/30] Loss: 1.5920   Acc: 51.9444\n",
      "********************val*************\n",
      "Epoch [19/30] Loss: 1.9195   Acc: 47.7778\n",
      "Epoch 19/29\n",
      "----------\n",
      "********************train*************\n",
      "Epoch [20/30] Loss: 1.5605   Acc: 52.9762\n",
      "********************val*************\n",
      "Epoch [20/30] Loss: 1.8993   Acc: 49.3519\n",
      "Epoch 20/29\n",
      "----------\n",
      "********************train*************\n",
      "Epoch [21/30] Loss: 1.5296   Acc: 54.5635\n",
      "********************val*************\n",
      "Epoch [21/30] Loss: 1.8795   Acc: 50.1852\n",
      "Epoch 21/29\n",
      "----------\n",
      "********************train*************\n",
      "Epoch [22/30] Loss: 1.4992   Acc: 55.7937\n",
      "********************val*************\n",
      "Epoch [22/30] Loss: 1.8599   Acc: 52.0370\n",
      "Epoch 22/29\n",
      "----------\n",
      "********************train*************\n",
      "Epoch [23/30] Loss: 1.4692   Acc: 56.7063\n",
      "********************val*************\n",
      "Epoch [23/30] Loss: 1.8405   Acc: 52.6852\n",
      "Epoch 23/29\n",
      "----------\n",
      "********************train*************\n",
      "Epoch [24/30] Loss: 1.4393   Acc: 57.8968\n",
      "********************val*************\n",
      "Epoch [24/30] Loss: 1.8212   Acc: 54.0741\n",
      "Epoch 24/29\n",
      "----------\n",
      "********************train*************\n",
      "Epoch [25/30] Loss: 1.4097   Acc: 58.7698\n",
      "********************val*************\n",
      "Epoch [25/30] Loss: 1.8021   Acc: 54.5370\n",
      "Epoch 25/29\n",
      "----------\n",
      "********************train*************\n",
      "Epoch [26/30] Loss: 1.3804   Acc: 59.5635\n",
      "********************val*************\n",
      "Epoch [26/30] Loss: 1.7831   Acc: 56.1111\n",
      "Epoch 26/29\n",
      "----------\n",
      "********************train*************\n",
      "Epoch [27/30] Loss: 1.3517   Acc: 60.7937\n",
      "********************val*************\n",
      "Epoch [27/30] Loss: 1.7642   Acc: 56.9444\n",
      "Epoch 27/29\n",
      "----------\n",
      "********************train*************\n",
      "Epoch [28/30] Loss: 1.3235   Acc: 62.5000\n",
      "********************val*************\n",
      "Epoch [28/30] Loss: 1.7455   Acc: 58.3333\n",
      "Epoch 28/29\n",
      "----------\n",
      "********************train*************\n",
      "Epoch [29/30] Loss: 1.2960   Acc: 63.4921\n",
      "********************val*************\n",
      "Epoch [29/30] Loss: 1.7269   Acc: 59.0741\n",
      "Epoch 29/29\n",
      "----------\n",
      "********************train*************\n",
      "Epoch [30/30] Loss: 1.2692   Acc: 64.1667\n",
      "********************val*************\n",
      "Epoch [30/30] Loss: 1.7086   Acc: 60.5556\n",
      "Training complete in 0m 1s\n",
      "Best val Acc: 60.555556\n",
      "Epoch generations ( 1 /100) :accuracy(fittest mask) tensor(64.7619, dtype=torch.float64) keep_prob 0.4652777777777778\n",
      "Epoch generations ( 2 /100) :accuracy(fittest mask) tensor(64.4841, dtype=torch.float64) keep_prob 0.5208333333333334\n",
      "Epoch generations ( 3 /100) :accuracy(fittest mask) tensor(62.5397, dtype=torch.float64) keep_prob 0.5277777777777778\n",
      "Epoch generations ( 4 /100) :accuracy(fittest mask) tensor(66.1905, dtype=torch.float64) keep_prob 0.5416666666666666\n",
      "Epoch generations ( 5 /100) :accuracy(fittest mask) tensor(61.9444, dtype=torch.float64) keep_prob 0.5555555555555556\n",
      "Epoch generations ( 6 /100) :accuracy(fittest mask) tensor(66.1905, dtype=torch.float64) keep_prob 0.5416666666666666\n",
      "Epoch generations ( 7 /100) :accuracy(fittest mask) tensor(65.1190, dtype=torch.float64) keep_prob 0.6041666666666666\n",
      "Epoch generations ( 8 /100) :accuracy(fittest mask) tensor(59.5635, dtype=torch.float64) keep_prob 0.6319444444444444\n",
      "Epoch generations ( 9 /100) :accuracy(fittest mask) tensor(62.3016, dtype=torch.float64) keep_prob 0.6666666666666666\n",
      "Epoch generations ( 10 /100) :accuracy(fittest mask) tensor(61.1508, dtype=torch.float64) keep_prob 0.6736111111111112\n",
      "Epoch 0/29\n",
      "----------\n",
      "********************train*************\n",
      "Epoch [1/30] Loss: 1.6366   Acc: 61.1508\n",
      "********************val*************\n",
      "Epoch [1/30] Loss: 1.6805   Acc: 60.0926\n",
      "Epoch 1/29\n",
      "----------\n",
      "********************train*************\n",
      "Epoch [2/30] Loss: 1.5987   Acc: 62.8175\n",
      "********************val*************\n",
      "Epoch [2/30] Loss: 1.6498   Acc: 60.2778\n",
      "Epoch 2/29\n",
      "----------\n",
      "********************train*************\n",
      "Epoch [3/30] Loss: 1.5551   Acc: 64.0079\n",
      "********************val*************\n",
      "Epoch [3/30] Loss: 1.6190   Acc: 59.6296\n",
      "Epoch 3/29\n",
      "----------\n",
      "********************train*************\n",
      "Epoch [4/30] Loss: 1.5098   Acc: 66.7460\n",
      "********************val*************\n",
      "Epoch [4/30] Loss: 1.5901   Acc: 58.7037\n",
      "Epoch 4/29\n",
      "----------\n",
      "********************train*************\n",
      "Epoch [5/30] Loss: 1.4658   Acc: 68.5317\n",
      "********************val*************\n",
      "Epoch [5/30] Loss: 1.5639   Acc: 57.4074\n",
      "Epoch 5/29\n",
      "----------\n",
      "********************train*************\n",
      "Epoch [6/30] Loss: 1.4253   Acc: 67.3016\n",
      "********************val*************\n",
      "Epoch [6/30] Loss: 1.5405   Acc: 57.0370\n",
      "Epoch 6/29\n",
      "----------\n",
      "********************train*************\n",
      "Epoch [7/30] Loss: 1.3892   Acc: 65.8730\n",
      "********************val*************\n",
      "Epoch [7/30] Loss: 1.5183   Acc: 56.6667\n",
      "Epoch 7/29\n",
      "----------\n",
      "********************train*************\n",
      "Epoch [8/30] Loss: 1.3565   Acc: 65.0794\n",
      "********************val*************\n",
      "Epoch [8/30] Loss: 1.4958   Acc: 56.2963\n",
      "Epoch 8/29\n",
      "----------\n",
      "********************train*************\n",
      "Epoch [9/30] Loss: 1.3256   Acc: 64.3651\n",
      "********************val*************\n",
      "Epoch [9/30] Loss: 1.4720   Acc: 56.8519\n",
      "Epoch 9/29\n",
      "----------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "********************train*************\n",
      "Epoch [10/30] Loss: 1.2948   Acc: 64.5238\n",
      "********************val*************\n",
      "Epoch [10/30] Loss: 1.4467   Acc: 57.5926\n",
      "Epoch 10/29\n",
      "----------\n",
      "********************train*************\n",
      "Epoch [11/30] Loss: 1.2642   Acc: 64.6825\n",
      "********************val*************\n",
      "Epoch [11/30] Loss: 1.4205   Acc: 58.3333\n",
      "Epoch 11/29\n",
      "----------\n",
      "********************train*************\n",
      "Epoch [12/30] Loss: 1.2340   Acc: 65.5159\n",
      "********************val*************\n",
      "Epoch [12/30] Loss: 1.3941   Acc: 59.5370\n",
      "Epoch 12/29\n",
      "----------\n",
      "********************train*************\n",
      "Epoch [13/30] Loss: 1.2050   Acc: 67.1825\n",
      "********************val*************\n",
      "Epoch [13/30] Loss: 1.3680   Acc: 60.1852\n",
      "Epoch 13/29\n",
      "----------\n",
      "********************train*************\n",
      "Epoch [14/30] Loss: 1.1770   Acc: 68.1746\n",
      "********************val*************\n",
      "Epoch [14/30] Loss: 1.3427   Acc: 61.1111\n",
      "Epoch 14/29\n",
      "----------\n",
      "********************train*************\n",
      "Epoch [15/30] Loss: 1.1497   Acc: 68.7698\n",
      "********************val*************\n",
      "Epoch [15/30] Loss: 1.3183   Acc: 61.2037\n",
      "Epoch 15/29\n",
      "----------\n",
      "********************train*************\n",
      "Epoch [16/30] Loss: 1.1228   Acc: 70.5952\n",
      "********************val*************\n",
      "Epoch [16/30] Loss: 1.2949   Acc: 62.8704\n",
      "Epoch 16/29\n",
      "----------\n",
      "********************train*************\n",
      "Epoch [17/30] Loss: 1.0960   Acc: 71.9444\n",
      "********************val*************\n",
      "Epoch [17/30] Loss: 1.2727   Acc: 63.9815\n",
      "Epoch 17/29\n",
      "----------\n",
      "********************train*************\n",
      "Epoch [18/30] Loss: 1.0695   Acc: 72.3810\n",
      "********************val*************\n",
      "Epoch [18/30] Loss: 1.2518   Acc: 65.1852\n",
      "Epoch 18/29\n",
      "----------\n",
      "********************train*************\n",
      "Epoch [19/30] Loss: 1.0435   Acc: 73.6111\n",
      "********************val*************\n",
      "Epoch [19/30] Loss: 1.2321   Acc: 66.1111\n",
      "Epoch 19/29\n",
      "----------\n",
      "********************train*************\n",
      "Epoch [20/30] Loss: 1.0184   Acc: 74.4048\n",
      "********************val*************\n",
      "Epoch [20/30] Loss: 1.2136   Acc: 67.4074\n",
      "Epoch 20/29\n",
      "----------\n",
      "********************train*************\n",
      "Epoch [21/30] Loss: 0.9941   Acc: 74.8810\n",
      "********************val*************\n",
      "Epoch [21/30] Loss: 1.1961   Acc: 68.0556\n",
      "Epoch 21/29\n",
      "----------\n",
      "********************train*************\n",
      "Epoch [22/30] Loss: 0.9707   Acc: 75.9127\n",
      "********************val*************\n",
      "Epoch [22/30] Loss: 1.1793   Acc: 67.9630\n",
      "Epoch 22/29\n",
      "----------\n",
      "********************train*************\n",
      "Epoch [23/30] Loss: 0.9482   Acc: 76.0714\n",
      "********************val*************\n",
      "Epoch [23/30] Loss: 1.1629   Acc: 68.3333\n",
      "Epoch 23/29\n",
      "----------\n",
      "********************train*************\n",
      "Epoch [24/30] Loss: 0.9265   Acc: 77.1429\n",
      "********************val*************\n",
      "Epoch [24/30] Loss: 1.1465   Acc: 68.7963\n",
      "Epoch 24/29\n",
      "----------\n",
      "********************train*************\n",
      "Epoch [25/30] Loss: 0.9055   Acc: 77.7381\n",
      "********************val*************\n",
      "Epoch [25/30] Loss: 1.1299   Acc: 69.2593\n",
      "Epoch 25/29\n",
      "----------\n",
      "********************train*************\n",
      "Epoch [26/30] Loss: 0.8849   Acc: 77.9762\n",
      "********************val*************\n",
      "Epoch [26/30] Loss: 1.1130   Acc: 69.6296\n",
      "Epoch 26/29\n",
      "----------\n",
      "********************train*************\n",
      "Epoch [27/30] Loss: 0.8647   Acc: 78.4921\n",
      "********************val*************\n",
      "Epoch [27/30] Loss: 1.0959   Acc: 69.8148\n",
      "Epoch 27/29\n",
      "----------\n",
      "********************train*************\n",
      "Epoch [28/30] Loss: 0.8451   Acc: 79.0476\n",
      "********************val*************\n",
      "Epoch [28/30] Loss: 1.0786   Acc: 70.3704\n",
      "Epoch 28/29\n",
      "----------\n",
      "********************train*************\n",
      "Epoch [29/30] Loss: 0.8259   Acc: 79.2857\n",
      "********************val*************\n",
      "Epoch [29/30] Loss: 1.0612   Acc: 71.2963\n",
      "Epoch 29/29\n",
      "----------\n",
      "********************train*************\n",
      "Epoch [30/30] Loss: 0.8072   Acc: 79.7619\n",
      "********************val*************\n",
      "Epoch [30/30] Loss: 1.0440   Acc: 71.6667\n",
      "Training complete in 0m 1s\n",
      "Best val Acc: 71.666667\n",
      "Epoch generations ( 11 /100) :accuracy(fittest mask) tensor(78.6508, dtype=torch.float64) keep_prob 0.625\n",
      "Epoch generations ( 12 /100) :accuracy(fittest mask) tensor(77.6984, dtype=torch.float64) keep_prob 0.6180555555555556\n",
      "Epoch generations ( 13 /100) :accuracy(fittest mask) tensor(78.8492, dtype=torch.float64) keep_prob 0.6319444444444444\n",
      "Epoch generations ( 14 /100) :accuracy(fittest mask) tensor(79.7222, dtype=torch.float64) keep_prob 0.6388888888888888\n",
      "Epoch generations ( 15 /100) :accuracy(fittest mask) tensor(78.5317, dtype=torch.float64) keep_prob 0.6111111111111112\n",
      "Epoch generations ( 16 /100) :accuracy(fittest mask) tensor(78.0159, dtype=torch.float64) keep_prob 0.6597222222222222\n",
      "Epoch generations ( 17 /100) :accuracy(fittest mask) tensor(78.0159, dtype=torch.float64) keep_prob 0.6875\n",
      "Epoch generations ( 18 /100) :accuracy(fittest mask) tensor(78.6905, dtype=torch.float64) keep_prob 0.6875\n",
      "Epoch generations ( 19 /100) :accuracy(fittest mask) tensor(79.4444, dtype=torch.float64) keep_prob 0.6805555555555556\n",
      "Epoch generations ( 20 /100) :accuracy(fittest mask) tensor(79.4048, dtype=torch.float64) keep_prob 0.6875\n",
      "Epoch 0/29\n",
      "----------\n",
      "********************train*************\n",
      "Epoch [1/30] Loss: 0.8718   Acc: 79.4048\n",
      "********************val*************\n",
      "Epoch [1/30] Loss: 1.0258   Acc: 71.7593\n",
      "Epoch 1/29\n",
      "----------\n",
      "********************train*************\n",
      "Epoch [2/30] Loss: 0.8503   Acc: 80.2778\n",
      "********************val*************\n",
      "Epoch [2/30] Loss: 1.0082   Acc: 71.6667\n",
      "Epoch 2/29\n",
      "----------\n",
      "********************train*************\n",
      "Epoch [3/30] Loss: 0.8282   Acc: 80.7143\n",
      "********************val*************\n",
      "Epoch [3/30] Loss: 0.9920   Acc: 72.5926\n",
      "Epoch 3/29\n",
      "----------\n",
      "********************train*************\n",
      "Epoch [4/30] Loss: 0.8067   Acc: 81.3492\n",
      "********************val*************\n",
      "Epoch [4/30] Loss: 0.9772   Acc: 72.5000\n",
      "Epoch 4/29\n",
      "----------\n",
      "********************train*************\n",
      "Epoch [5/30] Loss: 0.7867   Acc: 81.9048\n",
      "********************val*************\n",
      "Epoch [5/30] Loss: 0.9636   Acc: 72.9630\n",
      "Epoch 5/29\n",
      "----------\n",
      "********************train*************\n",
      "Epoch [6/30] Loss: 0.7683   Acc: 82.5000\n",
      "********************val*************\n",
      "Epoch [6/30] Loss: 0.9507   Acc: 73.1481\n",
      "Epoch 6/29\n",
      "----------\n",
      "********************train*************\n",
      "Epoch [7/30] Loss: 0.7511   Acc: 82.9762\n",
      "********************val*************\n",
      "Epoch [7/30] Loss: 0.9378   Acc: 72.5926\n",
      "Epoch 7/29\n",
      "----------\n",
      "********************train*************\n",
      "Epoch [8/30] Loss: 0.7348   Acc: 83.4127\n",
      "********************val*************\n",
      "Epoch [8/30] Loss: 0.9246   Acc: 72.7778\n",
      "Epoch 8/29\n",
      "----------\n",
      "********************train*************\n",
      "Epoch [9/30] Loss: 0.7191   Acc: 83.4127\n",
      "********************val*************\n",
      "Epoch [9/30] Loss: 0.9110   Acc: 73.1481\n",
      "Epoch 9/29\n",
      "----------\n",
      "********************train*************\n",
      "Epoch [10/30] Loss: 0.7036   Acc: 83.8492\n",
      "********************val*************\n",
      "Epoch [10/30] Loss: 0.8968   Acc: 74.0741\n",
      "Epoch 10/29\n",
      "----------\n",
      "********************train*************\n",
      "Epoch [11/30] Loss: 0.6882   Acc: 83.9683\n",
      "********************val*************\n",
      "Epoch [11/30] Loss: 0.8822   Acc: 75.0000\n",
      "Epoch 11/29\n",
      "----------\n",
      "********************train*************\n",
      "Epoch [12/30] Loss: 0.6730   Acc: 84.2460\n",
      "********************val*************\n",
      "Epoch [12/30] Loss: 0.8675   Acc: 76.2037\n",
      "Epoch 12/29\n",
      "----------\n",
      "********************train*************\n",
      "Epoch [13/30] Loss: 0.6580   Acc: 84.3651\n",
      "********************val*************\n",
      "Epoch [13/30] Loss: 0.8529   Acc: 77.2222\n",
      "Epoch 13/29\n",
      "----------\n",
      "********************train*************\n",
      "Epoch [14/30] Loss: 0.6433   Acc: 84.6429\n",
      "********************val*************\n",
      "Epoch [14/30] Loss: 0.8387   Acc: 77.5926\n",
      "Epoch 14/29\n",
      "----------\n",
      "********************train*************\n",
      "Epoch [15/30] Loss: 0.6290   Acc: 85.4365\n",
      "********************val*************\n",
      "Epoch [15/30] Loss: 0.8251   Acc: 77.8704\n",
      "Epoch 15/29\n",
      "----------\n",
      "********************train*************\n",
      "Epoch [16/30] Loss: 0.6150   Acc: 85.4762\n",
      "********************val*************\n",
      "Epoch [16/30] Loss: 0.8122   Acc: 78.4259\n",
      "Epoch 16/29\n",
      "----------\n",
      "********************train*************\n",
      "Epoch [17/30] Loss: 0.6015   Acc: 85.8333\n",
      "********************val*************\n",
      "Epoch [17/30] Loss: 0.7999   Acc: 78.8889\n",
      "Epoch 17/29\n",
      "----------\n",
      "********************train*************\n",
      "Epoch [18/30] Loss: 0.5884   Acc: 85.9127\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "********************val*************\n",
      "Epoch [18/30] Loss: 0.7882   Acc: 79.5370\n",
      "Epoch 18/29\n",
      "----------\n",
      "********************train*************\n",
      "Epoch [19/30] Loss: 0.5757   Acc: 86.2302\n",
      "********************val*************\n",
      "Epoch [19/30] Loss: 0.7768   Acc: 79.6296\n",
      "Epoch 19/29\n",
      "----------\n",
      "********************train*************\n",
      "Epoch [20/30] Loss: 0.5635   Acc: 86.6270\n",
      "********************val*************\n",
      "Epoch [20/30] Loss: 0.7655   Acc: 79.6296\n",
      "Epoch 20/29\n",
      "----------\n",
      "********************train*************\n",
      "Epoch [21/30] Loss: 0.5515   Acc: 87.1429\n",
      "********************val*************\n",
      "Epoch [21/30] Loss: 0.7543   Acc: 80.1852\n",
      "Epoch 21/29\n",
      "----------\n",
      "********************train*************\n",
      "Epoch [22/30] Loss: 0.5398   Acc: 87.5000\n",
      "********************val*************\n",
      "Epoch [22/30] Loss: 0.7432   Acc: 80.5556\n",
      "Epoch 22/29\n",
      "----------\n",
      "********************train*************\n",
      "Epoch [23/30] Loss: 0.5283   Acc: 87.6587\n",
      "********************val*************\n",
      "Epoch [23/30] Loss: 0.7322   Acc: 80.9259\n",
      "Epoch 23/29\n",
      "----------\n",
      "********************train*************\n",
      "Epoch [24/30] Loss: 0.5170   Acc: 87.8968\n",
      "********************val*************\n",
      "Epoch [24/30] Loss: 0.7215   Acc: 81.2963\n",
      "Epoch 24/29\n",
      "----------\n",
      "********************train*************\n",
      "Epoch [25/30] Loss: 0.5061   Acc: 88.1349\n",
      "********************val*************\n",
      "Epoch [25/30] Loss: 0.7111   Acc: 81.8519\n",
      "Epoch 25/29\n",
      "----------\n",
      "********************train*************\n",
      "Epoch [26/30] Loss: 0.4954   Acc: 88.4921\n",
      "********************val*************\n",
      "Epoch [26/30] Loss: 0.7012   Acc: 82.3148\n",
      "Epoch 26/29\n",
      "----------\n",
      "********************train*************\n",
      "Epoch [27/30] Loss: 0.4851   Acc: 88.6508\n",
      "********************val*************\n",
      "Epoch [27/30] Loss: 0.6919   Acc: 82.7778\n",
      "Epoch 27/29\n",
      "----------\n",
      "********************train*************\n",
      "Epoch [28/30] Loss: 0.4750   Acc: 88.5317\n",
      "********************val*************\n",
      "Epoch [28/30] Loss: 0.6830   Acc: 83.3333\n",
      "Epoch 28/29\n",
      "----------\n",
      "********************train*************\n",
      "Epoch [29/30] Loss: 0.4652   Acc: 88.6905\n",
      "********************val*************\n",
      "Epoch [29/30] Loss: 0.6746   Acc: 83.6111\n",
      "Epoch 29/29\n",
      "----------\n",
      "********************train*************\n",
      "Epoch [30/30] Loss: 0.4556   Acc: 88.9683\n",
      "********************val*************\n",
      "Epoch [30/30] Loss: 0.6664   Acc: 84.1667\n",
      "Training complete in 0m 1s\n",
      "Best val Acc: 84.166667\n",
      "Epoch generations ( 21 /100) :accuracy(fittest mask) tensor(88.4127, dtype=torch.float64) keep_prob 0.6736111111111112\n",
      "Epoch generations ( 22 /100) :accuracy(fittest mask) tensor(87.9762, dtype=torch.float64) keep_prob 0.6527777777777778\n",
      "Epoch generations ( 23 /100) :accuracy(fittest mask) tensor(87.5397, dtype=torch.float64) keep_prob 0.6875\n",
      "Epoch generations ( 24 /100) :accuracy(fittest mask) tensor(88.0952, dtype=torch.float64) keep_prob 0.7013888888888888\n",
      "Epoch generations ( 25 /100) :accuracy(fittest mask) tensor(88.0556, dtype=torch.float64) keep_prob 0.7013888888888888\n",
      "Epoch generations ( 26 /100) :accuracy(fittest mask) tensor(86.7460, dtype=torch.float64) keep_prob 0.7291666666666666\n",
      "Epoch generations ( 27 /100) :accuracy(fittest mask) tensor(87.3016, dtype=torch.float64) keep_prob 0.7083333333333334\n",
      "Epoch generations ( 28 /100) :accuracy(fittest mask) tensor(86.5873, dtype=torch.float64) keep_prob 0.6736111111111112\n",
      "Epoch generations ( 29 /100) :accuracy(fittest mask) tensor(87.1429, dtype=torch.float64) keep_prob 0.6666666666666666\n",
      "Epoch generations ( 30 /100) :accuracy(fittest mask) tensor(87.1825, dtype=torch.float64) keep_prob 0.6944444444444444\n",
      "Epoch 0/29\n",
      "----------\n",
      "********************train*************\n",
      "Epoch [1/30] Loss: 0.5422   Acc: 87.1825\n",
      "********************val*************\n",
      "Epoch [1/30] Loss: 0.6542   Acc: 84.6296\n",
      "Epoch 1/29\n",
      "----------\n",
      "********************train*************\n",
      "Epoch [2/30] Loss: 0.5242   Acc: 88.0952\n",
      "********************val*************\n",
      "Epoch [2/30] Loss: 0.6417   Acc: 85.1852\n",
      "Epoch 2/29\n",
      "----------\n",
      "********************train*************\n",
      "Epoch [3/30] Loss: 0.5044   Acc: 88.3333\n",
      "********************val*************\n",
      "Epoch [3/30] Loss: 0.6317   Acc: 85.3704\n",
      "Epoch 3/29\n",
      "----------\n",
      "********************train*************\n",
      "Epoch [4/30] Loss: 0.4876   Acc: 88.7302\n",
      "********************val*************\n",
      "Epoch [4/30] Loss: 0.6248   Acc: 84.9074\n",
      "Epoch 4/29\n",
      "----------\n",
      "********************train*************\n",
      "Epoch [5/30] Loss: 0.4755   Acc: 88.9683\n",
      "********************val*************\n",
      "Epoch [5/30] Loss: 0.6199   Acc: 85.0000\n",
      "Epoch 5/29\n",
      "----------\n",
      "********************train*************\n",
      "Epoch [6/30] Loss: 0.4665   Acc: 88.8492\n",
      "********************val*************\n",
      "Epoch [6/30] Loss: 0.6152   Acc: 84.8148\n",
      "Epoch 6/29\n",
      "----------\n",
      "********************train*************\n",
      "Epoch [7/30] Loss: 0.4583   Acc: 88.9286\n",
      "********************val*************\n",
      "Epoch [7/30] Loss: 0.6099   Acc: 84.9074\n",
      "Epoch 7/29\n",
      "----------\n",
      "********************train*************\n",
      "Epoch [8/30] Loss: 0.4497   Acc: 89.3254\n",
      "********************val*************\n",
      "Epoch [8/30] Loss: 0.6041   Acc: 84.8148\n",
      "Epoch 8/29\n",
      "----------\n",
      "********************train*************\n",
      "Epoch [9/30] Loss: 0.4405   Acc: 89.5635\n",
      "********************val*************\n",
      "Epoch [9/30] Loss: 0.5982   Acc: 85.1852\n",
      "Epoch 9/29\n",
      "----------\n",
      "********************train*************\n",
      "Epoch [10/30] Loss: 0.4312   Acc: 90.1587\n",
      "********************val*************\n",
      "Epoch [10/30] Loss: 0.5920   Acc: 85.0000\n",
      "Epoch 10/29\n",
      "----------\n",
      "********************train*************\n",
      "Epoch [11/30] Loss: 0.4222   Acc: 89.7619\n",
      "********************val*************\n",
      "Epoch [11/30] Loss: 0.5852   Acc: 85.1852\n",
      "Epoch 11/29\n",
      "----------\n",
      "********************train*************\n",
      "Epoch [12/30] Loss: 0.4132   Acc: 90.0397\n",
      "********************val*************\n",
      "Epoch [12/30] Loss: 0.5773   Acc: 85.2778\n",
      "Epoch 12/29\n",
      "----------\n",
      "********************train*************\n",
      "Epoch [13/30] Loss: 0.4039   Acc: 90.3175\n",
      "********************val*************\n",
      "Epoch [13/30] Loss: 0.5685   Acc: 85.7407\n",
      "Epoch 13/29\n",
      "----------\n",
      "********************train*************\n",
      "Epoch [14/30] Loss: 0.3947   Acc: 90.6349\n",
      "********************val*************\n",
      "Epoch [14/30] Loss: 0.5591   Acc: 85.6481\n",
      "Epoch 14/29\n",
      "----------\n",
      "********************train*************\n",
      "Epoch [15/30] Loss: 0.3858   Acc: 91.3889\n",
      "********************val*************\n",
      "Epoch [15/30] Loss: 0.5497   Acc: 86.0185\n",
      "Epoch 15/29\n",
      "----------\n",
      "********************train*************\n",
      "Epoch [16/30] Loss: 0.3775   Acc: 91.7063\n",
      "********************val*************\n",
      "Epoch [16/30] Loss: 0.5405   Acc: 86.8519\n",
      "Epoch 16/29\n",
      "----------\n",
      "********************train*************\n",
      "Epoch [17/30] Loss: 0.3698   Acc: 91.8651\n",
      "********************val*************\n",
      "Epoch [17/30] Loss: 0.5319   Acc: 87.0370\n",
      "Epoch 17/29\n",
      "----------\n",
      "********************train*************\n",
      "Epoch [18/30] Loss: 0.3625   Acc: 91.8651\n",
      "********************val*************\n",
      "Epoch [18/30] Loss: 0.5238   Acc: 87.5000\n",
      "Epoch 18/29\n",
      "----------\n",
      "********************train*************\n",
      "Epoch [19/30] Loss: 0.3554   Acc: 92.3413\n",
      "********************val*************\n",
      "Epoch [19/30] Loss: 0.5164   Acc: 87.8704\n",
      "Epoch 19/29\n",
      "----------\n",
      "********************train*************\n",
      "Epoch [20/30] Loss: 0.3482   Acc: 92.3413\n",
      "********************val*************\n",
      "Epoch [20/30] Loss: 0.5099   Acc: 87.8704\n",
      "Epoch 20/29\n",
      "----------\n",
      "********************train*************\n",
      "Epoch [21/30] Loss: 0.3412   Acc: 92.4603\n",
      "********************val*************\n",
      "Epoch [21/30] Loss: 0.5044   Acc: 87.9630\n",
      "Epoch 21/29\n",
      "----------\n",
      "********************train*************\n",
      "Epoch [22/30] Loss: 0.3343   Acc: 92.8968\n",
      "********************val*************\n",
      "Epoch [22/30] Loss: 0.4996   Acc: 88.0556\n",
      "Epoch 22/29\n",
      "----------\n",
      "********************train*************\n",
      "Epoch [23/30] Loss: 0.3275   Acc: 92.8968\n",
      "********************val*************\n",
      "Epoch [23/30] Loss: 0.4952   Acc: 88.0556\n",
      "Epoch 23/29\n",
      "----------\n",
      "********************train*************\n",
      "Epoch [24/30] Loss: 0.3209   Acc: 92.9365\n",
      "********************val*************\n",
      "Epoch [24/30] Loss: 0.4909   Acc: 88.0556\n",
      "Epoch 24/29\n",
      "----------\n",
      "********************train*************\n",
      "Epoch [25/30] Loss: 0.3143   Acc: 93.3730\n",
      "********************val*************\n",
      "Epoch [25/30] Loss: 0.4863   Acc: 87.8704\n",
      "Epoch 25/29\n",
      "----------\n",
      "********************train*************\n",
      "Epoch [26/30] Loss: 0.3079   Acc: 93.6508\n",
      "********************val*************\n",
      "Epoch [26/30] Loss: 0.4815   Acc: 87.6852\n",
      "Epoch 26/29\n",
      "----------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "********************train*************\n",
      "Epoch [27/30] Loss: 0.3017   Acc: 93.8889\n",
      "********************val*************\n",
      "Epoch [27/30] Loss: 0.4765   Acc: 87.8704\n",
      "Epoch 27/29\n",
      "----------\n",
      "********************train*************\n",
      "Epoch [28/30] Loss: 0.2956   Acc: 93.9683\n",
      "********************val*************\n",
      "Epoch [28/30] Loss: 0.4712   Acc: 87.9630\n",
      "Epoch 28/29\n",
      "----------\n",
      "********************train*************\n",
      "Epoch [29/30] Loss: 0.2898   Acc: 94.2460\n",
      "********************val*************\n",
      "Epoch [29/30] Loss: 0.4657   Acc: 88.2407\n",
      "Epoch 29/29\n",
      "----------\n",
      "********************train*************\n",
      "Epoch [30/30] Loss: 0.2842   Acc: 94.1270\n",
      "********************val*************\n",
      "Epoch [30/30] Loss: 0.4603   Acc: 88.4259\n",
      "Training complete in 0m 1s\n",
      "Best val Acc: 88.425926\n",
      "Epoch generations ( 31 /100) :accuracy(fittest mask) tensor(94.0476, dtype=torch.float64) keep_prob 0.6666666666666666\n",
      "Epoch generations ( 32 /100) :accuracy(fittest mask) tensor(94.1667, dtype=torch.float64) keep_prob 0.6944444444444444\n",
      "Epoch generations ( 33 /100) :accuracy(fittest mask) tensor(93.9286, dtype=torch.float64) keep_prob 0.7291666666666666\n",
      "Epoch generations ( 34 /100) :accuracy(fittest mask) tensor(93.9286, dtype=torch.float64) keep_prob 0.7013888888888888\n",
      "Epoch generations ( 35 /100) :accuracy(fittest mask) tensor(93.6905, dtype=torch.float64) keep_prob 0.7083333333333334\n",
      "Epoch generations ( 36 /100) :accuracy(fittest mask) tensor(94.1270, dtype=torch.float64) keep_prob 0.7013888888888888\n",
      "Epoch generations ( 37 /100) :accuracy(fittest mask) tensor(93.7302, dtype=torch.float64) keep_prob 0.7083333333333334\n",
      "Epoch generations ( 38 /100) :accuracy(fittest mask) tensor(93.7302, dtype=torch.float64) keep_prob 0.7430555555555556\n",
      "Epoch generations ( 39 /100) :accuracy(fittest mask) tensor(93.4921, dtype=torch.float64) keep_prob 0.7013888888888888\n",
      "Epoch generations ( 40 /100) :accuracy(fittest mask) tensor(93.4921, dtype=torch.float64) keep_prob 0.7013888888888888\n",
      "Epoch 0/29\n",
      "----------\n",
      "********************train*************\n",
      "Epoch [1/30] Loss: 0.3151   Acc: 93.4921\n",
      "********************val*************\n",
      "Epoch [1/30] Loss: 0.4536   Acc: 88.4259\n",
      "Epoch 1/29\n",
      "----------\n",
      "********************train*************\n",
      "Epoch [2/30] Loss: 0.3062   Acc: 93.6508\n",
      "********************val*************\n",
      "Epoch [2/30] Loss: 0.4473   Acc: 88.5185\n",
      "Epoch 2/29\n",
      "----------\n",
      "********************train*************\n",
      "Epoch [3/30] Loss: 0.2971   Acc: 93.4127\n",
      "********************val*************\n",
      "Epoch [3/30] Loss: 0.4422   Acc: 88.7037\n",
      "Epoch 3/29\n",
      "----------\n",
      "********************train*************\n",
      "Epoch [4/30] Loss: 0.2893   Acc: 93.4921\n",
      "********************val*************\n",
      "Epoch [4/30] Loss: 0.4378   Acc: 88.7037\n",
      "Epoch 4/29\n",
      "----------\n",
      "********************train*************\n",
      "Epoch [5/30] Loss: 0.2829   Acc: 94.0873\n",
      "********************val*************\n",
      "Epoch [5/30] Loss: 0.4334   Acc: 88.7037\n",
      "Epoch 5/29\n",
      "----------\n",
      "********************train*************\n",
      "Epoch [6/30] Loss: 0.2768   Acc: 94.2063\n",
      "********************val*************\n",
      "Epoch [6/30] Loss: 0.4286   Acc: 88.8889\n",
      "Epoch 6/29\n",
      "----------\n",
      "********************train*************\n",
      "Epoch [7/30] Loss: 0.2708   Acc: 94.3651\n",
      "********************val*************\n",
      "Epoch [7/30] Loss: 0.4238   Acc: 88.9815\n",
      "Epoch 7/29\n",
      "----------\n",
      "********************train*************\n",
      "Epoch [8/30] Loss: 0.2649   Acc: 94.7619\n",
      "********************val*************\n",
      "Epoch [8/30] Loss: 0.4192   Acc: 88.7963\n",
      "Epoch 8/29\n",
      "----------\n",
      "********************train*************\n",
      "Epoch [9/30] Loss: 0.2594   Acc: 95.0397\n",
      "********************val*************\n",
      "Epoch [9/30] Loss: 0.4149   Acc: 89.4444\n",
      "Epoch 9/29\n",
      "----------\n",
      "********************train*************\n",
      "Epoch [10/30] Loss: 0.2543   Acc: 95.3968\n",
      "********************val*************\n",
      "Epoch [10/30] Loss: 0.4104   Acc: 89.5370\n",
      "Epoch 10/29\n",
      "----------\n",
      "********************train*************\n",
      "Epoch [11/30] Loss: 0.2491   Acc: 95.5159\n",
      "********************val*************\n",
      "Epoch [11/30] Loss: 0.4059   Acc: 89.5370\n",
      "Epoch 11/29\n",
      "----------\n",
      "********************train*************\n",
      "Epoch [12/30] Loss: 0.2437   Acc: 95.6746\n",
      "********************val*************\n",
      "Epoch [12/30] Loss: 0.4014   Acc: 89.8148\n",
      "Epoch 12/29\n",
      "----------\n",
      "********************train*************\n",
      "Epoch [13/30] Loss: 0.2383   Acc: 95.9921\n",
      "********************val*************\n",
      "Epoch [13/30] Loss: 0.3972   Acc: 90.0926\n",
      "Epoch 13/29\n",
      "----------\n",
      "********************train*************\n",
      "Epoch [14/30] Loss: 0.2334   Acc: 96.0317\n",
      "********************val*************\n",
      "Epoch [14/30] Loss: 0.3934   Acc: 90.1852\n",
      "Epoch 14/29\n",
      "----------\n",
      "********************train*************\n",
      "Epoch [15/30] Loss: 0.2289   Acc: 96.1905\n",
      "********************val*************\n",
      "Epoch [15/30] Loss: 0.3896   Acc: 90.2778\n",
      "Epoch 15/29\n",
      "----------\n",
      "********************train*************\n",
      "Epoch [16/30] Loss: 0.2246   Acc: 96.2302\n",
      "********************val*************\n",
      "Epoch [16/30] Loss: 0.3856   Acc: 90.0926\n",
      "Epoch 16/29\n",
      "----------\n",
      "********************train*************\n",
      "Epoch [17/30] Loss: 0.2203   Acc: 96.1508\n",
      "********************val*************\n",
      "Epoch [17/30] Loss: 0.3814   Acc: 90.0926\n",
      "Epoch 17/29\n",
      "----------\n",
      "********************train*************\n",
      "Epoch [18/30] Loss: 0.2158   Acc: 96.3492\n",
      "********************val*************\n",
      "Epoch [18/30] Loss: 0.3771   Acc: 89.8148\n",
      "Epoch 18/29\n",
      "----------\n",
      "********************train*************\n",
      "Epoch [19/30] Loss: 0.2113   Acc: 96.3492\n",
      "********************val*************\n",
      "Epoch [19/30] Loss: 0.3729   Acc: 90.0000\n",
      "Epoch 19/29\n",
      "----------\n",
      "********************train*************\n",
      "Epoch [20/30] Loss: 0.2070   Acc: 96.2302\n",
      "********************val*************\n",
      "Epoch [20/30] Loss: 0.3688   Acc: 90.0000\n",
      "Epoch 20/29\n",
      "----------\n",
      "********************train*************\n",
      "Epoch [21/30] Loss: 0.2029   Acc: 96.5873\n",
      "********************val*************\n",
      "Epoch [21/30] Loss: 0.3649   Acc: 90.0926\n",
      "Epoch 21/29\n",
      "----------\n",
      "********************train*************\n",
      "Epoch [22/30] Loss: 0.1990   Acc: 96.8651\n",
      "********************val*************\n",
      "Epoch [22/30] Loss: 0.3610   Acc: 90.3704\n",
      "Epoch 22/29\n",
      "----------\n",
      "********************train*************\n",
      "Epoch [23/30] Loss: 0.1951   Acc: 96.8651\n",
      "********************val*************\n",
      "Epoch [23/30] Loss: 0.3571   Acc: 90.5556\n",
      "Epoch 23/29\n",
      "----------\n",
      "********************train*************\n",
      "Epoch [24/30] Loss: 0.1912   Acc: 96.9444\n",
      "********************val*************\n",
      "Epoch [24/30] Loss: 0.3533   Acc: 91.0185\n",
      "Epoch 24/29\n",
      "----------\n",
      "********************train*************\n",
      "Epoch [25/30] Loss: 0.1875   Acc: 97.0635\n",
      "********************val*************\n",
      "Epoch [25/30] Loss: 0.3494   Acc: 91.2037\n",
      "Epoch 25/29\n",
      "----------\n",
      "********************train*************\n",
      "Epoch [26/30] Loss: 0.1838   Acc: 97.1429\n",
      "********************val*************\n",
      "Epoch [26/30] Loss: 0.3456   Acc: 91.7593\n",
      "Epoch 26/29\n",
      "----------\n",
      "********************train*************\n",
      "Epoch [27/30] Loss: 0.1802   Acc: 97.1825\n",
      "********************val*************\n",
      "Epoch [27/30] Loss: 0.3418   Acc: 92.0370\n",
      "Epoch 27/29\n",
      "----------\n",
      "********************train*************\n",
      "Epoch [28/30] Loss: 0.1767   Acc: 97.1825\n",
      "********************val*************\n",
      "Epoch [28/30] Loss: 0.3381   Acc: 92.2222\n",
      "Epoch 28/29\n",
      "----------\n",
      "********************train*************\n",
      "Epoch [29/30] Loss: 0.1732   Acc: 97.2619\n",
      "********************val*************\n",
      "Epoch [29/30] Loss: 0.3345   Acc: 92.2222\n",
      "Epoch 29/29\n",
      "----------\n",
      "********************train*************\n",
      "Epoch [30/30] Loss: 0.1699   Acc: 97.3413\n",
      "********************val*************\n",
      "Epoch [30/30] Loss: 0.3311   Acc: 92.2222\n",
      "Training complete in 0m 1s\n",
      "Best val Acc: 92.222222\n",
      "Epoch generations ( 41 /100) :accuracy(fittest mask) tensor(97.2619, dtype=torch.float64) keep_prob 0.7083333333333334\n",
      "Epoch generations ( 42 /100) :accuracy(fittest mask) tensor(96.8254, dtype=torch.float64) keep_prob 0.7222222222222222\n",
      "Epoch generations ( 43 /100) :accuracy(fittest mask) tensor(96.7063, dtype=torch.float64) keep_prob 0.7430555555555556\n",
      "Epoch generations ( 44 /100) :accuracy(fittest mask) tensor(96.7460, dtype=torch.float64) keep_prob 0.75\n",
      "Epoch generations ( 45 /100) :accuracy(fittest mask) tensor(96.5476, dtype=torch.float64) keep_prob 0.75\n",
      "Epoch generations ( 46 /100) :accuracy(fittest mask) tensor(96.6270, dtype=torch.float64) keep_prob 0.7430555555555556\n",
      "Epoch generations ( 47 /100) :accuracy(fittest mask) tensor(96.7063, dtype=torch.float64) keep_prob 0.75\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch generations ( 48 /100) :accuracy(fittest mask) tensor(96.7857, dtype=torch.float64) keep_prob 0.7638888888888888\n",
      "Epoch generations ( 49 /100) :accuracy(fittest mask) tensor(96.8254, dtype=torch.float64) keep_prob 0.7638888888888888\n",
      "Epoch generations ( 50 /100) :accuracy(fittest mask) tensor(96.6667, dtype=torch.float64) keep_prob 0.75\n",
      "Epoch 0/29\n",
      "----------\n",
      "********************train*************\n",
      "Epoch [1/30] Loss: 0.1962   Acc: 96.6667\n",
      "********************val*************\n",
      "Epoch [1/30] Loss: 0.3341   Acc: 92.1296\n",
      "Epoch 1/29\n",
      "----------\n",
      "********************train*************\n",
      "Epoch [2/30] Loss: 0.1918   Acc: 96.6667\n",
      "********************val*************\n",
      "Epoch [2/30] Loss: 0.3299   Acc: 92.3148\n",
      "Epoch 2/29\n",
      "----------\n",
      "********************train*************\n",
      "Epoch [3/30] Loss: 0.1869   Acc: 96.7857\n",
      "********************val*************\n",
      "Epoch [3/30] Loss: 0.3259   Acc: 92.1296\n",
      "Epoch 3/29\n",
      "----------\n",
      "********************train*************\n",
      "Epoch [4/30] Loss: 0.1819   Acc: 96.7857\n",
      "********************val*************\n",
      "Epoch [4/30] Loss: 0.3220   Acc: 92.2222\n",
      "Epoch 4/29\n",
      "----------\n",
      "********************train*************\n",
      "Epoch [5/30] Loss: 0.1772   Acc: 97.1032\n",
      "********************val*************\n",
      "Epoch [5/30] Loss: 0.3184   Acc: 92.9630\n",
      "Epoch 5/29\n",
      "----------\n",
      "********************train*************\n",
      "Epoch [6/30] Loss: 0.1728   Acc: 97.3016\n",
      "********************val*************\n",
      "Epoch [6/30] Loss: 0.3151   Acc: 92.9630\n",
      "Epoch 6/29\n",
      "----------\n",
      "********************train*************\n",
      "Epoch [7/30] Loss: 0.1689   Acc: 97.3810\n",
      "********************val*************\n",
      "Epoch [7/30] Loss: 0.3121   Acc: 92.9630\n",
      "Epoch 7/29\n",
      "----------\n",
      "********************train*************\n",
      "Epoch [8/30] Loss: 0.1655   Acc: 97.3413\n",
      "********************val*************\n",
      "Epoch [8/30] Loss: 0.3094   Acc: 93.1481\n",
      "Epoch 8/29\n",
      "----------\n",
      "********************train*************\n",
      "Epoch [9/30] Loss: 0.1625   Acc: 97.5794\n",
      "********************val*************\n",
      "Epoch [9/30] Loss: 0.3068   Acc: 93.1481\n",
      "Epoch 9/29\n",
      "----------\n",
      "********************train*************\n",
      "Epoch [10/30] Loss: 0.1596   Acc: 97.6190\n",
      "********************val*************\n",
      "Epoch [10/30] Loss: 0.3042   Acc: 93.0556\n",
      "Epoch 10/29\n",
      "----------\n",
      "********************train*************\n",
      "Epoch [11/30] Loss: 0.1569   Acc: 97.8968\n",
      "********************val*************\n",
      "Epoch [11/30] Loss: 0.3014   Acc: 93.0556\n",
      "Epoch 11/29\n",
      "----------\n",
      "********************train*************\n",
      "Epoch [12/30] Loss: 0.1542   Acc: 98.0556\n",
      "********************val*************\n",
      "Epoch [12/30] Loss: 0.2984   Acc: 93.0556\n",
      "Epoch 12/29\n",
      "----------\n",
      "********************train*************\n",
      "Epoch [13/30] Loss: 0.1515   Acc: 98.0556\n",
      "********************val*************\n",
      "Epoch [13/30] Loss: 0.2951   Acc: 92.9630\n",
      "Epoch 13/29\n",
      "----------\n",
      "********************train*************\n",
      "Epoch [14/30] Loss: 0.1487   Acc: 98.2540\n",
      "********************val*************\n",
      "Epoch [14/30] Loss: 0.2915   Acc: 93.0556\n",
      "Epoch 14/29\n",
      "----------\n",
      "********************train*************\n",
      "Epoch [15/30] Loss: 0.1459   Acc: 98.2143\n",
      "********************val*************\n",
      "Epoch [15/30] Loss: 0.2878   Acc: 93.1481\n",
      "Epoch 15/29\n",
      "----------\n",
      "********************train*************\n",
      "Epoch [16/30] Loss: 0.1430   Acc: 98.2540\n",
      "********************val*************\n",
      "Epoch [16/30] Loss: 0.2841   Acc: 93.1481\n",
      "Epoch 16/29\n",
      "----------\n",
      "********************train*************\n",
      "Epoch [17/30] Loss: 0.1401   Acc: 98.2540\n",
      "********************val*************\n",
      "Epoch [17/30] Loss: 0.2804   Acc: 93.3333\n",
      "Epoch 17/29\n",
      "----------\n",
      "********************train*************\n",
      "Epoch [18/30] Loss: 0.1373   Acc: 98.3730\n",
      "********************val*************\n",
      "Epoch [18/30] Loss: 0.2767   Acc: 93.3333\n",
      "Epoch 18/29\n",
      "----------\n",
      "********************train*************\n",
      "Epoch [19/30] Loss: 0.1346   Acc: 98.4921\n",
      "********************val*************\n",
      "Epoch [19/30] Loss: 0.2731   Acc: 93.6111\n",
      "Epoch 19/29\n",
      "----------\n",
      "********************train*************\n",
      "Epoch [20/30] Loss: 0.1320   Acc: 98.6111\n",
      "********************val*************\n",
      "Epoch [20/30] Loss: 0.2696   Acc: 93.7037\n",
      "Epoch 20/29\n",
      "----------\n",
      "********************train*************\n",
      "Epoch [21/30] Loss: 0.1296   Acc: 98.6508\n",
      "********************val*************\n",
      "Epoch [21/30] Loss: 0.2662   Acc: 93.7037\n",
      "Epoch 21/29\n",
      "----------\n",
      "********************train*************\n",
      "Epoch [22/30] Loss: 0.1272   Acc: 98.6508\n",
      "********************val*************\n",
      "Epoch [22/30] Loss: 0.2629   Acc: 93.7037\n",
      "Epoch 22/29\n",
      "----------\n",
      "********************train*************\n",
      "Epoch [23/30] Loss: 0.1249   Acc: 98.6508\n",
      "********************val*************\n",
      "Epoch [23/30] Loss: 0.2597   Acc: 93.7037\n",
      "Epoch 23/29\n",
      "----------\n",
      "********************train*************\n",
      "Epoch [24/30] Loss: 0.1226   Acc: 98.7302\n",
      "********************val*************\n",
      "Epoch [24/30] Loss: 0.2566   Acc: 93.7963\n",
      "Epoch 24/29\n",
      "----------\n",
      "********************train*************\n",
      "Epoch [25/30] Loss: 0.1204   Acc: 98.8492\n",
      "********************val*************\n",
      "Epoch [25/30] Loss: 0.2537   Acc: 93.9815\n",
      "Epoch 25/29\n",
      "----------\n",
      "********************train*************\n",
      "Epoch [26/30] Loss: 0.1182   Acc: 98.9286\n",
      "********************val*************\n",
      "Epoch [26/30] Loss: 0.2509   Acc: 93.7963\n",
      "Epoch 26/29\n",
      "----------\n",
      "********************train*************\n",
      "Epoch [27/30] Loss: 0.1160   Acc: 98.9286\n",
      "********************val*************\n",
      "Epoch [27/30] Loss: 0.2482   Acc: 93.7963\n",
      "Epoch 27/29\n",
      "----------\n",
      "********************train*************\n",
      "Epoch [28/30] Loss: 0.1139   Acc: 98.9286\n",
      "********************val*************\n",
      "Epoch [28/30] Loss: 0.2455   Acc: 93.9815\n",
      "Epoch 28/29\n",
      "----------\n",
      "********************train*************\n",
      "Epoch [29/30] Loss: 0.1118   Acc: 99.0079\n",
      "********************val*************\n",
      "Epoch [29/30] Loss: 0.2428   Acc: 94.4444\n",
      "Epoch 29/29\n",
      "----------\n",
      "********************train*************\n",
      "Epoch [30/30] Loss: 0.1097   Acc: 99.0079\n",
      "********************val*************\n",
      "Epoch [30/30] Loss: 0.2401   Acc: 94.7222\n",
      "Training complete in 0m 1s\n",
      "Best val Acc: 94.722222\n",
      "Epoch generations ( 51 /100) :accuracy(fittest mask) tensor(99.0079, dtype=torch.float64) keep_prob 0.7569444444444444\n",
      "Epoch generations ( 52 /100) :accuracy(fittest mask) tensor(98.5317, dtype=torch.float64) keep_prob 0.7847222222222222\n",
      "Epoch generations ( 53 /100) :accuracy(fittest mask) tensor(98.4921, dtype=torch.float64) keep_prob 0.7847222222222222\n",
      "Epoch generations ( 54 /100) :accuracy(fittest mask) tensor(98.6905, dtype=torch.float64) keep_prob 0.7986111111111112\n",
      "Epoch generations ( 55 /100) :accuracy(fittest mask) tensor(98.6905, dtype=torch.float64) keep_prob 0.7986111111111112\n",
      "Epoch generations ( 56 /100) :accuracy(fittest mask) tensor(98.5317, dtype=torch.float64) keep_prob 0.7847222222222222\n",
      "Epoch generations ( 57 /100) :accuracy(fittest mask) tensor(98.5317, dtype=torch.float64) keep_prob 0.7777777777777778\n",
      "Epoch generations ( 58 /100) :accuracy(fittest mask) tensor(98.4921, dtype=torch.float64) keep_prob 0.7777777777777778\n",
      "Epoch generations ( 59 /100) :accuracy(fittest mask) tensor(98.6508, dtype=torch.float64) keep_prob 0.7708333333333334\n",
      "Epoch generations ( 60 /100) :accuracy(fittest mask) tensor(98.5317, dtype=torch.float64) keep_prob 0.75\n",
      "Epoch 0/29\n",
      "----------\n",
      "********************train*************\n",
      "Epoch [1/30] Loss: 0.1312   Acc: 98.5317\n",
      "********************val*************\n",
      "Epoch [1/30] Loss: 0.2367   Acc: 95.4630\n",
      "Epoch 1/29\n",
      "----------\n",
      "********************train*************\n",
      "Epoch [2/30] Loss: 0.1268   Acc: 98.6508\n",
      "********************val*************\n",
      "Epoch [2/30] Loss: 0.2336   Acc: 95.7407\n",
      "Epoch 2/29\n",
      "----------\n",
      "********************train*************\n",
      "Epoch [3/30] Loss: 0.1215   Acc: 98.6905\n",
      "********************val*************\n",
      "Epoch [3/30] Loss: 0.2312   Acc: 95.7407\n",
      "Epoch 3/29\n",
      "----------\n",
      "********************train*************\n",
      "Epoch [4/30] Loss: 0.1167   Acc: 98.8492\n",
      "********************val*************\n",
      "Epoch [4/30] Loss: 0.2295   Acc: 96.1111\n",
      "Epoch 4/29\n",
      "----------\n",
      "********************train*************\n",
      "Epoch [5/30] Loss: 0.1127   Acc: 98.8889\n",
      "********************val*************\n",
      "Epoch [5/30] Loss: 0.2277   Acc: 96.2037\n",
      "Epoch 5/29\n",
      "----------\n",
      "********************train*************\n",
      "Epoch [6/30] Loss: 0.1093   Acc: 98.8889\n",
      "********************val*************\n",
      "Epoch [6/30] Loss: 0.2255   Acc: 96.1111\n",
      "Epoch 6/29\n",
      "----------\n",
      "********************train*************\n",
      "Epoch [7/30] Loss: 0.1064   Acc: 99.1270\n",
      "********************val*************\n",
      "Epoch [7/30] Loss: 0.2229   Acc: 96.3889\n",
      "Epoch 7/29\n",
      "----------\n",
      "********************train*************\n",
      "Epoch [8/30] Loss: 0.1039   Acc: 99.1270\n",
      "********************val*************\n",
      "Epoch [8/30] Loss: 0.2203   Acc: 96.6667\n",
      "Epoch 8/29\n",
      "----------\n",
      "********************train*************\n",
      "Epoch [9/30] Loss: 0.1018   Acc: 99.1270\n",
      "********************val*************\n",
      "Epoch [9/30] Loss: 0.2178   Acc: 96.9444\n",
      "Epoch 9/29\n",
      "----------\n",
      "********************train*************\n",
      "Epoch [10/30] Loss: 0.1002   Acc: 99.2063\n",
      "********************val*************\n",
      "Epoch [10/30] Loss: 0.2157   Acc: 96.8519\n",
      "Epoch 10/29\n",
      "----------\n",
      "********************train*************\n",
      "Epoch [11/30] Loss: 0.0988   Acc: 99.1270\n",
      "********************val*************\n",
      "Epoch [11/30] Loss: 0.2137   Acc: 96.9444\n",
      "Epoch 11/29\n",
      "----------\n",
      "********************train*************\n",
      "Epoch [12/30] Loss: 0.0973   Acc: 99.1270\n",
      "********************val*************\n",
      "Epoch [12/30] Loss: 0.2119   Acc: 96.8519\n",
      "Epoch 12/29\n",
      "----------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "********************train*************\n",
      "Epoch [13/30] Loss: 0.0958   Acc: 99.1270\n",
      "********************val*************\n",
      "Epoch [13/30] Loss: 0.2102   Acc: 97.0370\n",
      "Epoch 13/29\n",
      "----------\n",
      "********************train*************\n",
      "Epoch [14/30] Loss: 0.0941   Acc: 99.1270\n",
      "********************val*************\n",
      "Epoch [14/30] Loss: 0.2082   Acc: 97.2222\n",
      "Epoch 14/29\n",
      "----------\n",
      "********************train*************\n",
      "Epoch [15/30] Loss: 0.0924   Acc: 99.2460\n",
      "********************val*************\n",
      "Epoch [15/30] Loss: 0.2059   Acc: 97.2222\n",
      "Epoch 15/29\n",
      "----------\n",
      "********************train*************\n",
      "Epoch [16/30] Loss: 0.0906   Acc: 99.2460\n",
      "********************val*************\n",
      "Epoch [16/30] Loss: 0.2032   Acc: 97.2222\n",
      "Epoch 16/29\n",
      "----------\n",
      "********************train*************\n",
      "Epoch [17/30] Loss: 0.0886   Acc: 99.3254\n",
      "********************val*************\n",
      "Epoch [17/30] Loss: 0.2004   Acc: 97.2222\n",
      "Epoch 17/29\n",
      "----------\n",
      "********************train*************\n",
      "Epoch [18/30] Loss: 0.0868   Acc: 99.2857\n",
      "********************val*************\n",
      "Epoch [18/30] Loss: 0.1976   Acc: 97.4074\n",
      "Epoch 18/29\n",
      "----------\n",
      "********************train*************\n",
      "Epoch [19/30] Loss: 0.0851   Acc: 99.3651\n",
      "********************val*************\n",
      "Epoch [19/30] Loss: 0.1949   Acc: 97.5000\n",
      "Epoch 19/29\n",
      "----------\n",
      "********************train*************\n",
      "Epoch [20/30] Loss: 0.0834   Acc: 99.4048\n",
      "********************val*************\n",
      "Epoch [20/30] Loss: 0.1923   Acc: 97.6852\n",
      "Epoch 20/29\n",
      "----------\n",
      "********************train*************\n",
      "Epoch [21/30] Loss: 0.0818   Acc: 99.4841\n",
      "********************val*************\n",
      "Epoch [21/30] Loss: 0.1900   Acc: 97.9630\n",
      "Epoch 21/29\n",
      "----------\n",
      "********************train*************\n",
      "Epoch [22/30] Loss: 0.0803   Acc: 99.4841\n",
      "********************val*************\n",
      "Epoch [22/30] Loss: 0.1878   Acc: 98.1481\n",
      "Epoch 22/29\n",
      "----------\n",
      "********************train*************\n",
      "Epoch [23/30] Loss: 0.0788   Acc: 99.4841\n",
      "********************val*************\n",
      "Epoch [23/30] Loss: 0.1857   Acc: 98.1481\n",
      "Epoch 23/29\n",
      "----------\n",
      "********************train*************\n",
      "Epoch [24/30] Loss: 0.0775   Acc: 99.4841\n",
      "********************val*************\n",
      "Epoch [24/30] Loss: 0.1837   Acc: 98.0556\n",
      "Epoch 24/29\n",
      "----------\n",
      "********************train*************\n",
      "Epoch [25/30] Loss: 0.0762   Acc: 99.5238\n",
      "********************val*************\n",
      "Epoch [25/30] Loss: 0.1817   Acc: 98.0556\n",
      "Epoch 25/29\n",
      "----------\n",
      "********************train*************\n",
      "Epoch [26/30] Loss: 0.0749   Acc: 99.5238\n",
      "********************val*************\n",
      "Epoch [26/30] Loss: 0.1797   Acc: 98.1481\n",
      "Epoch 26/29\n",
      "----------\n",
      "********************train*************\n",
      "Epoch [27/30] Loss: 0.0737   Acc: 99.5238\n",
      "********************val*************\n",
      "Epoch [27/30] Loss: 0.1776   Acc: 98.1481\n",
      "Epoch 27/29\n",
      "----------\n",
      "********************train*************\n",
      "Epoch [28/30] Loss: 0.0725   Acc: 99.5238\n",
      "********************val*************\n",
      "Epoch [28/30] Loss: 0.1756   Acc: 98.1481\n",
      "Epoch 28/29\n",
      "----------\n",
      "********************train*************\n",
      "Epoch [29/30] Loss: 0.0713   Acc: 99.5238\n",
      "********************val*************\n",
      "Epoch [29/30] Loss: 0.1736   Acc: 98.4259\n",
      "Epoch 29/29\n",
      "----------\n",
      "********************train*************\n",
      "Epoch [30/30] Loss: 0.0701   Acc: 99.6032\n",
      "********************val*************\n",
      "Epoch [30/30] Loss: 0.1718   Acc: 98.4259\n",
      "Training complete in 0m 1s\n",
      "Best val Acc: 98.425926\n",
      "Epoch generations ( 61 /100) :accuracy(fittest mask) tensor(99.6032, dtype=torch.float64) keep_prob 0.7430555555555556\n",
      "Epoch generations ( 62 /100) :accuracy(fittest mask) tensor(99.6429, dtype=torch.float64) keep_prob 0.75\n",
      "Epoch generations ( 63 /100) :accuracy(fittest mask) tensor(99.6429, dtype=torch.float64) keep_prob 0.7569444444444444\n",
      "Epoch generations ( 64 /100) :accuracy(fittest mask) tensor(99.7222, dtype=torch.float64) keep_prob 0.7986111111111112\n",
      "Epoch generations ( 65 /100) :accuracy(fittest mask) tensor(99.7222, dtype=torch.float64) keep_prob 0.8055555555555556\n",
      "Epoch generations ( 66 /100) :Epoch generations ( 67 /100) :Epoch generations ( 68 /100) :Epoch generations ( 69 /100) :Epoch generations ( 70 /100) :Epoch 0/29\n",
      "----------\n",
      "********************train*************\n",
      "Epoch [1/30] Loss: 0.0880   Acc: 99.7222\n",
      "********************val*************\n",
      "Epoch [1/30] Loss: 0.1713   Acc: 98.0556\n",
      "Epoch 1/29\n",
      "----------\n",
      "********************train*************\n",
      "Epoch [2/30] Loss: 0.0846   Acc: 99.7222\n",
      "********************val*************\n",
      "Epoch [2/30] Loss: 0.1697   Acc: 98.1481\n",
      "Epoch 2/29\n",
      "----------\n",
      "********************train*************\n",
      "Epoch [3/30] Loss: 0.0807   Acc: 99.7619\n",
      "********************val*************\n",
      "Epoch [3/30] Loss: 0.1694   Acc: 97.9630\n",
      "Epoch 3/29\n",
      "----------\n",
      "********************train*************\n",
      "Epoch [4/30] Loss: 0.0778   Acc: 99.6825\n",
      "********************val*************\n",
      "Epoch [4/30] Loss: 0.1696   Acc: 97.7778\n",
      "Epoch 4/29\n",
      "----------\n",
      "********************train*************\n",
      "Epoch [5/30] Loss: 0.0757   Acc: 99.5238\n",
      "********************val*************\n",
      "Epoch [5/30] Loss: 0.1694   Acc: 97.7778\n",
      "Epoch 5/29\n",
      "----------\n",
      "********************train*************\n",
      "Epoch [6/30] Loss: 0.0741   Acc: 99.5238\n",
      "********************val*************\n",
      "Epoch [6/30] Loss: 0.1685   Acc: 97.9630\n",
      "Epoch 6/29\n",
      "----------\n",
      "********************train*************\n",
      "Epoch [7/30] Loss: 0.0726   Acc: 99.4841\n",
      "********************val*************\n",
      "Epoch [7/30] Loss: 0.1672   Acc: 97.7778\n",
      "Epoch 7/29\n",
      "----------\n",
      "********************train*************\n",
      "Epoch [8/30] Loss: 0.0711   Acc: 99.5635\n",
      "********************val*************\n",
      "Epoch [8/30] Loss: 0.1660   Acc: 97.7778\n",
      "Epoch 8/29\n",
      "----------\n",
      "********************train*************\n",
      "Epoch [9/30] Loss: 0.0696   Acc: 99.5635\n",
      "********************val*************\n",
      "Epoch [9/30] Loss: 0.1651   Acc: 97.5000\n",
      "Epoch 9/29\n",
      "----------\n",
      "********************train*************\n",
      "Epoch [10/30] Loss: 0.0682   Acc: 99.5635\n",
      "********************val*************\n",
      "Epoch [10/30] Loss: 0.1645   Acc: 97.6852\n",
      "Epoch 10/29\n",
      "----------\n",
      "********************train*************\n",
      "Epoch [11/30] Loss: 0.0670   Acc: 99.6825\n",
      "********************val*************\n",
      "Epoch [11/30] Loss: 0.1640   Acc: 97.6852\n",
      "Epoch 11/29\n",
      "----------\n",
      "********************train*************\n",
      "Epoch [12/30] Loss: 0.0660   Acc: 99.7222\n",
      "********************val*************\n",
      "Epoch [12/30] Loss: 0.1634   Acc: 98.0556\n",
      "Epoch 12/29\n",
      "----------\n",
      "********************train*************\n",
      "Epoch [13/30] Loss: 0.0652   Acc: 99.7619\n",
      "********************val*************\n",
      "Epoch [13/30] Loss: 0.1624   Acc: 97.9630\n",
      "Epoch 13/29\n",
      "----------\n",
      "********************train*************\n",
      "Epoch [14/30] Loss: 0.0643   Acc: 99.7619\n",
      "********************val*************\n",
      "Epoch [14/30] Loss: 0.1609   Acc: 97.9630\n",
      "Epoch 14/29\n",
      "----------\n",
      "********************train*************\n",
      "Epoch [15/30] Loss: 0.0634   Acc: 99.7619\n",
      "********************val*************\n",
      "Epoch [15/30] Loss: 0.1592   Acc: 98.1481\n",
      "Epoch 15/29\n",
      "----------\n",
      "********************train*************\n",
      "Epoch [16/30] Loss: 0.0625   Acc: 99.7619\n",
      "********************val*************\n",
      "Epoch [16/30] Loss: 0.1574   Acc: 98.1481\n",
      "Epoch 16/29\n",
      "----------\n",
      "********************train*************\n",
      "Epoch [17/30] Loss: 0.0615   Acc: 99.6825\n",
      "********************val*************\n",
      "Epoch [17/30] Loss: 0.1556   Acc: 98.1481\n",
      "Epoch 17/29\n",
      "----------\n",
      "********************train*************\n",
      "Epoch [18/30] Loss: 0.0604   Acc: 99.7222\n",
      "********************val*************\n",
      "Epoch [18/30] Loss: 0.1539   Acc: 98.2407\n",
      "Epoch 18/29\n",
      "----------\n",
      "********************train*************\n",
      "Epoch [19/30] Loss: 0.0592   Acc: 99.7619\n",
      "********************val*************\n",
      "Epoch [19/30] Loss: 0.1521   Acc: 98.2407\n",
      "Epoch 19/29\n",
      "----------\n",
      "********************train*************\n",
      "Epoch [20/30] Loss: 0.0581   Acc: 99.8810\n",
      "********************val*************\n",
      "Epoch [20/30] Loss: 0.1505   Acc: 98.2407\n",
      "Epoch 20/29\n",
      "----------\n",
      "********************train*************\n",
      "Epoch [21/30] Loss: 0.0571   Acc: 99.8810\n",
      "********************val*************\n",
      "Epoch [21/30] Loss: 0.1488   Acc: 98.0556\n",
      "Epoch 21/29\n",
      "----------\n",
      "********************train*************\n",
      "Epoch [22/30] Loss: 0.0562   Acc: 99.8810\n",
      "********************val*************\n",
      "Epoch [22/30] Loss: 0.1468   Acc: 98.2407\n",
      "Epoch 22/29\n",
      "----------\n",
      "********************train*************\n",
      "Epoch [23/30] Loss: 0.0553   Acc: 99.8810\n",
      "********************val*************\n",
      "Epoch [23/30] Loss: 0.1447   Acc: 98.4259\n",
      "Epoch 23/29\n",
      "----------\n",
      "********************train*************\n",
      "Epoch [24/30] Loss: 0.0545   Acc: 99.8810\n",
      "********************val*************\n",
      "Epoch [24/30] Loss: 0.1424   Acc: 98.7963\n",
      "Epoch 24/29\n",
      "----------\n",
      "********************train*************\n",
      "Epoch [25/30] Loss: 0.0536   Acc: 99.8810\n",
      "********************val*************\n",
      "Epoch [25/30] Loss: 0.1401   Acc: 98.7963\n",
      "Epoch 25/29\n",
      "----------\n",
      "********************train*************\n",
      "Epoch [26/30] Loss: 0.0527   Acc: 99.8810\n",
      "********************val*************\n",
      "Epoch [26/30] Loss: 0.1381   Acc: 98.7963\n",
      "Epoch 26/29\n",
      "----------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "********************train*************\n",
      "Epoch [27/30] Loss: 0.0519   Acc: 99.8810\n",
      "********************val*************\n",
      "Epoch [27/30] Loss: 0.1364   Acc: 98.8889\n",
      "Epoch 27/29\n",
      "----------\n",
      "********************train*************\n",
      "Epoch [28/30] Loss: 0.0512   Acc: 99.9206\n",
      "********************val*************\n",
      "Epoch [28/30] Loss: 0.1349   Acc: 98.8889\n",
      "Epoch 28/29\n",
      "----------\n",
      "********************train*************\n",
      "Epoch [29/30] Loss: 0.0505   Acc: 99.9206\n",
      "********************val*************\n",
      "Epoch [29/30] Loss: 0.1336   Acc: 98.8889\n",
      "Epoch 29/29\n",
      "----------\n",
      "********************train*************\n",
      "Epoch [30/30] Loss: 0.0498   Acc: 99.9603\n",
      "********************val*************\n",
      "Epoch [30/30] Loss: 0.1323   Acc: 99.0741\n",
      "Training complete in 0m 1s\n",
      "Best val Acc: 99.074074\n",
      "Epoch generations ( 71 /100) :Epoch generations ( 72 /100) :Epoch generations ( 73 /100) :Epoch generations ( 74 /100) :Epoch generations ( 75 /100) :Epoch generations ( 76 /100) :Epoch generations ( 77 /100) :Epoch generations ( 78 /100) :Epoch generations ( 79 /100) :Epoch generations ( 80 /100) :Epoch 0/29\n",
      "----------\n",
      "********************train*************\n",
      "Epoch [1/30] Loss: 0.0490   Acc: 99.9603\n",
      "********************val*************\n",
      "Epoch [1/30] Loss: 0.1310   Acc: 99.0741\n",
      "Epoch 1/29\n",
      "----------\n",
      "********************train*************\n",
      "Epoch [2/30] Loss: 0.0483   Acc: 99.9603\n",
      "********************val*************\n",
      "Epoch [2/30] Loss: 0.1298   Acc: 99.0741\n",
      "Epoch 2/29\n",
      "----------\n",
      "********************train*************\n",
      "Epoch [3/30] Loss: 0.0476   Acc: 99.9603\n",
      "********************val*************\n",
      "Epoch [3/30] Loss: 0.1287   Acc: 99.0741\n",
      "Epoch 3/29\n",
      "----------\n",
      "********************train*************\n",
      "Epoch [4/30] Loss: 0.0469   Acc: 99.9603\n",
      "********************val*************\n",
      "Epoch [4/30] Loss: 0.1276   Acc: 99.0741\n",
      "Epoch 4/29\n",
      "----------\n",
      "********************train*************\n",
      "Epoch [5/30] Loss: 0.0462   Acc: 99.9603\n",
      "********************val*************\n",
      "Epoch [5/30] Loss: 0.1266   Acc: 99.0741\n",
      "Epoch 5/29\n",
      "----------\n",
      "********************train*************\n",
      "Epoch [6/30] Loss: 0.0456   Acc: 99.9603\n",
      "********************val*************\n",
      "Epoch [6/30] Loss: 0.1257   Acc: 99.0741\n",
      "Epoch 6/29\n",
      "----------\n",
      "********************train*************\n",
      "Epoch [7/30] Loss: 0.0449   Acc: 99.9603\n",
      "********************val*************\n",
      "Epoch [7/30] Loss: 0.1248   Acc: 99.0741\n",
      "Epoch 7/29\n",
      "----------\n",
      "********************train*************\n",
      "Epoch [8/30] Loss: 0.0443   Acc: 99.9603\n",
      "********************val*************\n",
      "Epoch [8/30] Loss: 0.1239   Acc: 99.2593\n",
      "Epoch 8/29\n",
      "----------\n",
      "********************train*************\n",
      "Epoch [9/30] Loss: 0.0437   Acc: 99.9603\n",
      "********************val*************\n",
      "Epoch [9/30] Loss: 0.1230   Acc: 99.2593\n",
      "Epoch 9/29\n",
      "----------\n",
      "********************train*************\n",
      "Epoch [10/30] Loss: 0.0431   Acc: 99.9603\n",
      "********************val*************\n",
      "Epoch [10/30] Loss: 0.1221   Acc: 99.2593\n",
      "Epoch 10/29\n",
      "----------\n",
      "********************train*************\n",
      "Epoch [11/30] Loss: 0.0425   Acc: 99.9603\n",
      "********************val*************\n",
      "Epoch [11/30] Loss: 0.1211   Acc: 99.2593\n",
      "Epoch 11/29\n",
      "----------\n",
      "********************train*************\n",
      "Epoch [12/30] Loss: 0.0419   Acc: 99.9603\n",
      "********************val*************\n",
      "Epoch [12/30] Loss: 0.1201   Acc: 99.3519\n",
      "Epoch 12/29\n",
      "----------\n",
      "********************train*************\n",
      "Epoch [13/30] Loss: 0.0413   Acc: 99.9603\n",
      "********************val*************\n",
      "Epoch [13/30] Loss: 0.1191   Acc: 99.3519\n",
      "Epoch 13/29\n",
      "----------\n",
      "********************train*************\n",
      "Epoch [14/30] Loss: 0.0407   Acc: 99.9603\n",
      "********************val*************\n",
      "Epoch [14/30] Loss: 0.1181   Acc: 99.3519\n",
      "Epoch 14/29\n",
      "----------\n",
      "********************train*************\n",
      "Epoch [15/30] Loss: 0.0402   Acc: 99.9603\n",
      "********************val*************\n",
      "Epoch [15/30] Loss: 0.1171   Acc: 99.3519\n",
      "Epoch 15/29\n",
      "----------\n",
      "********************train*************\n",
      "Epoch [16/30] Loss: 0.0397   Acc: 99.9603\n",
      "********************val*************\n",
      "Epoch [16/30] Loss: 0.1162   Acc: 99.3519\n",
      "Epoch 16/29\n",
      "----------\n",
      "********************train*************\n",
      "Epoch [17/30] Loss: 0.0391   Acc: 99.9603\n",
      "********************val*************\n",
      "Epoch [17/30] Loss: 0.1152   Acc: 99.3519\n",
      "Epoch 17/29\n",
      "----------\n",
      "********************train*************\n",
      "Epoch [18/30] Loss: 0.0386   Acc: 99.9603\n",
      "********************val*************\n",
      "Epoch [18/30] Loss: 0.1143   Acc: 99.3519\n",
      "Epoch 18/29\n",
      "----------\n",
      "********************train*************\n",
      "Epoch [19/30] Loss: 0.0381   Acc: 99.9603\n",
      "********************val*************\n",
      "Epoch [19/30] Loss: 0.1134   Acc: 99.3519\n",
      "Epoch 19/29\n",
      "----------\n",
      "********************train*************\n",
      "Epoch [20/30] Loss: 0.0376   Acc: 99.9603\n",
      "********************val*************\n",
      "Epoch [20/30] Loss: 0.1125   Acc: 99.3519\n",
      "Epoch 20/29\n",
      "----------\n",
      "********************train*************\n",
      "Epoch [21/30] Loss: 0.0371   Acc: 99.9603\n",
      "********************val*************\n",
      "Epoch [21/30] Loss: 0.1115   Acc: 99.3519\n",
      "Epoch 21/29\n",
      "----------\n",
      "********************train*************\n",
      "Epoch [22/30] Loss: 0.0366   Acc: 99.9603\n",
      "********************val*************\n",
      "Epoch [22/30] Loss: 0.1106   Acc: 99.3519\n",
      "Epoch 22/29\n",
      "----------\n",
      "********************train*************\n",
      "Epoch [23/30] Loss: 0.0362   Acc: 99.9603\n",
      "********************val*************\n",
      "Epoch [23/30] Loss: 0.1096   Acc: 99.5370\n",
      "Epoch 23/29\n",
      "----------\n",
      "********************train*************\n",
      "Epoch [24/30] Loss: 0.0357   Acc: 99.9603\n",
      "********************val*************\n",
      "Epoch [24/30] Loss: 0.1086   Acc: 99.5370\n",
      "Epoch 24/29\n",
      "----------\n",
      "********************train*************\n",
      "Epoch [25/30] Loss: 0.0352   Acc: 99.9603\n",
      "********************val*************\n",
      "Epoch [25/30] Loss: 0.1077   Acc: 99.5370\n",
      "Epoch 25/29\n",
      "----------\n",
      "********************train*************\n",
      "Epoch [26/30] Loss: 0.0348   Acc: 99.9603\n",
      "********************val*************\n",
      "Epoch [26/30] Loss: 0.1067   Acc: 99.5370\n",
      "Epoch 26/29\n",
      "----------\n",
      "********************train*************\n",
      "Epoch [27/30] Loss: 0.0343   Acc: 99.9603\n",
      "********************val*************\n",
      "Epoch [27/30] Loss: 0.1057   Acc: 99.5370\n",
      "Epoch 27/29\n",
      "----------\n",
      "********************train*************\n",
      "Epoch [28/30] Loss: 0.0339   Acc: 99.9603\n",
      "********************val*************\n",
      "Epoch [28/30] Loss: 0.1048   Acc: 99.5370\n",
      "Epoch 28/29\n",
      "----------\n",
      "********************train*************\n",
      "Epoch [29/30] Loss: 0.0335   Acc: 99.9603\n",
      "********************val*************\n",
      "Epoch [29/30] Loss: 0.1039   Acc: 99.5370\n",
      "Epoch 29/29\n",
      "----------\n",
      "********************train*************\n",
      "Epoch [30/30] Loss: 0.0330   Acc: 99.9603\n",
      "********************val*************\n",
      "Epoch [30/30] Loss: 0.1030   Acc: 99.5370\n",
      "Training complete in 0m 1s\n",
      "Best val Acc: 99.537037\n",
      "Epoch generations ( 81 /100) :Epoch generations ( 82 /100) :Epoch generations ( 83 /100) :Epoch generations ( 84 /100) :Epoch generations ( 85 /100) :Epoch generations ( 86 /100) :Epoch generations ( 87 /100) :Epoch generations ( 88 /100) :Epoch generations ( 89 /100) :Epoch generations ( 90 /100) :Epoch 0/29\n",
      "----------\n",
      "********************train*************\n",
      "Epoch [1/30] Loss: 0.0357   Acc: 99.9603\n",
      "********************val*************\n",
      "Epoch [1/30] Loss: 0.1086   Acc: 99.5370\n",
      "Epoch 1/29\n",
      "----------\n",
      "********************train*************\n",
      "Epoch [2/30] Loss: 0.0352   Acc: 99.9603\n",
      "********************val*************\n",
      "Epoch [2/30] Loss: 0.1077   Acc: 99.5370\n",
      "Epoch 2/29\n",
      "----------\n",
      "********************train*************\n",
      "Epoch [3/30] Loss: 0.0348   Acc: 99.9603\n",
      "********************val*************\n",
      "Epoch [3/30] Loss: 0.1067   Acc: 99.5370\n",
      "Epoch 3/29\n",
      "----------\n",
      "********************train*************\n",
      "Epoch [4/30] Loss: 0.0344   Acc: 99.9603\n",
      "********************val*************\n",
      "Epoch [4/30] Loss: 0.1058   Acc: 99.5370\n",
      "Epoch 4/29\n",
      "----------\n",
      "********************train*************\n",
      "Epoch [5/30] Loss: 0.0339   Acc: 99.9603\n",
      "********************val*************\n",
      "Epoch [5/30] Loss: 0.1049   Acc: 99.5370\n",
      "Epoch 5/29\n",
      "----------\n",
      "********************train*************\n",
      "Epoch [6/30] Loss: 0.0335   Acc: 99.9603\n",
      "********************val*************\n",
      "Epoch [6/30] Loss: 0.1040   Acc: 99.5370\n",
      "Epoch 6/29\n",
      "----------\n",
      "********************train*************\n",
      "Epoch [7/30] Loss: 0.0331   Acc: 99.9603\n",
      "********************val*************\n",
      "Epoch [7/30] Loss: 0.1031   Acc: 99.5370\n",
      "Epoch 7/29\n",
      "----------\n",
      "********************train*************\n",
      "Epoch [8/30] Loss: 0.0327   Acc: 99.9603\n",
      "********************val*************\n",
      "Epoch [8/30] Loss: 0.1022   Acc: 99.5370\n",
      "Epoch 8/29\n",
      "----------\n",
      "********************train*************\n",
      "Epoch [9/30] Loss: 0.0323   Acc: 99.9603\n",
      "********************val*************\n",
      "Epoch [9/30] Loss: 0.1014   Acc: 99.5370\n",
      "Epoch 9/29\n",
      "----------\n",
      "********************train*************\n",
      "Epoch [10/30] Loss: 0.0319   Acc: 99.9603\n",
      "********************val*************\n",
      "Epoch [10/30] Loss: 0.1006   Acc: 99.5370\n",
      "Epoch 10/29\n",
      "----------\n",
      "********************train*************\n",
      "Epoch [11/30] Loss: 0.0315   Acc: 99.9603\n",
      "********************val*************\n",
      "Epoch [11/30] Loss: 0.0998   Acc: 99.5370\n",
      "Epoch 11/29\n",
      "----------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "********************train*************\n",
      "Epoch [12/30] Loss: 0.0312   Acc: 99.9603\n",
      "********************val*************\n",
      "Epoch [12/30] Loss: 0.0990   Acc: 99.5370\n",
      "Epoch 12/29\n",
      "----------\n",
      "********************train*************\n",
      "Epoch [13/30] Loss: 0.0308   Acc: 99.9603\n",
      "********************val*************\n",
      "Epoch [13/30] Loss: 0.0983   Acc: 99.5370\n",
      "Epoch 13/29\n",
      "----------\n",
      "********************train*************\n",
      "Epoch [14/30] Loss: 0.0304   Acc: 99.9603\n",
      "********************val*************\n",
      "Epoch [14/30] Loss: 0.0976   Acc: 99.5370\n",
      "Epoch 14/29\n",
      "----------\n",
      "********************train*************\n",
      "Epoch [15/30] Loss: 0.0301   Acc: 99.9603\n",
      "********************val*************\n",
      "Epoch [15/30] Loss: 0.0969   Acc: 99.5370\n",
      "Epoch 15/29\n",
      "----------\n",
      "********************train*************\n",
      "Epoch [16/30] Loss: 0.0297   Acc: 99.9603\n",
      "********************val*************\n",
      "Epoch [16/30] Loss: 0.0961   Acc: 99.5370\n",
      "Epoch 16/29\n",
      "----------\n",
      "********************train*************\n",
      "Epoch [17/30] Loss: 0.0293   Acc: 99.9603\n",
      "********************val*************\n",
      "Epoch [17/30] Loss: 0.0954   Acc: 99.5370\n",
      "Epoch 17/29\n",
      "----------\n",
      "********************train*************\n",
      "Epoch [18/30] Loss: 0.0290   Acc: 99.9603\n",
      "********************val*************\n",
      "Epoch [18/30] Loss: 0.0946   Acc: 99.5370\n",
      "Epoch 18/29\n",
      "----------\n",
      "********************train*************\n",
      "Epoch [19/30] Loss: 0.0287   Acc: 99.9603\n",
      "********************val*************\n",
      "Epoch [19/30] Loss: 0.0939   Acc: 99.5370\n",
      "Epoch 19/29\n",
      "----------\n",
      "********************train*************\n",
      "Epoch [20/30] Loss: 0.0283   Acc: 99.9603\n",
      "********************val*************\n",
      "Epoch [20/30] Loss: 0.0931   Acc: 99.5370\n",
      "Epoch 20/29\n",
      "----------\n",
      "********************train*************\n",
      "Epoch [21/30] Loss: 0.0280   Acc: 99.9603\n",
      "********************val*************\n",
      "Epoch [21/30] Loss: 0.0925   Acc: 99.5370\n",
      "Epoch 21/29\n",
      "----------\n",
      "********************train*************\n",
      "Epoch [22/30] Loss: 0.0277   Acc: 99.9603\n",
      "********************val*************\n",
      "Epoch [22/30] Loss: 0.0918   Acc: 99.5370\n",
      "Epoch 22/29\n",
      "----------\n",
      "********************train*************\n",
      "Epoch [23/30] Loss: 0.0273   Acc: 99.9603\n",
      "********************val*************\n",
      "Epoch [23/30] Loss: 0.0913   Acc: 99.5370\n",
      "Epoch 23/29\n",
      "----------\n",
      "********************train*************\n",
      "Epoch [24/30] Loss: 0.0270   Acc: 99.9603\n",
      "********************val*************\n",
      "Epoch [24/30] Loss: 0.0906   Acc: 99.5370\n",
      "Epoch 24/29\n",
      "----------\n",
      "********************train*************\n",
      "Epoch [25/30] Loss: 0.0267   Acc: 99.9603\n",
      "********************val*************\n",
      "Epoch [25/30] Loss: 0.0899   Acc: 99.5370\n",
      "Epoch 25/29\n",
      "----------\n",
      "********************train*************\n",
      "Epoch [26/30] Loss: 0.0264   Acc: 99.9603\n",
      "********************val*************\n",
      "Epoch [26/30] Loss: 0.0893   Acc: 99.5370\n",
      "Epoch 26/29\n",
      "----------\n",
      "********************train*************\n",
      "Epoch [27/30] Loss: 0.0261   Acc: 99.9603\n",
      "********************val*************\n",
      "Epoch [27/30] Loss: 0.0887   Acc: 99.5370\n",
      "Epoch 27/29\n",
      "----------\n",
      "********************train*************\n",
      "Epoch [28/30] Loss: 0.0258   Acc: 99.9603\n",
      "********************val*************\n",
      "Epoch [28/30] Loss: 0.0881   Acc: 99.5370\n",
      "Epoch 28/29\n",
      "----------\n",
      "********************train*************\n",
      "Epoch [29/30] Loss: 0.0255   Acc: 99.9603\n",
      "********************val*************\n",
      "Epoch [29/30] Loss: 0.0875   Acc: 99.7222\n",
      "Epoch 29/29\n",
      "----------\n",
      "********************train*************\n",
      "Epoch [30/30] Loss: 0.0252   Acc: 99.9603\n",
      "********************val*************\n",
      "Epoch [30/30] Loss: 0.0869   Acc: 99.7222\n",
      "Training complete in 0m 1s\n",
      "Best val Acc: 99.722222\n",
      "Epoch generations ( 91 /100) :Epoch generations ( 92 /100) :Epoch generations ( 93 /100) :Epoch generations ( 94 /100) :Epoch generations ( 95 /100) :Epoch generations ( 96 /100) :Epoch generations ( 97 /100) :Epoch generations ( 98 /100) :Epoch generations ( 99 /100) :Epoch generations ( 100 /100) :Epoch 0/29\n",
      "----------\n",
      "********************train*************\n",
      "Epoch [1/30] Loss: 0.0252   Acc: 99.9603\n",
      "********************val*************\n",
      "Epoch [1/30] Loss: 0.0869   Acc: 99.7222\n",
      "Epoch 1/29\n",
      "----------\n",
      "********************train*************\n",
      "Epoch [2/30] Loss: 0.0250   Acc: 99.9603\n",
      "********************val*************\n",
      "Epoch [2/30] Loss: 0.0862   Acc: 99.7222\n",
      "Epoch 2/29\n",
      "----------\n",
      "********************train*************\n",
      "Epoch [3/30] Loss: 0.0247   Acc: 99.9603\n",
      "********************val*************\n",
      "Epoch [3/30] Loss: 0.0856   Acc: 99.7222\n",
      "Epoch 3/29\n",
      "----------\n",
      "********************train*************\n",
      "Epoch [4/30] Loss: 0.0244   Acc: 99.9603\n",
      "********************val*************\n",
      "Epoch [4/30] Loss: 0.0851   Acc: 99.7222\n",
      "Epoch 4/29\n",
      "----------\n",
      "********************train*************\n",
      "Epoch [5/30] Loss: 0.0241   Acc: 99.9603\n",
      "********************val*************\n",
      "Epoch [5/30] Loss: 0.0845   Acc: 99.7222\n",
      "Epoch 5/29\n",
      "----------\n",
      "********************train*************\n",
      "Epoch [6/30] Loss: 0.0239   Acc: 99.9603\n",
      "********************val*************\n",
      "Epoch [6/30] Loss: 0.0839   Acc: 99.7222\n",
      "Epoch 6/29\n",
      "----------\n",
      "********************train*************\n",
      "Epoch [7/30] Loss: 0.0236   Acc: 99.9603\n",
      "********************val*************\n",
      "Epoch [7/30] Loss: 0.0833   Acc: 99.7222\n",
      "Epoch 7/29\n",
      "----------\n",
      "********************train*************\n",
      "Epoch [8/30] Loss: 0.0234   Acc: 99.9603\n",
      "********************val*************\n",
      "Epoch [8/30] Loss: 0.0828   Acc: 99.7222\n",
      "Epoch 8/29\n",
      "----------\n",
      "********************train*************\n",
      "Epoch [9/30] Loss: 0.0231   Acc: 99.9603\n",
      "********************val*************\n",
      "Epoch [9/30] Loss: 0.0822   Acc: 99.7222\n",
      "Epoch 9/29\n",
      "----------\n",
      "********************train*************\n",
      "Epoch [10/30] Loss: 0.0229   Acc: 99.9603\n",
      "********************val*************\n",
      "Epoch [10/30] Loss: 0.0816   Acc: 99.7222\n",
      "Epoch 10/29\n",
      "----------\n",
      "********************train*************\n",
      "Epoch [11/30] Loss: 0.0226   Acc: 99.9603\n",
      "********************val*************\n",
      "Epoch [11/30] Loss: 0.0811   Acc: 99.7222\n",
      "Epoch 11/29\n",
      "----------\n",
      "********************train*************\n",
      "Epoch [12/30] Loss: 0.0224   Acc: 99.9603\n",
      "********************val*************\n",
      "Epoch [12/30] Loss: 0.0805   Acc: 99.7222\n",
      "Epoch 12/29\n",
      "----------\n",
      "********************train*************\n",
      "Epoch [13/30] Loss: 0.0222   Acc: 99.9603\n",
      "********************val*************\n",
      "Epoch [13/30] Loss: 0.0800   Acc: 99.7222\n",
      "Epoch 13/29\n",
      "----------\n",
      "********************train*************\n",
      "Epoch [14/30] Loss: 0.0219   Acc: 100.0000\n",
      "********************val*************\n",
      "Epoch [14/30] Loss: 0.0795   Acc: 99.7222\n",
      "Epoch 14/29\n",
      "----------\n",
      "********************train*************\n",
      "Epoch [15/30] Loss: 0.0217   Acc: 100.0000\n",
      "********************val*************\n",
      "Epoch [15/30] Loss: 0.0790   Acc: 99.7222\n",
      "Epoch 15/29\n",
      "----------\n",
      "********************train*************\n",
      "Epoch [16/30] Loss: 0.0215   Acc: 100.0000\n",
      "********************val*************\n",
      "Epoch [16/30] Loss: 0.0785   Acc: 99.7222\n",
      "Epoch 16/29\n",
      "----------\n",
      "********************train*************\n",
      "Epoch [17/30] Loss: 0.0213   Acc: 100.0000\n",
      "********************val*************\n",
      "Epoch [17/30] Loss: 0.0779   Acc: 99.7222\n",
      "Epoch 17/29\n",
      "----------\n",
      "********************train*************\n",
      "Epoch [18/30] Loss: 0.0210   Acc: 100.0000\n",
      "********************val*************\n",
      "Epoch [18/30] Loss: 0.0774   Acc: 99.7222\n",
      "Epoch 18/29\n",
      "----------\n",
      "********************train*************\n",
      "Epoch [19/30] Loss: 0.0208   Acc: 100.0000\n",
      "********************val*************\n",
      "Epoch [19/30] Loss: 0.0770   Acc: 99.7222\n",
      "Epoch 19/29\n",
      "----------\n",
      "********************train*************\n",
      "Epoch [20/30] Loss: 0.0206   Acc: 100.0000\n",
      "********************val*************\n",
      "Epoch [20/30] Loss: 0.0765   Acc: 99.7222\n",
      "Epoch 20/29\n",
      "----------\n",
      "********************train*************\n",
      "Epoch [21/30] Loss: 0.0204   Acc: 100.0000\n",
      "********************val*************\n",
      "Epoch [21/30] Loss: 0.0761   Acc: 99.7222\n",
      "Epoch 21/29\n",
      "----------\n",
      "********************train*************\n",
      "Epoch [22/30] Loss: 0.0202   Acc: 100.0000\n",
      "********************val*************\n",
      "Epoch [22/30] Loss: 0.0756   Acc: 99.7222\n",
      "Epoch 22/29\n",
      "----------\n",
      "********************train*************\n",
      "Epoch [23/30] Loss: 0.0200   Acc: 100.0000\n",
      "********************val*************\n",
      "Epoch [23/30] Loss: 0.0751   Acc: 99.7222\n",
      "Epoch 23/29\n",
      "----------\n",
      "********************train*************\n",
      "Epoch [24/30] Loss: 0.0198   Acc: 100.0000\n",
      "********************val*************\n",
      "Epoch [24/30] Loss: 0.0746   Acc: 99.7222\n",
      "Epoch 24/29\n",
      "----------\n",
      "********************train*************\n",
      "Epoch [25/30] Loss: 0.0196   Acc: 100.0000\n",
      "********************val*************\n",
      "Epoch [25/30] Loss: 0.0742   Acc: 99.7222\n",
      "Epoch 25/29\n",
      "----------\n",
      "********************train*************\n",
      "Epoch [26/30] Loss: 0.0194   Acc: 100.0000\n",
      "********************val*************\n",
      "Epoch [26/30] Loss: 0.0737   Acc: 99.7222\n",
      "Epoch 26/29\n",
      "----------\n",
      "********************train*************\n",
      "Epoch [27/30] Loss: 0.0192   Acc: 100.0000\n",
      "********************val*************\n",
      "Epoch [27/30] Loss: 0.0733   Acc: 99.7222\n",
      "Epoch 27/29\n",
      "----------\n",
      "********************train*************\n",
      "Epoch [28/30] Loss: 0.0190   Acc: 100.0000\n",
      "********************val*************\n",
      "Epoch [28/30] Loss: 0.0728   Acc: 99.7222\n",
      "Epoch 28/29\n",
      "----------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "********************train*************\n",
      "Epoch [29/30] Loss: 0.0188   Acc: 100.0000\n",
      "********************val*************\n",
      "Epoch [29/30] Loss: 0.0724   Acc: 99.7222\n",
      "Epoch 29/29\n",
      "----------\n",
      "********************train*************\n",
      "Epoch [30/30] Loss: 0.0187   Acc: 100.0000\n",
      "********************val*************\n",
      "Epoch [30/30] Loss: 0.0719   Acc: 99.7222\n",
      "Training complete in 0m 1s\n",
      "Best val Acc: 99.722222\n"
     ]
    }
   ],
   "source": [
    "#**CONTROL BLOCK** controls the epochs and the generations of mask\n",
    "#step1: an object of the population class randomly generating the first population \n",
    "#step2: calculate fitness of each entitiy of the population \n",
    "#step3: creates a mating pool of the population based on the worst two performing parent \n",
    "#step4: fittest mask of the generating along with keep_prob found \n",
    "#step5: if 0th ,10th ,20th, the epochs starts training on the worst performing mask /other wise new generation is created \n",
    "\n",
    "epochgens=0\n",
    "population =Population(mutation_rate,max_population,maskLength)\n",
    "total_acc=[]\n",
    "p=0\n",
    "while (epochgens<=num_epochs):\n",
    "    print ('Epoch generations (',epochgens,'/{})'.format(num_epochs),end=' :')\n",
    "    if p < 0.8:\n",
    "        population .calcFitness(model)\n",
    "        population.naturalSelection()\n",
    "        fittestmask,p = population .fittest()\n",
    "        accuracy=fittestmask.fitness(model)\n",
    "        print (\"accuracy(fittest mask)\",accuracy,\"keep_prob\",p,end='\\n')\n",
    "    if (epochgens%10==0):\n",
    "        model,losses,accuracies=train_model(model,criterion,optimizer,fittestmask.gene,p,30)\n",
    "        total_acc=total_acc+accuracies\n",
    "    population.generate()\n",
    "    epochgens+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "name": "SimpleANN_ga.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
