{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import random\n",
    "import torch\n",
    "import torchvision\n",
    "import time \n",
    "from torchvision import datasets ,transforms\n",
    "from torchvision.datasets import ImageFolder\n",
    "from torch.utils.data import DataLoader,Dataset\n",
    "from torch import nn \n",
    "from torch.nn.functional import relu ,softmax \n",
    "import copy \n",
    "from torch.utils.data import Subset\n",
    "import pandas as pd \n",
    "import numpy as np\n",
    "from PIL import Image\n",
    " \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_transform=transforms.Compose ([transforms.ToTensor()])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5000\n",
      "3000\n",
      "2000\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "class light_source_dataset(Dataset):\n",
    "    def __init__(self,path,transform):\n",
    "        self.df_data=pd.read_csv(path)\n",
    "        self.df_data['Classifier'] = pd.Categorical(pd.factorize(self.df_data['Classifier'])[0])\n",
    "        self.labels=np.asarray(self.df_data.iloc[:,self.df_data.shape[1]-1])\n",
    "        self.image_as_np=np.asarray(self.df_data.iloc[:,0:self.df_data.shape[1]-1]).astype('uint8')\n",
    "        self.trans=transform\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.df_data.index)\n",
    "    def __getitem__(self,index):\n",
    "        image_np=self.image_as_np[index,:,None]\n",
    "       \n",
    "        pillow_image=Image.fromarray(image_np.astype('uint8'))\n",
    "        \n",
    "        single_label=self.labels[index]\n",
    "        if (self.trans is not None):\n",
    "            img_as_tensor=self.trans(pillow_image)\n",
    "        \n",
    "        \n",
    "            \n",
    "        return (img_as_tensor,single_label)\n",
    "        \n",
    "dataset=light_source_dataset('WaveformEW\\WaveformEW.csv',data_transform)\n",
    "print(len(dataset))\n",
    "train_size=int (len(dataset)*0.6)\n",
    "test_size=len(dataset)-train_size\n",
    "trainloader=DataLoader(dataset,batch_size=1)\n",
    "torch.manual_seed(1)\n",
    "train_data,test_data=torch.utils.data.random_split(dataset,[int (train_size), int (test_size)])\n",
    "dataloader={'train':DataLoader(train_data,shuffle=False ,batch_size=16),\n",
    "            'val':DataLoader(test_data,shuffle=False,batch_size=16\n",
    "                            )}\n",
    "\n",
    "dataset_sizes={'train':len(train_data),\n",
    "               'val':len(test_data)}\n",
    "\n",
    "device=torch.device(\"cuda:0\"if torch.cuda.is_available () else \"cpu\")\n",
    "print(dataset_sizes[\"train\"])\n",
    "print (dataset_sizes[\"val\"])\n",
    "#for images,labels in dataloader['train']:\n",
    " #   print(labels)\n",
    "\n",
    " \n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class Model(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super (Model,self ).__init__()\n",
    "        self .linear1=nn.Linear (40,20)\n",
    "        self.linear2=nn.Linear (20,3)\n",
    "        \n",
    "    def Forward (self,x,mask,p):\n",
    "        #feed forward function \n",
    "        x=x.view(-1,40)\n",
    "        act1=relu(self.linear1(x))\n",
    "        act1_masked =self.masking (act1,mask,p)\n",
    "        act2=softmax(self.linear2(act1_masked ))\n",
    "        return act2\n",
    "    \n",
    "    \n",
    "    \n",
    "    def masking (self,act1,mask,p):\n",
    "        if (self.training ==True ):\n",
    "            return ((act1*mask)/p)\n",
    "        else :\n",
    "            return (act1)\n",
    "        \n",
    "\n",
    "\n",
    "\n",
    "  \n",
    "  \n",
    "  \n",
    "\n",
    "\n",
    "      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, criterion, optimizer,mask,p, num_epochs=25):\n",
    "  # the mask has to be specified \n",
    "\n",
    "      \n",
    "      since = time.time()\n",
    "\n",
    "      best_model_wts = copy.deepcopy(model.state_dict())\n",
    "      best_acc = 0.0\n",
    "      losses=[]\n",
    "      accuracies=[]\n",
    "\n",
    "      for epoch in range(num_epochs):\n",
    "          print('Epoch {}/{}'.format(epoch, num_epochs - 1))\n",
    "          print('-' * 10)\n",
    "\n",
    "          # Each epoch has a training and validation phase\n",
    "          for phase in ['train', 'val']:\n",
    "              if phase == 'train':\n",
    "                  \n",
    "                  model.train()  # Set model to training mode\n",
    "              else:\n",
    "\n",
    "                  model.eval()   # Set model to evaluate mode\n",
    "\n",
    "              running_loss = 0.0\n",
    "              running_corrects = 0\n",
    "\n",
    "              # Iterate over data.\n",
    "              for inputs, labels in dataloader[phase]:\n",
    "                inputs=inputs.to(device)\n",
    "                labels=labels.to(device)\n",
    "                # zero the parameter gradients\n",
    "                optimizer.zero_grad()\n",
    "\n",
    "                # forward\n",
    "                # track history if only in train\n",
    "                with torch.set_grad_enabled(phase == 'train'):\n",
    "                    outputs= model.Forward(inputs,mask,p)\n",
    "\n",
    "                    _, preds = torch.max(outputs, 1)\n",
    "                    loss = criterion(outputs, labels)# backward + optimize only if in training phase\n",
    "                    if phase == 'train':\n",
    "                        loss.backward()\n",
    "                        optimizer.step()\n",
    "                        #print (model.linear1.weight.grad)\n",
    "                        #print (model.linear2.weight.grad)\n",
    "\n",
    "                  # statistics\n",
    "                running_loss += loss.item() * inputs.size(0)\n",
    "                running_corrects += torch.sum(preds == labels.data)\n",
    "                \n",
    "                  \n",
    "\n",
    "              epoch_loss = running_loss / dataset_sizes[phase]\n",
    "              epoch_acc = running_corrects.double() / dataset_sizes[phase]\n",
    "            \n",
    "\n",
    "              print('{} Loss: {:.4f} Acc: {:.4f} '.format(\n",
    "                  phase, epoch_loss, epoch_acc))\n",
    "\n",
    "              # deep copy the model\n",
    "              if phase == 'val' and epoch_acc > best_acc:\n",
    "                  best_acc = epoch_acc\n",
    "                  best_model_wts = copy.deepcopy(model.state_dict())\n",
    "              if phase=='train':\n",
    "                losses.append(epoch_loss)\n",
    "                accuracies.append(epoch_acc)\n",
    "\n",
    "\n",
    "          \n",
    "\n",
    "      time_elapsed = time.time() - since\n",
    "      print('Training complete in {:.0f}m {:.0f}s'.format(\n",
    "          time_elapsed // 60, time_elapsed % 60))\n",
    "      print('Best val Acc: {:4f}'.format(best_acc))\n",
    "\n",
    "      # load best model weights\n",
    "      model.load_state_dict(best_model_wts)\n",
    "      return model,losses,accuracies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class Population:\n",
    "    \n",
    "    def __init__(self,m,num,maskLength):\n",
    "        # constructor for initialising the population list\n",
    "        #list of DNA objects\n",
    "        self.population=[]\n",
    "        #muation rate for mutation\n",
    "        self.mutation_rate=m\n",
    "        #maximum number of entities in the population\n",
    "        self.popmax=num\n",
    "\n",
    "        self.maskLength=maskLength\n",
    "        for i in range (num):\n",
    "            #creating a dna object\n",
    "            #an initial random population created \n",
    "            dna =DNA(self.maskLength)\n",
    "            self.population.append (dna)\n",
    "      \n",
    "        self.matingPool=[]\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    def calcFitness (self,model):\n",
    "        # going through all the entities of population \n",
    "        #finding fitness of all population entities \n",
    "        for i in range(0,self.popmax):\n",
    "            self.population[i].fitness (model)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    def naturalSelection(self):\n",
    "        self.matingPool=[]\n",
    "        maxFitness=0\n",
    "        for i in range (self.popmax):\n",
    "            # moving throught the entire population \n",
    "            if (self.population[i].fit>maxFitness):\n",
    "                maxFitness=self.population[i].fit\n",
    "       \n",
    "        # max Fitness has the maximum loss score of the entire population  \n",
    "        for i in range (self.popmax ):\n",
    "        # iterating through the all inviduals of the population\n",
    "            n=self.Mymap(self.population[i].fit,0,maxFitness,0,1)\n",
    "        \n",
    "            n=math.floor(n*100)\n",
    "            \n",
    "            for j in range (n):\n",
    "                #creating mating pool\n",
    "                self.matingPool.append (self.population[i])\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    def Mymap(self,num,prevlow,prevhigh,nextlow,nexthigh):\n",
    "        \n",
    "        prevrange =float((num-prevlow)/(prevhigh-prevlow))\n",
    "        return nextlow+(nexthigh-nextlow)*prevrange\n",
    "\n",
    "\n",
    "\n",
    "    def   generate (self):\n",
    "        for i in range (self.popmax ):\n",
    "            index_1=math.floor(random.randint  (0,len(self.matingPool)-1))\n",
    "            index_2=math.floor (random.randint (0,len(self.matingPool)-1))\n",
    "            parent1=self.matingPool[index_1]\n",
    "            parent2=self.matingPool[index_2]\n",
    "            child=parent1.crossover(parent2)\n",
    "            child.mutate(self.mutation_rate)\n",
    "            self.population[i]=child \n",
    "\n",
    "\n",
    "    def fittest(self):\n",
    "        #returns the fiitest individual mask of the population \n",
    "        #also returns the keeping probability of the fittest mask \n",
    "        fittest=self.population[0]\n",
    "        for i  in range (self.popmax):\n",
    "            if (fittest.fit<self.population[i].fit):\n",
    "                fittest=self.population[i]\n",
    "        return fittest,fittest.keep_prob()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "class DNA:\n",
    "    \n",
    "    \n",
    "    def __init__(self,maskLength):\n",
    "        #constructor for the creation of the mask as a gene object \n",
    "        self.maskLength=maskLength\n",
    "        #creation of mask \n",
    "        self.gene=torch.bernoulli(torch.empty(1,maskLength).uniform_(0,1))\n",
    "        self.fit=0\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    def keep_prob (self):\n",
    "\n",
    "        num_one =0\n",
    "        for i in range (self.maskLength):\n",
    "            if (self.gene[0,i]==1):\n",
    "                num_one=num_one+1\n",
    "        return float(num_one/self.maskLength)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    def fitness(self,model):\n",
    "        # finding the fitness of a particular mask\n",
    "        #accuracy of all training set is the fitness in one epoch\n",
    "        #putting model in train mode \n",
    "        running_loss=0\n",
    "        running_corrects=0\n",
    "        model.train()\n",
    "        \n",
    "        for inputs,labels in dataloader ['train']:\n",
    "            inputs=inputs.to(device)\n",
    "            labels=labels.to(device )\n",
    "            outputs=model.Forward(inputs,self.gene,self.keep_prob())\n",
    "            _,preds=torch.max(outputs,1)\n",
    "            loss=criterion (outputs,labels)\n",
    "            running_loss += loss.item() * inputs.size(0)\n",
    "            running_corrects += torch.sum(preds == labels.data)\n",
    "        epoch_loss = running_loss / dataset_sizes['train']\n",
    "        epoch_acc = running_corrects.double() / dataset_sizes['train']\n",
    "        \n",
    "        self.fit=epoch_acc\n",
    "        return epoch_acc\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    def crossover (self,parent2):\n",
    "        #one parent is the passed in the argument \n",
    "        #another parent is the one from which this function is called \n",
    "        #another parent is self.gene\n",
    "        child =DNA(self.maskLength)\n",
    "        midpoint =random .randint (0,self.maskLength-1)\n",
    "        for i in range (0,self.maskLength):\n",
    "            if (i>midpoint):\n",
    "                child.gene [0,i]=self.gene[0,i]\n",
    "            else :\n",
    "                child.gene [0,i]=parent2.gene[0,i]\n",
    "        \n",
    "        return child \n",
    "\n",
    "    def mutate(self,mutation_rate):\n",
    "        #randomly activate some of the nodes  \n",
    "        #mutate some of the genes \n",
    "        for i in range (self.maskLength):\n",
    "            if (random.randint (0,99)<=mutation_rate*100):\n",
    "                self.gene[0,i]=1\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "mutation_rate =0.20\n",
    "max_population=30\n",
    "maskLength=20\n",
    "#seeded so that each time same initial weights generated \n",
    "torch.manual_seed(6)\n",
    "model=Model()#creating the object of the class\n",
    "model.to(device)\n",
    "criterion=nn.CrossEntropyLoss()\n",
    "optimizer=torch.optim.SGD(model.parameters(),lr=0.001,momentum=0.9)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch generations ( 0 /200) :"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\ana\\envs\\packt_torch\\lib\\site-packages\\ipykernel_launcher.py:12: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  if sys.path[0] == '':\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy(fittest mask) tensor(0.3590, dtype=torch.float64) keep_prob 0.4\n",
      "Epoch 0/29\n",
      "----------\n",
      "train Loss: 1.0983 Acc: 0.3637 \n",
      "val Loss: 1.1001 Acc: 0.3410 \n",
      "Epoch 1/29\n",
      "----------\n",
      "train Loss: 1.0974 Acc: 0.3753 \n",
      "val Loss: 1.0998 Acc: 0.3445 \n",
      "Epoch 2/29\n",
      "----------\n",
      "train Loss: 1.0966 Acc: 0.3853 \n",
      "val Loss: 1.0994 Acc: 0.3475 \n",
      "Epoch 3/29\n",
      "----------\n",
      "train Loss: 1.0957 Acc: 0.3880 \n",
      "val Loss: 1.0990 Acc: 0.3560 \n",
      "Epoch 4/29\n",
      "----------\n",
      "train Loss: 1.0947 Acc: 0.3887 \n",
      "val Loss: 1.0986 Acc: 0.3590 \n",
      "Epoch 5/29\n",
      "----------\n",
      "train Loss: 1.0936 Acc: 0.3953 \n",
      "val Loss: 1.0981 Acc: 0.3695 \n",
      "Epoch 6/29\n",
      "----------\n",
      "train Loss: 1.0924 Acc: 0.4037 \n",
      "val Loss: 1.0976 Acc: 0.3800 \n",
      "Epoch 7/29\n",
      "----------\n",
      "train Loss: 1.0909 Acc: 0.4123 \n",
      "val Loss: 1.0969 Acc: 0.3910 \n",
      "Epoch 8/29\n",
      "----------\n",
      "train Loss: 1.0892 Acc: 0.4127 \n",
      "val Loss: 1.0962 Acc: 0.3940 \n",
      "Epoch 9/29\n",
      "----------\n",
      "train Loss: 1.0871 Acc: 0.4133 \n",
      "val Loss: 1.0952 Acc: 0.4090 \n",
      "Epoch 10/29\n",
      "----------\n",
      "train Loss: 1.0846 Acc: 0.4220 \n",
      "val Loss: 1.0941 Acc: 0.4175 \n",
      "Epoch 11/29\n",
      "----------\n",
      "train Loss: 1.0816 Acc: 0.4273 \n",
      "val Loss: 1.0928 Acc: 0.4340 \n",
      "Epoch 12/29\n",
      "----------\n",
      "train Loss: 1.0779 Acc: 0.4300 \n",
      "val Loss: 1.0912 Acc: 0.4470 \n",
      "Epoch 13/29\n",
      "----------\n",
      "train Loss: 1.0736 Acc: 0.4380 \n",
      "val Loss: 1.0893 Acc: 0.4640 \n",
      "Epoch 14/29\n",
      "----------\n",
      "train Loss: 1.0686 Acc: 0.4507 \n",
      "val Loss: 1.0871 Acc: 0.4795 \n",
      "Epoch 15/29\n",
      "----------\n",
      "train Loss: 1.0629 Acc: 0.4590 \n",
      "val Loss: 1.0846 Acc: 0.4900 \n",
      "Epoch 16/29\n",
      "----------\n",
      "train Loss: 1.0567 Acc: 0.4633 \n",
      "val Loss: 1.0818 Acc: 0.4990 \n",
      "Epoch 17/29\n",
      "----------\n",
      "train Loss: 1.0501 Acc: 0.4710 \n",
      "val Loss: 1.0787 Acc: 0.5135 \n",
      "Epoch 18/29\n",
      "----------\n",
      "train Loss: 1.0434 Acc: 0.4817 \n",
      "val Loss: 1.0755 Acc: 0.5230 \n",
      "Epoch 19/29\n",
      "----------\n",
      "train Loss: 1.0367 Acc: 0.4877 \n",
      "val Loss: 1.0722 Acc: 0.5280 \n",
      "Epoch 20/29\n",
      "----------\n",
      "train Loss: 1.0303 Acc: 0.4907 \n",
      "val Loss: 1.0689 Acc: 0.5365 \n",
      "Epoch 21/29\n",
      "----------\n",
      "train Loss: 1.0243 Acc: 0.4970 \n",
      "val Loss: 1.0656 Acc: 0.5415 \n",
      "Epoch 22/29\n",
      "----------\n",
      "train Loss: 1.0187 Acc: 0.5053 \n",
      "val Loss: 1.0625 Acc: 0.5450 \n",
      "Epoch 23/29\n",
      "----------\n",
      "train Loss: 1.0136 Acc: 0.5203 \n",
      "val Loss: 1.0596 Acc: 0.5475 \n",
      "Epoch 24/29\n",
      "----------\n",
      "train Loss: 1.0090 Acc: 0.5260 \n",
      "val Loss: 1.0569 Acc: 0.5440 \n",
      "Epoch 25/29\n",
      "----------\n",
      "train Loss: 1.0048 Acc: 0.5377 \n",
      "val Loss: 1.0543 Acc: 0.5410 \n",
      "Epoch 26/29\n",
      "----------\n",
      "train Loss: 1.0009 Acc: 0.5457 \n",
      "val Loss: 1.0518 Acc: 0.5425 \n",
      "Epoch 27/29\n",
      "----------\n",
      "train Loss: 0.9973 Acc: 0.5527 \n",
      "val Loss: 1.0495 Acc: 0.5410 \n",
      "Epoch 28/29\n",
      "----------\n",
      "train Loss: 0.9941 Acc: 0.5540 \n",
      "val Loss: 1.0473 Acc: 0.5395 \n",
      "Epoch 29/29\n",
      "----------\n",
      "train Loss: 0.9911 Acc: 0.5617 \n",
      "val Loss: 1.0452 Acc: 0.5395 \n",
      "Training complete in 0m 31s\n",
      "Best val Acc: 0.547500\n",
      "Epoch generations ( 1 /200) :accuracy(fittest mask) tensor(0.5673, dtype=torch.float64) keep_prob 0.75\n",
      "Epoch generations ( 2 /200) :accuracy(fittest mask) tensor(0.5657, dtype=torch.float64) keep_prob 0.8\n",
      "Epoch generations ( 3 /200) :accuracy(fittest mask) tensor(0.5643, dtype=torch.float64) keep_prob 0.85\n",
      "Epoch generations ( 4 /200) :accuracy(fittest mask) tensor(0.5657, dtype=torch.float64) keep_prob 0.75\n",
      "Epoch generations ( 5 /200) :accuracy(fittest mask) tensor(0.5623, dtype=torch.float64) keep_prob 0.85\n",
      "Epoch generations ( 6 /200) :accuracy(fittest mask) tensor(0.5647, dtype=torch.float64) keep_prob 0.85\n",
      "Epoch generations ( 7 /200) :accuracy(fittest mask) tensor(0.5647, dtype=torch.float64) keep_prob 0.9\n",
      "Epoch generations ( 8 /200) :accuracy(fittest mask) tensor(0.5607, dtype=torch.float64) keep_prob 0.95\n",
      "Epoch generations ( 9 /200) :accuracy(fittest mask) tensor(0.5607, dtype=torch.float64) keep_prob 0.95\n",
      "Epoch generations ( 10 /200) :accuracy(fittest mask) tensor(0.5633, dtype=torch.float64) keep_prob 0.9\n",
      "Epoch 0/29\n",
      "----------\n",
      "train Loss: 1.0495 Acc: 0.5593 \n",
      "val Loss: 1.0555 Acc: 0.5590 \n",
      "Epoch 1/29\n",
      "----------\n",
      "train Loss: 1.0447 Acc: 0.5640 \n",
      "val Loss: 1.0516 Acc: 0.5605 \n",
      "Epoch 2/29\n",
      "----------\n",
      "train Loss: 1.0403 Acc: 0.5530 \n",
      "val Loss: 1.0481 Acc: 0.5505 \n",
      "Epoch 3/29\n",
      "----------\n",
      "train Loss: 1.0364 Acc: 0.5380 \n",
      "val Loss: 1.0447 Acc: 0.5410 \n",
      "Epoch 4/29\n",
      "----------\n",
      "train Loss: 1.0327 Acc: 0.5293 \n",
      "val Loss: 1.0415 Acc: 0.5340 \n",
      "Epoch 5/29\n",
      "----------\n",
      "train Loss: 1.0292 Acc: 0.5260 \n",
      "val Loss: 1.0385 Acc: 0.5300 \n",
      "Epoch 6/29\n",
      "----------\n",
      "train Loss: 1.0260 Acc: 0.5247 \n",
      "val Loss: 1.0357 Acc: 0.5295 \n",
      "Epoch 7/29\n",
      "----------\n",
      "train Loss: 1.0229 Acc: 0.5263 \n",
      "val Loss: 1.0330 Acc: 0.5330 \n",
      "Epoch 8/29\n",
      "----------\n",
      "train Loss: 1.0200 Acc: 0.5273 \n",
      "val Loss: 1.0304 Acc: 0.5325 \n",
      "Epoch 9/29\n",
      "----------\n",
      "train Loss: 1.0173 Acc: 0.5290 \n",
      "val Loss: 1.0280 Acc: 0.5360 \n",
      "Epoch 10/29\n",
      "----------\n",
      "train Loss: 1.0147 Acc: 0.5333 \n",
      "val Loss: 1.0256 Acc: 0.5415 \n",
      "Epoch 11/29\n",
      "----------\n",
      "train Loss: 1.0122 Acc: 0.5400 \n",
      "val Loss: 1.0234 Acc: 0.5495 \n",
      "Epoch 12/29\n",
      "----------\n",
      "train Loss: 1.0099 Acc: 0.5460 \n",
      "val Loss: 1.0213 Acc: 0.5565 \n",
      "Epoch 13/29\n",
      "----------\n",
      "train Loss: 1.0077 Acc: 0.5503 \n",
      "val Loss: 1.0192 Acc: 0.5590 \n",
      "Epoch 14/29\n",
      "----------\n",
      "train Loss: 1.0056 Acc: 0.5553 \n",
      "val Loss: 1.0173 Acc: 0.5630 \n",
      "Epoch 15/29\n",
      "----------\n",
      "train Loss: 1.0037 Acc: 0.5590 \n",
      "val Loss: 1.0155 Acc: 0.5645 \n",
      "Epoch 16/29\n",
      "----------\n",
      "train Loss: 1.0018 Acc: 0.5633 \n",
      "val Loss: 1.0137 Acc: 0.5670 \n",
      "Epoch 17/29\n",
      "----------\n",
      "train Loss: 1.0000 Acc: 0.5673 \n",
      "val Loss: 1.0120 Acc: 0.5700 \n",
      "Epoch 18/29\n",
      "----------\n",
      "train Loss: 0.9983 Acc: 0.5717 \n",
      "val Loss: 1.0104 Acc: 0.5710 \n",
      "Epoch 19/29\n",
      "----------\n",
      "train Loss: 0.9966 Acc: 0.5720 \n",
      "val Loss: 1.0089 Acc: 0.5725 \n",
      "Epoch 20/29\n",
      "----------\n",
      "train Loss: 0.9951 Acc: 0.5723 \n",
      "val Loss: 1.0074 Acc: 0.5745 \n",
      "Epoch 21/29\n",
      "----------\n",
      "train Loss: 0.9936 Acc: 0.5700 \n",
      "val Loss: 1.0060 Acc: 0.5760 \n",
      "Epoch 22/29\n",
      "----------\n",
      "train Loss: 0.9922 Acc: 0.5707 \n",
      "val Loss: 1.0046 Acc: 0.5765 \n",
      "Epoch 23/29\n",
      "----------\n",
      "train Loss: 0.9908 Acc: 0.5710 \n",
      "val Loss: 1.0033 Acc: 0.5765 \n",
      "Epoch 24/29\n",
      "----------\n",
      "train Loss: 0.9896 Acc: 0.5710 \n",
      "val Loss: 1.0021 Acc: 0.5780 \n",
      "Epoch 25/29\n",
      "----------\n",
      "train Loss: 0.9883 Acc: 0.5703 \n",
      "val Loss: 1.0009 Acc: 0.5775 \n",
      "Epoch 26/29\n",
      "----------\n",
      "train Loss: 0.9871 Acc: 0.5707 \n",
      "val Loss: 0.9997 Acc: 0.5775 \n",
      "Epoch 27/29\n",
      "----------\n",
      "train Loss: 0.9860 Acc: 0.5710 \n",
      "val Loss: 0.9986 Acc: 0.5775 \n",
      "Epoch 28/29\n",
      "----------\n",
      "train Loss: 0.9849 Acc: 0.5713 \n",
      "val Loss: 0.9976 Acc: 0.5770 \n",
      "Epoch 29/29\n",
      "----------\n",
      "train Loss: 0.9839 Acc: 0.5730 \n",
      "val Loss: 0.9966 Acc: 0.5775 \n",
      "Training complete in 0m 31s\n",
      "Best val Acc: 0.578000\n",
      "Epoch generations ( 11 /200) :accuracy(fittest mask) tensor(0.5733, dtype=torch.float64) keep_prob 0.9\n",
      "Epoch generations ( 12 /200) :accuracy(fittest mask) tensor(0.5733, dtype=torch.float64) keep_prob 0.9\n",
      "Epoch generations ( 13 /200) :accuracy(fittest mask) tensor(0.5733, dtype=torch.float64) keep_prob 0.9\n",
      "Epoch generations ( 14 /200) :accuracy(fittest mask) tensor(0.5733, dtype=torch.float64) keep_prob 0.9\n",
      "Epoch generations ( 15 /200) :accuracy(fittest mask) tensor(0.5733, dtype=torch.float64) keep_prob 0.9\n",
      "Epoch generations ( 16 /200) :accuracy(fittest mask) tensor(0.5733, dtype=torch.float64) keep_prob 0.9\n",
      "Epoch generations ( 17 /200) :accuracy(fittest mask) tensor(0.5733, dtype=torch.float64) keep_prob 0.9\n",
      "Epoch generations ( 18 /200) :accuracy(fittest mask) tensor(0.5733, dtype=torch.float64) keep_prob 0.9\n",
      "Epoch generations ( 19 /200) :accuracy(fittest mask) tensor(0.5730, dtype=torch.float64) keep_prob 0.95\n",
      "Epoch generations ( 20 /200) :accuracy(fittest mask) tensor(0.5727, dtype=torch.float64) keep_prob 1.0\n",
      "Epoch 0/29\n",
      "----------\n",
      "train Loss: 0.9930 Acc: 0.5720 \n",
      "val Loss: 1.0007 Acc: 0.5775 \n",
      "Epoch 1/29\n",
      "----------\n",
      "train Loss: 0.9917 Acc: 0.5713 \n",
      "val Loss: 0.9995 Acc: 0.5780 \n",
      "Epoch 2/29\n",
      "----------\n",
      "train Loss: 0.9904 Acc: 0.5720 \n",
      "val Loss: 0.9983 Acc: 0.5780 \n",
      "Epoch 3/29\n",
      "----------\n",
      "train Loss: 0.9892 Acc: 0.5723 \n",
      "val Loss: 0.9972 Acc: 0.5780 \n",
      "Epoch 4/29\n",
      "----------\n",
      "train Loss: 0.9881 Acc: 0.5723 \n",
      "val Loss: 0.9961 Acc: 0.5775 \n",
      "Epoch 5/29\n",
      "----------\n",
      "train Loss: 0.9870 Acc: 0.5727 \n",
      "val Loss: 0.9951 Acc: 0.5780 \n",
      "Epoch 6/29\n",
      "----------\n",
      "train Loss: 0.9860 Acc: 0.5730 \n",
      "val Loss: 0.9941 Acc: 0.5780 \n",
      "Epoch 7/29\n",
      "----------\n",
      "train Loss: 0.9850 Acc: 0.5737 \n",
      "val Loss: 0.9932 Acc: 0.5780 \n",
      "Epoch 8/29\n",
      "----------\n",
      "train Loss: 0.9840 Acc: 0.5740 \n",
      "val Loss: 0.9923 Acc: 0.5780 \n",
      "Epoch 9/29\n",
      "----------\n",
      "train Loss: 0.9831 Acc: 0.5740 \n",
      "val Loss: 0.9914 Acc: 0.5780 \n",
      "Epoch 10/29\n",
      "----------\n",
      "train Loss: 0.9822 Acc: 0.5737 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val Loss: 0.9906 Acc: 0.5780 \n",
      "Epoch 11/29\n",
      "----------\n",
      "train Loss: 0.9814 Acc: 0.5733 \n",
      "val Loss: 0.9898 Acc: 0.5785 \n",
      "Epoch 12/29\n",
      "----------\n",
      "train Loss: 0.9806 Acc: 0.5743 \n",
      "val Loss: 0.9890 Acc: 0.5780 \n",
      "Epoch 13/29\n",
      "----------\n",
      "train Loss: 0.9798 Acc: 0.5740 \n",
      "val Loss: 0.9883 Acc: 0.5780 \n",
      "Epoch 14/29\n",
      "----------\n",
      "train Loss: 0.9790 Acc: 0.5737 \n",
      "val Loss: 0.9876 Acc: 0.5780 \n",
      "Epoch 15/29\n",
      "----------\n",
      "train Loss: 0.9783 Acc: 0.5730 \n",
      "val Loss: 0.9869 Acc: 0.5775 \n",
      "Epoch 16/29\n",
      "----------\n",
      "train Loss: 0.9776 Acc: 0.5733 \n",
      "val Loss: 0.9862 Acc: 0.5770 \n",
      "Epoch 17/29\n",
      "----------\n",
      "train Loss: 0.9769 Acc: 0.5740 \n",
      "val Loss: 0.9856 Acc: 0.5770 \n",
      "Epoch 18/29\n",
      "----------\n",
      "train Loss: 0.9763 Acc: 0.5747 \n",
      "val Loss: 0.9850 Acc: 0.5770 \n",
      "Epoch 19/29\n",
      "----------\n",
      "train Loss: 0.9757 Acc: 0.5743 \n",
      "val Loss: 0.9844 Acc: 0.5770 \n",
      "Epoch 20/29\n",
      "----------\n",
      "train Loss: 0.9751 Acc: 0.5743 \n",
      "val Loss: 0.9839 Acc: 0.5780 \n",
      "Epoch 21/29\n",
      "----------\n",
      "train Loss: 0.9745 Acc: 0.5743 \n",
      "val Loss: 0.9833 Acc: 0.5780 \n",
      "Epoch 22/29\n",
      "----------\n",
      "train Loss: 0.9739 Acc: 0.5740 \n",
      "val Loss: 0.9828 Acc: 0.5780 \n",
      "Epoch 23/29\n",
      "----------\n",
      "train Loss: 0.9734 Acc: 0.5743 \n",
      "val Loss: 0.9823 Acc: 0.5780 \n",
      "Epoch 24/29\n",
      "----------\n",
      "train Loss: 0.9729 Acc: 0.5743 \n",
      "val Loss: 0.9819 Acc: 0.5780 \n",
      "Epoch 25/29\n",
      "----------\n",
      "train Loss: 0.9724 Acc: 0.5740 \n",
      "val Loss: 0.9814 Acc: 0.5780 \n",
      "Epoch 26/29\n",
      "----------\n",
      "train Loss: 0.9719 Acc: 0.5740 \n",
      "val Loss: 0.9810 Acc: 0.5780 \n",
      "Epoch 27/29\n",
      "----------\n",
      "train Loss: 0.9714 Acc: 0.5740 \n",
      "val Loss: 0.9806 Acc: 0.5785 \n",
      "Epoch 28/29\n",
      "----------\n",
      "train Loss: 0.9710 Acc: 0.5743 \n",
      "val Loss: 0.9802 Acc: 0.5785 \n",
      "Epoch 29/29\n",
      "----------\n",
      "train Loss: 0.9706 Acc: 0.5747 \n",
      "val Loss: 0.9798 Acc: 0.5785 \n",
      "Training complete in 0m 26s\n",
      "Best val Acc: 0.578500\n",
      "Epoch generations ( 21 /200) :accuracy(fittest mask) tensor(0.5737, dtype=torch.float64) keep_prob 1.0\n",
      "Epoch generations ( 22 /200) :accuracy(fittest mask) tensor(0.5737, dtype=torch.float64) keep_prob 1.0\n",
      "Epoch generations ( 23 /200) :accuracy(fittest mask) tensor(0.5737, dtype=torch.float64) keep_prob 1.0\n",
      "Epoch generations ( 24 /200) :accuracy(fittest mask) tensor(0.5737, dtype=torch.float64) keep_prob 1.0\n",
      "Epoch generations ( 25 /200) :accuracy(fittest mask) tensor(0.5737, dtype=torch.float64) keep_prob 1.0\n",
      "Epoch generations ( 26 /200) :accuracy(fittest mask) tensor(0.5737, dtype=torch.float64) keep_prob 1.0\n",
      "Epoch generations ( 27 /200) :accuracy(fittest mask) tensor(0.5737, dtype=torch.float64) keep_prob 1.0\n",
      "Epoch generations ( 28 /200) :accuracy(fittest mask) tensor(0.5737, dtype=torch.float64) keep_prob 1.0\n",
      "Epoch generations ( 29 /200) :accuracy(fittest mask) tensor(0.5737, dtype=torch.float64) keep_prob 1.0\n",
      "Epoch generations ( 30 /200) :accuracy(fittest mask) tensor(0.5737, dtype=torch.float64) keep_prob 1.0\n",
      "Epoch 0/29\n",
      "----------\n",
      "train Loss: 0.9806 Acc: 0.5743 \n",
      "val Loss: 0.9890 Acc: 0.5780 \n",
      "Epoch 1/29\n",
      "----------\n",
      "train Loss: 0.9798 Acc: 0.5743 \n",
      "val Loss: 0.9883 Acc: 0.5780 \n",
      "Epoch 2/29\n",
      "----------\n",
      "train Loss: 0.9790 Acc: 0.5737 \n",
      "val Loss: 0.9876 Acc: 0.5780 \n",
      "Epoch 3/29\n",
      "----------\n",
      "train Loss: 0.9783 Acc: 0.5730 \n",
      "val Loss: 0.9869 Acc: 0.5775 \n",
      "Epoch 4/29\n",
      "----------\n",
      "train Loss: 0.9776 Acc: 0.5733 \n",
      "val Loss: 0.9862 Acc: 0.5770 \n",
      "Epoch 5/29\n",
      "----------\n",
      "train Loss: 0.9769 Acc: 0.5740 \n",
      "val Loss: 0.9856 Acc: 0.5770 \n",
      "Epoch 6/29\n",
      "----------\n",
      "train Loss: 0.9763 Acc: 0.5747 \n",
      "val Loss: 0.9850 Acc: 0.5770 \n",
      "Epoch 7/29\n",
      "----------\n",
      "train Loss: 0.9757 Acc: 0.5743 \n",
      "val Loss: 0.9844 Acc: 0.5770 \n",
      "Epoch 8/29\n",
      "----------\n",
      "train Loss: 0.9751 Acc: 0.5743 \n",
      "val Loss: 0.9839 Acc: 0.5780 \n",
      "Epoch 9/29\n",
      "----------\n",
      "train Loss: 0.9745 Acc: 0.5743 \n",
      "val Loss: 0.9834 Acc: 0.5780 \n",
      "Epoch 10/29\n",
      "----------\n",
      "train Loss: 0.9740 Acc: 0.5740 \n",
      "val Loss: 0.9828 Acc: 0.5780 \n",
      "Epoch 11/29\n",
      "----------\n",
      "train Loss: 0.9734 Acc: 0.5743 \n",
      "val Loss: 0.9824 Acc: 0.5780 \n",
      "Epoch 12/29\n",
      "----------\n",
      "train Loss: 0.9729 Acc: 0.5743 \n",
      "val Loss: 0.9819 Acc: 0.5775 \n",
      "Epoch 13/29\n",
      "----------\n",
      "train Loss: 0.9724 Acc: 0.5740 \n",
      "val Loss: 0.9814 Acc: 0.5780 \n",
      "Epoch 14/29\n",
      "----------\n",
      "train Loss: 0.9719 Acc: 0.5747 \n",
      "val Loss: 0.9810 Acc: 0.5780 \n",
      "Epoch 15/29\n",
      "----------\n",
      "train Loss: 0.9714 Acc: 0.5740 \n",
      "val Loss: 0.9806 Acc: 0.5785 \n",
      "Epoch 16/29\n",
      "----------\n",
      "train Loss: 0.9710 Acc: 0.5743 \n",
      "val Loss: 0.9802 Acc: 0.5790 \n",
      "Epoch 17/29\n",
      "----------\n",
      "train Loss: 0.9706 Acc: 0.5747 \n",
      "val Loss: 0.9798 Acc: 0.5785 \n",
      "Epoch 18/29\n",
      "----------\n",
      "train Loss: 0.9701 Acc: 0.5750 \n",
      "val Loss: 0.9794 Acc: 0.5785 \n",
      "Epoch 19/29\n",
      "----------\n",
      "train Loss: 0.9697 Acc: 0.5757 \n",
      "val Loss: 0.9790 Acc: 0.5770 \n",
      "Epoch 20/29\n",
      "----------\n",
      "train Loss: 0.9693 Acc: 0.5757 \n",
      "val Loss: 0.9787 Acc: 0.5765 \n",
      "Epoch 21/29\n",
      "----------\n",
      "train Loss: 0.9689 Acc: 0.5757 \n",
      "val Loss: 0.9783 Acc: 0.5765 \n",
      "Epoch 22/29\n",
      "----------\n",
      "train Loss: 0.9686 Acc: 0.5757 \n",
      "val Loss: 0.9780 Acc: 0.5760 \n",
      "Epoch 23/29\n",
      "----------\n",
      "train Loss: 0.9682 Acc: 0.5760 \n",
      "val Loss: 0.9777 Acc: 0.5755 \n",
      "Epoch 24/29\n",
      "----------\n",
      "train Loss: 0.9678 Acc: 0.5760 \n",
      "val Loss: 0.9774 Acc: 0.5760 \n",
      "Epoch 25/29\n",
      "----------\n",
      "train Loss: 0.9675 Acc: 0.5763 \n",
      "val Loss: 0.9771 Acc: 0.5760 \n",
      "Epoch 26/29\n",
      "----------\n",
      "train Loss: 0.9672 Acc: 0.5760 \n",
      "val Loss: 0.9768 Acc: 0.5760 \n",
      "Epoch 27/29\n",
      "----------\n",
      "train Loss: 0.9668 Acc: 0.5760 \n",
      "val Loss: 0.9765 Acc: 0.5765 \n",
      "Epoch 28/29\n",
      "----------\n",
      "train Loss: 0.9665 Acc: 0.5763 \n",
      "val Loss: 0.9763 Acc: 0.5765 \n",
      "Epoch 29/29\n",
      "----------\n",
      "train Loss: 0.9662 Acc: 0.5767 \n",
      "val Loss: 0.9760 Acc: 0.5770 \n",
      "Training complete in 0m 19s\n",
      "Best val Acc: 0.579000\n",
      "Epoch generations ( 31 /200) :accuracy(fittest mask) tensor(0.5750, dtype=torch.float64) keep_prob 1.0\n",
      "Epoch generations ( 32 /200) :accuracy(fittest mask) tensor(0.5750, dtype=torch.float64) keep_prob 1.0\n",
      "Epoch generations ( 33 /200) :accuracy(fittest mask) tensor(0.5750, dtype=torch.float64) keep_prob 1.0\n",
      "Epoch generations ( 34 /200) :accuracy(fittest mask) tensor(0.5750, dtype=torch.float64) keep_prob 1.0\n",
      "Epoch generations ( 35 /200) :accuracy(fittest mask) tensor(0.5750, dtype=torch.float64) keep_prob 1.0\n",
      "Epoch generations ( 36 /200) :accuracy(fittest mask) tensor(0.5750, dtype=torch.float64) keep_prob 1.0\n",
      "Epoch generations ( 37 /200) :accuracy(fittest mask) tensor(0.5750, dtype=torch.float64) keep_prob 1.0\n",
      "Epoch generations ( 38 /200) :accuracy(fittest mask) tensor(0.5750, dtype=torch.float64) keep_prob 1.0\n",
      "Epoch generations ( 39 /200) :accuracy(fittest mask) tensor(0.5750, dtype=torch.float64) keep_prob 1.0\n",
      "Epoch generations ( 40 /200) :accuracy(fittest mask) tensor(0.5750, dtype=torch.float64) keep_prob 1.0\n",
      "Epoch 0/29\n",
      "----------\n",
      "train Loss: 0.9706 Acc: 0.5747 \n",
      "val Loss: 0.9798 Acc: 0.5785 \n",
      "Epoch 1/29\n",
      "----------\n",
      "train Loss: 0.9701 Acc: 0.5750 \n",
      "val Loss: 0.9794 Acc: 0.5785 \n",
      "Epoch 2/29\n",
      "----------\n",
      "train Loss: 0.9697 Acc: 0.5757 \n",
      "val Loss: 0.9790 Acc: 0.5770 \n",
      "Epoch 3/29\n",
      "----------\n",
      "train Loss: 0.9693 Acc: 0.5757 \n",
      "val Loss: 0.9787 Acc: 0.5765 \n",
      "Epoch 4/29\n",
      "----------\n",
      "train Loss: 0.9689 Acc: 0.5757 \n",
      "val Loss: 0.9783 Acc: 0.5765 \n",
      "Epoch 5/29\n",
      "----------\n",
      "train Loss: 0.9686 Acc: 0.5757 \n",
      "val Loss: 0.9780 Acc: 0.5765 \n",
      "Epoch 6/29\n",
      "----------\n",
      "train Loss: 0.9682 Acc: 0.5760 \n",
      "val Loss: 0.9777 Acc: 0.5755 \n",
      "Epoch 7/29\n",
      "----------\n",
      "train Loss: 0.9679 Acc: 0.5760 \n",
      "val Loss: 0.9774 Acc: 0.5760 \n",
      "Epoch 8/29\n",
      "----------\n",
      "train Loss: 0.9675 Acc: 0.5760 \n",
      "val Loss: 0.9771 Acc: 0.5760 \n",
      "Epoch 9/29\n",
      "----------\n",
      "train Loss: 0.9672 Acc: 0.5760 \n",
      "val Loss: 0.9768 Acc: 0.5760 \n",
      "Epoch 10/29\n",
      "----------\n",
      "train Loss: 0.9668 Acc: 0.5763 \n",
      "val Loss: 0.9765 Acc: 0.5765 \n",
      "Epoch 11/29\n",
      "----------\n",
      "train Loss: 0.9665 Acc: 0.5767 \n",
      "val Loss: 0.9763 Acc: 0.5765 \n",
      "Epoch 12/29\n",
      "----------\n",
      "train Loss: 0.9662 Acc: 0.5770 \n",
      "val Loss: 0.9760 Acc: 0.5770 \n",
      "Epoch 13/29\n",
      "----------\n",
      "train Loss: 0.9659 Acc: 0.5770 \n",
      "val Loss: 0.9758 Acc: 0.5765 \n",
      "Epoch 14/29\n",
      "----------\n",
      "train Loss: 0.9656 Acc: 0.5767 \n",
      "val Loss: 0.9756 Acc: 0.5765 \n",
      "Epoch 15/29\n",
      "----------\n",
      "train Loss: 0.9654 Acc: 0.5763 \n",
      "val Loss: 0.9753 Acc: 0.5765 \n",
      "Epoch 16/29\n",
      "----------\n",
      "train Loss: 0.9651 Acc: 0.5763 \n",
      "val Loss: 0.9751 Acc: 0.5765 \n",
      "Epoch 17/29\n",
      "----------\n",
      "train Loss: 0.9648 Acc: 0.5767 \n",
      "val Loss: 0.9749 Acc: 0.5755 \n",
      "Epoch 18/29\n",
      "----------\n",
      "train Loss: 0.9646 Acc: 0.5777 \n",
      "val Loss: 0.9747 Acc: 0.5760 \n",
      "Epoch 19/29\n",
      "----------\n",
      "train Loss: 0.9643 Acc: 0.5777 \n",
      "val Loss: 0.9745 Acc: 0.5760 \n",
      "Epoch 20/29\n",
      "----------\n",
      "train Loss: 0.9641 Acc: 0.5773 \n",
      "val Loss: 0.9743 Acc: 0.5760 \n",
      "Epoch 21/29\n",
      "----------\n",
      "train Loss: 0.9638 Acc: 0.5773 \n",
      "val Loss: 0.9741 Acc: 0.5760 \n",
      "Epoch 22/29\n",
      "----------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train Loss: 0.9636 Acc: 0.5777 \n",
      "val Loss: 0.9739 Acc: 0.5760 \n",
      "Epoch 23/29\n",
      "----------\n",
      "train Loss: 0.9634 Acc: 0.5777 \n",
      "val Loss: 0.9738 Acc: 0.5765 \n",
      "Epoch 24/29\n",
      "----------\n",
      "train Loss: 0.9632 Acc: 0.5780 \n",
      "val Loss: 0.9736 Acc: 0.5765 \n",
      "Epoch 25/29\n",
      "----------\n",
      "train Loss: 0.9629 Acc: 0.5783 \n",
      "val Loss: 0.9735 Acc: 0.5765 \n",
      "Epoch 26/29\n",
      "----------\n",
      "train Loss: 0.9627 Acc: 0.5787 \n",
      "val Loss: 0.9733 Acc: 0.5760 \n",
      "Epoch 27/29\n",
      "----------\n",
      "train Loss: 0.9625 Acc: 0.5790 \n",
      "val Loss: 0.9732 Acc: 0.5775 \n",
      "Epoch 28/29\n",
      "----------\n",
      "train Loss: 0.9623 Acc: 0.5793 \n",
      "val Loss: 0.9730 Acc: 0.5770 \n",
      "Epoch 29/29\n",
      "----------\n",
      "train Loss: 0.9621 Acc: 0.5790 \n",
      "val Loss: 0.9729 Acc: 0.5775 \n",
      "Training complete in 0m 19s\n",
      "Best val Acc: 0.578500\n",
      "Epoch generations ( 41 /200) :accuracy(fittest mask) tensor(0.5750, dtype=torch.float64) keep_prob 1.0\n",
      "Epoch generations ( 42 /200) :accuracy(fittest mask) tensor(0.5750, dtype=torch.float64) keep_prob 1.0\n",
      "Epoch generations ( 43 /200) :accuracy(fittest mask) tensor(0.5750, dtype=torch.float64) keep_prob 1.0\n",
      "Epoch generations ( 44 /200) :accuracy(fittest mask) tensor(0.5750, dtype=torch.float64) keep_prob 1.0\n",
      "Epoch generations ( 45 /200) :accuracy(fittest mask) tensor(0.5750, dtype=torch.float64) keep_prob 1.0\n",
      "Epoch generations ( 46 /200) :accuracy(fittest mask) tensor(0.5750, dtype=torch.float64) keep_prob 1.0\n",
      "Epoch generations ( 47 /200) :accuracy(fittest mask) tensor(0.5750, dtype=torch.float64) keep_prob 1.0\n",
      "Epoch generations ( 48 /200) :accuracy(fittest mask) tensor(0.5750, dtype=torch.float64) keep_prob 1.0\n",
      "Epoch generations ( 49 /200) :accuracy(fittest mask) tensor(0.5750, dtype=torch.float64) keep_prob 1.0\n",
      "Epoch generations ( 50 /200) :accuracy(fittest mask) tensor(0.5750, dtype=torch.float64) keep_prob 1.0\n",
      "Epoch 0/29\n",
      "----------\n",
      "train Loss: 0.9701 Acc: 0.5750 \n",
      "val Loss: 0.9794 Acc: 0.5780 \n",
      "Epoch 1/29\n",
      "----------\n",
      "train Loss: 0.9697 Acc: 0.5753 \n",
      "val Loss: 0.9790 Acc: 0.5770 \n",
      "Epoch 2/29\n",
      "----------\n",
      "train Loss: 0.9693 Acc: 0.5757 \n",
      "val Loss: 0.9787 Acc: 0.5765 \n",
      "Epoch 3/29\n",
      "----------\n",
      "train Loss: 0.9689 Acc: 0.5757 \n",
      "val Loss: 0.9783 Acc: 0.5765 \n",
      "Epoch 4/29\n",
      "----------\n",
      "train Loss: 0.9686 Acc: 0.5757 \n",
      "val Loss: 0.9780 Acc: 0.5760 \n",
      "Epoch 5/29\n",
      "----------\n",
      "train Loss: 0.9682 Acc: 0.5760 \n",
      "val Loss: 0.9777 Acc: 0.5755 \n",
      "Epoch 6/29\n",
      "----------\n",
      "train Loss: 0.9679 Acc: 0.5760 \n",
      "val Loss: 0.9774 Acc: 0.5755 \n",
      "Epoch 7/29\n",
      "----------\n",
      "train Loss: 0.9675 Acc: 0.5760 \n",
      "val Loss: 0.9771 Acc: 0.5760 \n",
      "Epoch 8/29\n",
      "----------\n",
      "train Loss: 0.9672 Acc: 0.5760 \n",
      "val Loss: 0.9768 Acc: 0.5760 \n",
      "Epoch 9/29\n",
      "----------\n",
      "train Loss: 0.9668 Acc: 0.5760 \n",
      "val Loss: 0.9766 Acc: 0.5765 \n",
      "Epoch 10/29\n",
      "----------\n",
      "train Loss: 0.9665 Acc: 0.5767 \n",
      "val Loss: 0.9763 Acc: 0.5765 \n",
      "Epoch 11/29\n",
      "----------\n",
      "train Loss: 0.9662 Acc: 0.5770 \n",
      "val Loss: 0.9760 Acc: 0.5770 \n",
      "Epoch 12/29\n",
      "----------\n",
      "train Loss: 0.9659 Acc: 0.5770 \n",
      "val Loss: 0.9758 Acc: 0.5765 \n",
      "Epoch 13/29\n",
      "----------\n",
      "train Loss: 0.9656 Acc: 0.5767 \n",
      "val Loss: 0.9756 Acc: 0.5765 \n",
      "Epoch 14/29\n",
      "----------\n",
      "train Loss: 0.9654 Acc: 0.5763 \n",
      "val Loss: 0.9753 Acc: 0.5765 \n",
      "Epoch 15/29\n",
      "----------\n",
      "train Loss: 0.9651 Acc: 0.5763 \n",
      "val Loss: 0.9751 Acc: 0.5765 \n",
      "Epoch 16/29\n",
      "----------\n",
      "train Loss: 0.9648 Acc: 0.5767 \n",
      "val Loss: 0.9749 Acc: 0.5755 \n",
      "Epoch 17/29\n",
      "----------\n",
      "train Loss: 0.9646 Acc: 0.5777 \n",
      "val Loss: 0.9747 Acc: 0.5760 \n",
      "Epoch 18/29\n",
      "----------\n",
      "train Loss: 0.9643 Acc: 0.5777 \n",
      "val Loss: 0.9745 Acc: 0.5760 \n",
      "Epoch 19/29\n",
      "----------\n",
      "train Loss: 0.9641 Acc: 0.5773 \n",
      "val Loss: 0.9743 Acc: 0.5760 \n",
      "Epoch 20/29\n",
      "----------\n",
      "train Loss: 0.9638 Acc: 0.5777 \n",
      "val Loss: 0.9741 Acc: 0.5760 \n",
      "Epoch 21/29\n",
      "----------\n",
      "train Loss: 0.9636 Acc: 0.5777 \n",
      "val Loss: 0.9740 Acc: 0.5760 \n",
      "Epoch 22/29\n",
      "----------\n",
      "train Loss: 0.9634 Acc: 0.5777 \n",
      "val Loss: 0.9738 Acc: 0.5765 \n",
      "Epoch 23/29\n",
      "----------\n",
      "train Loss: 0.9632 Acc: 0.5780 \n",
      "val Loss: 0.9736 Acc: 0.5765 \n",
      "Epoch 24/29\n",
      "----------\n",
      "train Loss: 0.9629 Acc: 0.5783 \n",
      "val Loss: 0.9735 Acc: 0.5765 \n",
      "Epoch 25/29\n",
      "----------\n",
      "train Loss: 0.9627 Acc: 0.5787 \n",
      "val Loss: 0.9733 Acc: 0.5760 \n",
      "Epoch 26/29\n",
      "----------\n",
      "train Loss: 0.9625 Acc: 0.5793 \n",
      "val Loss: 0.9732 Acc: 0.5770 \n",
      "Epoch 27/29\n",
      "----------\n",
      "train Loss: 0.9623 Acc: 0.5793 \n",
      "val Loss: 0.9730 Acc: 0.5770 \n",
      "Epoch 28/29\n",
      "----------\n",
      "train Loss: 0.9621 Acc: 0.5790 \n",
      "val Loss: 0.9729 Acc: 0.5775 \n",
      "Epoch 29/29\n",
      "----------\n",
      "train Loss: 0.9619 Acc: 0.5790 \n",
      "val Loss: 0.9728 Acc: 0.5775 \n",
      "Training complete in 0m 18s\n",
      "Best val Acc: 0.578000\n",
      "Epoch generations ( 51 /200) :accuracy(fittest mask) tensor(0.5753, dtype=torch.float64) keep_prob 1.0\n",
      "Epoch generations ( 52 /200) :accuracy(fittest mask) tensor(0.5753, dtype=torch.float64) keep_prob 1.0\n",
      "Epoch generations ( 53 /200) :accuracy(fittest mask) tensor(0.5753, dtype=torch.float64) keep_prob 1.0\n",
      "Epoch generations ( 54 /200) :accuracy(fittest mask) tensor(0.5753, dtype=torch.float64) keep_prob 1.0\n",
      "Epoch generations ( 55 /200) :accuracy(fittest mask) tensor(0.5753, dtype=torch.float64) keep_prob 1.0\n",
      "Epoch generations ( 56 /200) :accuracy(fittest mask) tensor(0.5753, dtype=torch.float64) keep_prob 1.0\n",
      "Epoch generations ( 57 /200) :accuracy(fittest mask) tensor(0.5753, dtype=torch.float64) keep_prob 1.0\n",
      "Epoch generations ( 58 /200) :accuracy(fittest mask) tensor(0.5753, dtype=torch.float64) keep_prob 1.0\n",
      "Epoch generations ( 59 /200) :accuracy(fittest mask) tensor(0.5753, dtype=torch.float64) keep_prob 1.0\n",
      "Epoch generations ( 60 /200) :accuracy(fittest mask) tensor(0.5753, dtype=torch.float64) keep_prob 1.0\n",
      "Epoch 0/29\n",
      "----------\n",
      "train Loss: 0.9697 Acc: 0.5753 \n",
      "val Loss: 0.9790 Acc: 0.5770 \n",
      "Epoch 1/29\n",
      "----------\n",
      "train Loss: 0.9693 Acc: 0.5757 \n",
      "val Loss: 0.9787 Acc: 0.5765 \n",
      "Epoch 2/29\n",
      "----------\n",
      "train Loss: 0.9690 Acc: 0.5757 \n",
      "val Loss: 0.9783 Acc: 0.5765 \n",
      "Epoch 3/29\n",
      "----------\n",
      "train Loss: 0.9686 Acc: 0.5757 \n",
      "val Loss: 0.9780 Acc: 0.5760 \n",
      "Epoch 4/29\n",
      "----------\n",
      "train Loss: 0.9682 Acc: 0.5760 \n",
      "val Loss: 0.9777 Acc: 0.5755 \n",
      "Epoch 5/29\n",
      "----------\n",
      "train Loss: 0.9679 Acc: 0.5760 \n",
      "val Loss: 0.9774 Acc: 0.5760 \n",
      "Epoch 6/29\n",
      "----------\n",
      "train Loss: 0.9675 Acc: 0.5760 \n",
      "val Loss: 0.9771 Acc: 0.5760 \n",
      "Epoch 7/29\n",
      "----------\n",
      "train Loss: 0.9672 Acc: 0.5760 \n",
      "val Loss: 0.9768 Acc: 0.5760 \n",
      "Epoch 8/29\n",
      "----------\n",
      "train Loss: 0.9669 Acc: 0.5763 \n",
      "val Loss: 0.9766 Acc: 0.5765 \n",
      "Epoch 9/29\n",
      "----------\n",
      "train Loss: 0.9665 Acc: 0.5767 \n",
      "val Loss: 0.9763 Acc: 0.5765 \n",
      "Epoch 10/29\n",
      "----------\n",
      "train Loss: 0.9662 Acc: 0.5770 \n",
      "val Loss: 0.9760 Acc: 0.5770 \n",
      "Epoch 11/29\n",
      "----------\n",
      "train Loss: 0.9659 Acc: 0.5770 \n",
      "val Loss: 0.9758 Acc: 0.5765 \n",
      "Epoch 12/29\n",
      "----------\n",
      "train Loss: 0.9656 Acc: 0.5767 \n",
      "val Loss: 0.9756 Acc: 0.5765 \n",
      "Epoch 13/29\n",
      "----------\n",
      "train Loss: 0.9654 Acc: 0.5763 \n",
      "val Loss: 0.9753 Acc: 0.5765 \n",
      "Epoch 14/29\n",
      "----------\n",
      "train Loss: 0.9651 Acc: 0.5763 \n",
      "val Loss: 0.9751 Acc: 0.5765 \n",
      "Epoch 15/29\n",
      "----------\n",
      "train Loss: 0.9648 Acc: 0.5767 \n",
      "val Loss: 0.9749 Acc: 0.5755 \n",
      "Epoch 16/29\n",
      "----------\n",
      "train Loss: 0.9646 Acc: 0.5777 \n",
      "val Loss: 0.9747 Acc: 0.5760 \n",
      "Epoch 17/29\n",
      "----------\n",
      "train Loss: 0.9643 Acc: 0.5777 \n",
      "val Loss: 0.9745 Acc: 0.5760 \n",
      "Epoch 18/29\n",
      "----------\n",
      "train Loss: 0.9641 Acc: 0.5773 \n",
      "val Loss: 0.9743 Acc: 0.5760 \n",
      "Epoch 19/29\n",
      "----------\n",
      "train Loss: 0.9638 Acc: 0.5773 \n",
      "val Loss: 0.9741 Acc: 0.5760 \n",
      "Epoch 20/29\n",
      "----------\n",
      "train Loss: 0.9636 Acc: 0.5777 \n",
      "val Loss: 0.9740 Acc: 0.5760 \n",
      "Epoch 21/29\n",
      "----------\n",
      "train Loss: 0.9634 Acc: 0.5777 \n",
      "val Loss: 0.9738 Acc: 0.5765 \n",
      "Epoch 22/29\n",
      "----------\n",
      "train Loss: 0.9632 Acc: 0.5780 \n",
      "val Loss: 0.9736 Acc: 0.5765 \n",
      "Epoch 23/29\n",
      "----------\n",
      "train Loss: 0.9629 Acc: 0.5783 \n",
      "val Loss: 0.9735 Acc: 0.5765 \n",
      "Epoch 24/29\n",
      "----------\n",
      "train Loss: 0.9627 Acc: 0.5787 \n",
      "val Loss: 0.9733 Acc: 0.5765 \n",
      "Epoch 25/29\n",
      "----------\n",
      "train Loss: 0.9625 Acc: 0.5793 \n",
      "val Loss: 0.9732 Acc: 0.5770 \n",
      "Epoch 26/29\n",
      "----------\n",
      "train Loss: 0.9623 Acc: 0.5793 \n",
      "val Loss: 0.9730 Acc: 0.5770 \n",
      "Epoch 27/29\n",
      "----------\n",
      "train Loss: 0.9621 Acc: 0.5790 \n",
      "val Loss: 0.9729 Acc: 0.5775 \n",
      "Epoch 28/29\n",
      "----------\n",
      "train Loss: 0.9619 Acc: 0.5790 \n",
      "val Loss: 0.9728 Acc: 0.5775 \n",
      "Epoch 29/29\n",
      "----------\n",
      "train Loss: 0.9617 Acc: 0.5790 \n",
      "val Loss: 0.9726 Acc: 0.5775 \n",
      "Training complete in 0m 19s\n",
      "Best val Acc: 0.577500\n",
      "Epoch generations ( 61 /200) :accuracy(fittest mask) tensor(0.5797, dtype=torch.float64) keep_prob 1.0\n",
      "Epoch generations ( 62 /200) :accuracy(fittest mask) tensor(0.5797, dtype=torch.float64) keep_prob 1.0\n",
      "Epoch generations ( 63 /200) :accuracy(fittest mask) tensor(0.5797, dtype=torch.float64) keep_prob 1.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch generations ( 64 /200) :accuracy(fittest mask) tensor(0.5797, dtype=torch.float64) keep_prob 1.0\n",
      "Epoch generations ( 65 /200) :accuracy(fittest mask) tensor(0.5797, dtype=torch.float64) keep_prob 1.0\n",
      "Epoch generations ( 66 /200) :accuracy(fittest mask) tensor(0.5797, dtype=torch.float64) keep_prob 1.0\n",
      "Epoch generations ( 67 /200) :accuracy(fittest mask) tensor(0.5797, dtype=torch.float64) keep_prob 1.0\n",
      "Epoch generations ( 68 /200) :accuracy(fittest mask) tensor(0.5797, dtype=torch.float64) keep_prob 1.0\n",
      "Epoch generations ( 69 /200) :accuracy(fittest mask) tensor(0.5797, dtype=torch.float64) keep_prob 1.0\n",
      "Epoch generations ( 70 /200) :accuracy(fittest mask) tensor(0.5797, dtype=torch.float64) keep_prob 1.0\n",
      "Epoch 0/29\n",
      "----------\n",
      "train Loss: 0.9619 Acc: 0.5790 \n",
      "val Loss: 0.9728 Acc: 0.5775 \n",
      "Epoch 1/29\n",
      "----------\n",
      "train Loss: 0.9617 Acc: 0.5790 \n",
      "val Loss: 0.9726 Acc: 0.5770 \n",
      "Epoch 2/29\n",
      "----------\n",
      "train Loss: 0.9616 Acc: 0.5790 \n",
      "val Loss: 0.9725 Acc: 0.5770 \n",
      "Epoch 3/29\n",
      "----------\n",
      "train Loss: 0.9614 Acc: 0.5793 \n",
      "val Loss: 0.9724 Acc: 0.5770 \n",
      "Epoch 4/29\n",
      "----------\n",
      "train Loss: 0.9612 Acc: 0.5797 \n",
      "val Loss: 0.9723 Acc: 0.5770 \n",
      "Epoch 5/29\n",
      "----------\n",
      "train Loss: 0.9610 Acc: 0.5797 \n",
      "val Loss: 0.9722 Acc: 0.5770 \n",
      "Epoch 6/29\n",
      "----------\n",
      "train Loss: 0.9609 Acc: 0.5793 \n",
      "val Loss: 0.9721 Acc: 0.5765 \n",
      "Epoch 7/29\n",
      "----------\n",
      "train Loss: 0.9607 Acc: 0.5793 \n",
      "val Loss: 0.9720 Acc: 0.5765 \n",
      "Epoch 8/29\n",
      "----------\n",
      "train Loss: 0.9605 Acc: 0.5797 \n",
      "val Loss: 0.9719 Acc: 0.5770 \n",
      "Epoch 9/29\n",
      "----------\n",
      "train Loss: 0.9604 Acc: 0.5797 \n",
      "val Loss: 0.9718 Acc: 0.5775 \n",
      "Epoch 10/29\n",
      "----------\n",
      "train Loss: 0.9602 Acc: 0.5797 \n",
      "val Loss: 0.9717 Acc: 0.5775 \n",
      "Epoch 11/29\n",
      "----------\n",
      "train Loss: 0.9601 Acc: 0.5800 \n",
      "val Loss: 0.9716 Acc: 0.5780 \n",
      "Epoch 12/29\n",
      "----------\n",
      "train Loss: 0.9599 Acc: 0.5803 \n",
      "val Loss: 0.9715 Acc: 0.5780 \n",
      "Epoch 13/29\n",
      "----------\n",
      "train Loss: 0.9598 Acc: 0.5803 \n",
      "val Loss: 0.9715 Acc: 0.5775 \n",
      "Epoch 14/29\n",
      "----------\n",
      "train Loss: 0.9597 Acc: 0.5807 \n",
      "val Loss: 0.9714 Acc: 0.5780 \n",
      "Epoch 15/29\n",
      "----------\n",
      "train Loss: 0.9595 Acc: 0.5803 \n",
      "val Loss: 0.9713 Acc: 0.5780 \n",
      "Epoch 16/29\n",
      "----------\n",
      "train Loss: 0.9594 Acc: 0.5797 \n",
      "val Loss: 0.9712 Acc: 0.5780 \n",
      "Epoch 17/29\n",
      "----------\n",
      "train Loss: 0.9592 Acc: 0.5800 \n",
      "val Loss: 0.9712 Acc: 0.5780 \n",
      "Epoch 18/29\n",
      "----------\n",
      "train Loss: 0.9591 Acc: 0.5800 \n",
      "val Loss: 0.9711 Acc: 0.5780 \n",
      "Epoch 19/29\n",
      "----------\n",
      "train Loss: 0.9590 Acc: 0.5800 \n",
      "val Loss: 0.9710 Acc: 0.5780 \n",
      "Epoch 20/29\n",
      "----------\n",
      "train Loss: 0.9588 Acc: 0.5800 \n",
      "val Loss: 0.9710 Acc: 0.5785 \n",
      "Epoch 21/29\n",
      "----------\n",
      "train Loss: 0.9587 Acc: 0.5797 \n",
      "val Loss: 0.9709 Acc: 0.5790 \n",
      "Epoch 22/29\n",
      "----------\n",
      "train Loss: 0.9586 Acc: 0.5797 \n",
      "val Loss: 0.9708 Acc: 0.5795 \n",
      "Epoch 23/29\n",
      "----------\n",
      "train Loss: 0.9585 Acc: 0.5797 \n",
      "val Loss: 0.9708 Acc: 0.5795 \n",
      "Epoch 24/29\n",
      "----------\n",
      "train Loss: 0.9583 Acc: 0.5800 \n",
      "val Loss: 0.9707 Acc: 0.5795 \n",
      "Epoch 25/29\n",
      "----------\n",
      "train Loss: 0.9582 Acc: 0.5803 \n",
      "val Loss: 0.9707 Acc: 0.5795 \n",
      "Epoch 26/29\n",
      "----------\n",
      "train Loss: 0.9581 Acc: 0.5807 \n",
      "val Loss: 0.9706 Acc: 0.5795 \n",
      "Epoch 27/29\n",
      "----------\n",
      "train Loss: 0.9580 Acc: 0.5803 \n",
      "val Loss: 0.9706 Acc: 0.5790 \n",
      "Epoch 28/29\n",
      "----------\n",
      "train Loss: 0.9579 Acc: 0.5807 \n",
      "val Loss: 0.9705 Acc: 0.5790 \n",
      "Epoch 29/29\n",
      "----------\n",
      "train Loss: 0.9578 Acc: 0.5807 \n",
      "val Loss: 0.9705 Acc: 0.5790 \n",
      "Training complete in 0m 18s\n",
      "Best val Acc: 0.579500\n",
      "Epoch generations ( 71 /200) :accuracy(fittest mask) tensor(0.5800, dtype=torch.float64) keep_prob 1.0\n",
      "Epoch generations ( 72 /200) :accuracy(fittest mask) tensor(0.5800, dtype=torch.float64) keep_prob 1.0\n",
      "Epoch generations ( 73 /200) :accuracy(fittest mask) tensor(0.5800, dtype=torch.float64) keep_prob 1.0\n",
      "Epoch generations ( 74 /200) :accuracy(fittest mask) tensor(0.5800, dtype=torch.float64) keep_prob 1.0\n",
      "Epoch generations ( 75 /200) :accuracy(fittest mask) tensor(0.5800, dtype=torch.float64) keep_prob 1.0\n",
      "Epoch generations ( 76 /200) :accuracy(fittest mask) tensor(0.5800, dtype=torch.float64) keep_prob 1.0\n",
      "Epoch generations ( 77 /200) :accuracy(fittest mask) tensor(0.5800, dtype=torch.float64) keep_prob 1.0\n",
      "Epoch generations ( 78 /200) :accuracy(fittest mask) tensor(0.5800, dtype=torch.float64) keep_prob 1.0\n",
      "Epoch generations ( 79 /200) :accuracy(fittest mask) tensor(0.5800, dtype=torch.float64) keep_prob 1.0\n",
      "Epoch generations ( 80 /200) :accuracy(fittest mask) tensor(0.5800, dtype=torch.float64) keep_prob 1.0\n",
      "Epoch 0/29\n",
      "----------\n",
      "train Loss: 0.9585 Acc: 0.5800 \n",
      "val Loss: 0.9708 Acc: 0.5795 \n",
      "Epoch 1/29\n",
      "----------\n",
      "train Loss: 0.9583 Acc: 0.5800 \n",
      "val Loss: 0.9707 Acc: 0.5795 \n",
      "Epoch 2/29\n",
      "----------\n",
      "train Loss: 0.9582 Acc: 0.5803 \n",
      "val Loss: 0.9707 Acc: 0.5795 \n",
      "Epoch 3/29\n",
      "----------\n",
      "train Loss: 0.9581 Acc: 0.5807 \n",
      "val Loss: 0.9706 Acc: 0.5795 \n",
      "Epoch 4/29\n",
      "----------\n",
      "train Loss: 0.9580 Acc: 0.5803 \n",
      "val Loss: 0.9706 Acc: 0.5790 \n",
      "Epoch 5/29\n",
      "----------\n",
      "train Loss: 0.9579 Acc: 0.5803 \n",
      "val Loss: 0.9705 Acc: 0.5790 \n",
      "Epoch 6/29\n",
      "----------\n",
      "train Loss: 0.9578 Acc: 0.5807 \n",
      "val Loss: 0.9705 Acc: 0.5795 \n",
      "Epoch 7/29\n",
      "----------\n",
      "train Loss: 0.9576 Acc: 0.5807 \n",
      "val Loss: 0.9704 Acc: 0.5800 \n",
      "Epoch 8/29\n",
      "----------\n",
      "train Loss: 0.9575 Acc: 0.5813 \n",
      "val Loss: 0.9704 Acc: 0.5800 \n",
      "Epoch 9/29\n",
      "----------\n",
      "train Loss: 0.9574 Acc: 0.5813 \n",
      "val Loss: 0.9703 Acc: 0.5800 \n",
      "Epoch 10/29\n",
      "----------\n",
      "train Loss: 0.9573 Acc: 0.5813 \n",
      "val Loss: 0.9703 Acc: 0.5800 \n",
      "Epoch 11/29\n",
      "----------\n",
      "train Loss: 0.9572 Acc: 0.5817 \n",
      "val Loss: 0.9702 Acc: 0.5800 \n",
      "Epoch 12/29\n",
      "----------\n",
      "train Loss: 0.9571 Acc: 0.5817 \n",
      "val Loss: 0.9702 Acc: 0.5800 \n",
      "Epoch 13/29\n",
      "----------\n",
      "train Loss: 0.9570 Acc: 0.5817 \n",
      "val Loss: 0.9702 Acc: 0.5805 \n",
      "Epoch 14/29\n",
      "----------\n",
      "train Loss: 0.9569 Acc: 0.5820 \n",
      "val Loss: 0.9701 Acc: 0.5800 \n",
      "Epoch 15/29\n",
      "----------\n",
      "train Loss: 0.9568 Acc: 0.5820 \n",
      "val Loss: 0.9701 Acc: 0.5795 \n",
      "Epoch 16/29\n",
      "----------\n",
      "train Loss: 0.9567 Acc: 0.5820 \n",
      "val Loss: 0.9700 Acc: 0.5795 \n",
      "Epoch 17/29\n",
      "----------\n",
      "train Loss: 0.9566 Acc: 0.5817 \n",
      "val Loss: 0.9700 Acc: 0.5800 \n",
      "Epoch 18/29\n",
      "----------\n",
      "train Loss: 0.9565 Acc: 0.5817 \n",
      "val Loss: 0.9700 Acc: 0.5795 \n",
      "Epoch 19/29\n",
      "----------\n",
      "train Loss: 0.9564 Acc: 0.5820 \n",
      "val Loss: 0.9699 Acc: 0.5795 \n",
      "Epoch 20/29\n",
      "----------\n",
      "train Loss: 0.9563 Acc: 0.5820 \n",
      "val Loss: 0.9699 Acc: 0.5795 \n",
      "Epoch 21/29\n",
      "----------\n",
      "train Loss: 0.9562 Acc: 0.5823 \n",
      "val Loss: 0.9698 Acc: 0.5785 \n",
      "Epoch 22/29\n",
      "----------\n",
      "train Loss: 0.9561 Acc: 0.5823 \n",
      "val Loss: 0.9698 Acc: 0.5790 \n",
      "Epoch 23/29\n",
      "----------\n",
      "train Loss: 0.9560 Acc: 0.5823 \n",
      "val Loss: 0.9698 Acc: 0.5790 \n",
      "Epoch 24/29\n",
      "----------\n",
      "train Loss: 0.9559 Acc: 0.5823 \n",
      "val Loss: 0.9697 Acc: 0.5790 \n",
      "Epoch 25/29\n",
      "----------\n",
      "train Loss: 0.9558 Acc: 0.5823 \n",
      "val Loss: 0.9697 Acc: 0.5790 \n",
      "Epoch 26/29\n",
      "----------\n",
      "train Loss: 0.9557 Acc: 0.5823 \n",
      "val Loss: 0.9697 Acc: 0.5790 \n",
      "Epoch 27/29\n",
      "----------\n",
      "train Loss: 0.9556 Acc: 0.5823 \n",
      "val Loss: 0.9696 Acc: 0.5785 \n",
      "Epoch 28/29\n",
      "----------\n",
      "train Loss: 0.9555 Acc: 0.5820 \n",
      "val Loss: 0.9696 Acc: 0.5785 \n",
      "Epoch 29/29\n",
      "----------\n",
      "train Loss: 0.9554 Acc: 0.5820 \n",
      "val Loss: 0.9696 Acc: 0.5785 \n",
      "Training complete in 0m 19s\n",
      "Best val Acc: 0.580500\n",
      "Epoch generations ( 81 /200) :accuracy(fittest mask) tensor(0.5820, dtype=torch.float64) keep_prob 1.0\n",
      "Epoch generations ( 82 /200) :accuracy(fittest mask) tensor(0.5820, dtype=torch.float64) keep_prob 1.0\n",
      "Epoch generations ( 83 /200) :accuracy(fittest mask) tensor(0.5820, dtype=torch.float64) keep_prob 1.0\n",
      "Epoch generations ( 84 /200) :accuracy(fittest mask) tensor(0.5820, dtype=torch.float64) keep_prob 1.0\n",
      "Epoch generations ( 85 /200) :accuracy(fittest mask) tensor(0.5820, dtype=torch.float64) keep_prob 1.0\n",
      "Epoch generations ( 86 /200) :accuracy(fittest mask) tensor(0.5820, dtype=torch.float64) keep_prob 1.0\n",
      "Epoch generations ( 87 /200) :accuracy(fittest mask) tensor(0.5820, dtype=torch.float64) keep_prob 1.0\n",
      "Epoch generations ( 88 /200) :accuracy(fittest mask) tensor(0.5820, dtype=torch.float64) keep_prob 1.0\n",
      "Epoch generations ( 89 /200) :accuracy(fittest mask) tensor(0.5820, dtype=torch.float64) keep_prob 1.0\n",
      "Epoch generations ( 90 /200) :accuracy(fittest mask) tensor(0.5820, dtype=torch.float64) keep_prob 1.0\n",
      "Epoch 0/29\n",
      "----------\n",
      "train Loss: 0.9569 Acc: 0.5820 \n",
      "val Loss: 0.9701 Acc: 0.5800 \n",
      "Epoch 1/29\n",
      "----------\n",
      "train Loss: 0.9568 Acc: 0.5820 \n",
      "val Loss: 0.9701 Acc: 0.5795 \n",
      "Epoch 2/29\n",
      "----------\n",
      "train Loss: 0.9567 Acc: 0.5817 \n",
      "val Loss: 0.9700 Acc: 0.5795 \n",
      "Epoch 3/29\n",
      "----------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train Loss: 0.9566 Acc: 0.5817 \n",
      "val Loss: 0.9700 Acc: 0.5800 \n",
      "Epoch 4/29\n",
      "----------\n",
      "train Loss: 0.9565 Acc: 0.5817 \n",
      "val Loss: 0.9700 Acc: 0.5795 \n",
      "Epoch 5/29\n",
      "----------\n",
      "train Loss: 0.9564 Acc: 0.5820 \n",
      "val Loss: 0.9699 Acc: 0.5795 \n",
      "Epoch 6/29\n",
      "----------\n",
      "train Loss: 0.9563 Acc: 0.5820 \n",
      "val Loss: 0.9699 Acc: 0.5795 \n",
      "Epoch 7/29\n",
      "----------\n",
      "train Loss: 0.9562 Acc: 0.5823 \n",
      "val Loss: 0.9698 Acc: 0.5785 \n",
      "Epoch 8/29\n",
      "----------\n",
      "train Loss: 0.9561 Acc: 0.5823 \n",
      "val Loss: 0.9698 Acc: 0.5790 \n",
      "Epoch 9/29\n",
      "----------\n",
      "train Loss: 0.9560 Acc: 0.5823 \n",
      "val Loss: 0.9698 Acc: 0.5790 \n",
      "Epoch 10/29\n",
      "----------\n",
      "train Loss: 0.9559 Acc: 0.5823 \n",
      "val Loss: 0.9697 Acc: 0.5790 \n",
      "Epoch 11/29\n",
      "----------\n",
      "train Loss: 0.9558 Acc: 0.5823 \n",
      "val Loss: 0.9697 Acc: 0.5790 \n",
      "Epoch 12/29\n",
      "----------\n",
      "train Loss: 0.9557 Acc: 0.5823 \n",
      "val Loss: 0.9697 Acc: 0.5790 \n",
      "Epoch 13/29\n",
      "----------\n",
      "train Loss: 0.9556 Acc: 0.5820 \n",
      "val Loss: 0.9697 Acc: 0.5785 \n",
      "Epoch 14/29\n",
      "----------\n",
      "train Loss: 0.9555 Acc: 0.5820 \n",
      "val Loss: 0.9696 Acc: 0.5785 \n",
      "Epoch 15/29\n",
      "----------\n",
      "train Loss: 0.9554 Acc: 0.5820 \n",
      "val Loss: 0.9696 Acc: 0.5785 \n",
      "Epoch 16/29\n",
      "----------\n",
      "train Loss: 0.9553 Acc: 0.5820 \n",
      "val Loss: 0.9696 Acc: 0.5790 \n",
      "Epoch 17/29\n",
      "----------\n",
      "train Loss: 0.9553 Acc: 0.5820 \n",
      "val Loss: 0.9695 Acc: 0.5795 \n",
      "Epoch 18/29\n",
      "----------\n",
      "train Loss: 0.9552 Acc: 0.5827 \n",
      "val Loss: 0.9695 Acc: 0.5790 \n",
      "Epoch 19/29\n",
      "----------\n",
      "train Loss: 0.9551 Acc: 0.5830 \n",
      "val Loss: 0.9695 Acc: 0.5790 \n",
      "Epoch 20/29\n",
      "----------\n",
      "train Loss: 0.9550 Acc: 0.5833 \n",
      "val Loss: 0.9695 Acc: 0.5785 \n",
      "Epoch 21/29\n",
      "----------\n",
      "train Loss: 0.9549 Acc: 0.5833 \n",
      "val Loss: 0.9694 Acc: 0.5775 \n",
      "Epoch 22/29\n",
      "----------\n",
      "train Loss: 0.9548 Acc: 0.5837 \n",
      "val Loss: 0.9694 Acc: 0.5775 \n",
      "Epoch 23/29\n",
      "----------\n",
      "train Loss: 0.9547 Acc: 0.5837 \n",
      "val Loss: 0.9694 Acc: 0.5780 \n",
      "Epoch 24/29\n",
      "----------\n",
      "train Loss: 0.9546 Acc: 0.5837 \n",
      "val Loss: 0.9694 Acc: 0.5780 \n",
      "Epoch 25/29\n",
      "----------\n",
      "train Loss: 0.9546 Acc: 0.5840 \n",
      "val Loss: 0.9693 Acc: 0.5780 \n",
      "Epoch 26/29\n",
      "----------\n",
      "train Loss: 0.9545 Acc: 0.5843 \n",
      "val Loss: 0.9693 Acc: 0.5775 \n",
      "Epoch 27/29\n",
      "----------\n",
      "train Loss: 0.9544 Acc: 0.5843 \n",
      "val Loss: 0.9693 Acc: 0.5780 \n",
      "Epoch 28/29\n",
      "----------\n",
      "train Loss: 0.9543 Acc: 0.5840 \n",
      "val Loss: 0.9693 Acc: 0.5785 \n",
      "Epoch 29/29\n",
      "----------\n",
      "train Loss: 0.9542 Acc: 0.5837 \n",
      "val Loss: 0.9692 Acc: 0.5785 \n",
      "Training complete in 0m 19s\n",
      "Best val Acc: 0.580000\n",
      "Epoch generations ( 91 /200) :accuracy(fittest mask) tensor(0.5820, dtype=torch.float64) keep_prob 1.0\n",
      "Epoch generations ( 92 /200) :accuracy(fittest mask) tensor(0.5820, dtype=torch.float64) keep_prob 1.0\n",
      "Epoch generations ( 93 /200) :accuracy(fittest mask) tensor(0.5820, dtype=torch.float64) keep_prob 1.0\n",
      "Epoch generations ( 94 /200) :accuracy(fittest mask) tensor(0.5820, dtype=torch.float64) keep_prob 1.0\n",
      "Epoch generations ( 95 /200) :accuracy(fittest mask) tensor(0.5820, dtype=torch.float64) keep_prob 1.0\n",
      "Epoch generations ( 96 /200) :accuracy(fittest mask) tensor(0.5820, dtype=torch.float64) keep_prob 1.0\n",
      "Epoch generations ( 97 /200) :accuracy(fittest mask) tensor(0.5820, dtype=torch.float64) keep_prob 1.0\n",
      "Epoch generations ( 98 /200) :accuracy(fittest mask) tensor(0.5820, dtype=torch.float64) keep_prob 1.0\n",
      "Epoch generations ( 99 /200) :accuracy(fittest mask) tensor(0.5820, dtype=torch.float64) keep_prob 1.0\n",
      "Epoch generations ( 100 /200) :accuracy(fittest mask) tensor(0.5820, dtype=torch.float64) keep_prob 1.0\n",
      "Epoch 0/29\n",
      "----------\n",
      "train Loss: 0.9568 Acc: 0.5820 \n",
      "val Loss: 0.9701 Acc: 0.5795 \n",
      "Epoch 1/29\n",
      "----------\n",
      "train Loss: 0.9567 Acc: 0.5820 \n",
      "val Loss: 0.9700 Acc: 0.5795 \n",
      "Epoch 2/29\n",
      "----------\n",
      "train Loss: 0.9566 Acc: 0.5817 \n",
      "val Loss: 0.9700 Acc: 0.5800 \n",
      "Epoch 3/29\n",
      "----------\n",
      "train Loss: 0.9565 Acc: 0.5817 \n",
      "val Loss: 0.9700 Acc: 0.5795 \n",
      "Epoch 4/29\n",
      "----------\n",
      "train Loss: 0.9564 Acc: 0.5820 \n",
      "val Loss: 0.9699 Acc: 0.5795 \n",
      "Epoch 5/29\n",
      "----------\n",
      "train Loss: 0.9563 Acc: 0.5820 \n",
      "val Loss: 0.9699 Acc: 0.5795 \n",
      "Epoch 6/29\n",
      "----------\n",
      "train Loss: 0.9562 Acc: 0.5823 \n",
      "val Loss: 0.9698 Acc: 0.5785 \n",
      "Epoch 7/29\n",
      "----------\n",
      "train Loss: 0.9561 Acc: 0.5823 \n",
      "val Loss: 0.9698 Acc: 0.5790 \n",
      "Epoch 8/29\n",
      "----------\n",
      "train Loss: 0.9560 Acc: 0.5823 \n",
      "val Loss: 0.9698 Acc: 0.5790 \n",
      "Epoch 9/29\n",
      "----------\n",
      "train Loss: 0.9559 Acc: 0.5823 \n",
      "val Loss: 0.9697 Acc: 0.5790 \n",
      "Epoch 10/29\n",
      "----------\n",
      "train Loss: 0.9558 Acc: 0.5823 \n",
      "val Loss: 0.9697 Acc: 0.5790 \n",
      "Epoch 11/29\n",
      "----------\n",
      "train Loss: 0.9557 Acc: 0.5823 \n",
      "val Loss: 0.9697 Acc: 0.5790 \n",
      "Epoch 12/29\n",
      "----------\n",
      "train Loss: 0.9556 Acc: 0.5820 \n",
      "val Loss: 0.9697 Acc: 0.5785 \n",
      "Epoch 13/29\n",
      "----------\n",
      "train Loss: 0.9555 Acc: 0.5820 \n",
      "val Loss: 0.9696 Acc: 0.5785 \n",
      "Epoch 14/29\n",
      "----------\n",
      "train Loss: 0.9554 Acc: 0.5820 \n",
      "val Loss: 0.9696 Acc: 0.5785 \n",
      "Epoch 15/29\n",
      "----------\n",
      "train Loss: 0.9553 Acc: 0.5820 \n",
      "val Loss: 0.9696 Acc: 0.5790 \n",
      "Epoch 16/29\n",
      "----------\n",
      "train Loss: 0.9553 Acc: 0.5820 \n",
      "val Loss: 0.9695 Acc: 0.5795 \n",
      "Epoch 17/29\n",
      "----------\n",
      "train Loss: 0.9552 Acc: 0.5830 \n",
      "val Loss: 0.9695 Acc: 0.5790 \n",
      "Epoch 18/29\n",
      "----------\n",
      "train Loss: 0.9551 Acc: 0.5830 \n",
      "val Loss: 0.9695 Acc: 0.5790 \n",
      "Epoch 19/29\n",
      "----------\n",
      "train Loss: 0.9550 Acc: 0.5833 \n",
      "val Loss: 0.9695 Acc: 0.5785 \n",
      "Epoch 20/29\n",
      "----------\n",
      "train Loss: 0.9549 Acc: 0.5833 \n",
      "val Loss: 0.9694 Acc: 0.5775 \n",
      "Epoch 21/29\n",
      "----------\n",
      "train Loss: 0.9548 Acc: 0.5837 \n",
      "val Loss: 0.9694 Acc: 0.5775 \n",
      "Epoch 22/29\n",
      "----------\n",
      "train Loss: 0.9547 Acc: 0.5837 \n",
      "val Loss: 0.9694 Acc: 0.5780 \n",
      "Epoch 23/29\n",
      "----------\n",
      "train Loss: 0.9546 Acc: 0.5840 \n",
      "val Loss: 0.9694 Acc: 0.5780 \n",
      "Epoch 24/29\n",
      "----------\n",
      "train Loss: 0.9546 Acc: 0.5840 \n",
      "val Loss: 0.9693 Acc: 0.5780 \n",
      "Epoch 25/29\n",
      "----------\n",
      "train Loss: 0.9545 Acc: 0.5843 \n",
      "val Loss: 0.9693 Acc: 0.5775 \n",
      "Epoch 26/29\n",
      "----------\n",
      "train Loss: 0.9544 Acc: 0.5840 \n",
      "val Loss: 0.9693 Acc: 0.5780 \n",
      "Epoch 27/29\n",
      "----------\n",
      "train Loss: 0.9543 Acc: 0.5837 \n",
      "val Loss: 0.9693 Acc: 0.5785 \n",
      "Epoch 28/29\n",
      "----------\n",
      "train Loss: 0.9542 Acc: 0.5837 \n",
      "val Loss: 0.9692 Acc: 0.5785 \n",
      "Epoch 29/29\n",
      "----------\n",
      "train Loss: 0.9541 Acc: 0.5837 \n",
      "val Loss: 0.9692 Acc: 0.5785 \n",
      "Training complete in 0m 19s\n",
      "Best val Acc: 0.580000\n",
      "Epoch generations ( 101 /200) :accuracy(fittest mask) tensor(0.5823, dtype=torch.float64) keep_prob 1.0\n",
      "Epoch generations ( 102 /200) :accuracy(fittest mask) tensor(0.5823, dtype=torch.float64) keep_prob 1.0\n",
      "Epoch generations ( 103 /200) :accuracy(fittest mask) tensor(0.5823, dtype=torch.float64) keep_prob 1.0\n",
      "Epoch generations ( 104 /200) :accuracy(fittest mask) tensor(0.5823, dtype=torch.float64) keep_prob 1.0\n",
      "Epoch generations ( 105 /200) :accuracy(fittest mask) tensor(0.5823, dtype=torch.float64) keep_prob 1.0\n",
      "Epoch generations ( 106 /200) :accuracy(fittest mask) tensor(0.5823, dtype=torch.float64) keep_prob 1.0\n",
      "Epoch generations ( 107 /200) :accuracy(fittest mask) tensor(0.5823, dtype=torch.float64) keep_prob 1.0\n",
      "Epoch generations ( 108 /200) :accuracy(fittest mask) tensor(0.5823, dtype=torch.float64) keep_prob 1.0\n",
      "Epoch generations ( 109 /200) :accuracy(fittest mask) tensor(0.5823, dtype=torch.float64) keep_prob 1.0\n",
      "Epoch generations ( 110 /200) :accuracy(fittest mask) tensor(0.5823, dtype=torch.float64) keep_prob 1.0\n",
      "Epoch 0/29\n",
      "----------\n",
      "train Loss: 0.9565 Acc: 0.5817 \n",
      "val Loss: 0.9700 Acc: 0.5795 \n",
      "Epoch 1/29\n",
      "----------\n",
      "train Loss: 0.9564 Acc: 0.5820 \n",
      "val Loss: 0.9699 Acc: 0.5795 \n",
      "Epoch 2/29\n",
      "----------\n",
      "train Loss: 0.9563 Acc: 0.5820 \n",
      "val Loss: 0.9699 Acc: 0.5790 \n",
      "Epoch 3/29\n",
      "----------\n",
      "train Loss: 0.9562 Acc: 0.5823 \n",
      "val Loss: 0.9698 Acc: 0.5785 \n",
      "Epoch 4/29\n",
      "----------\n",
      "train Loss: 0.9561 Acc: 0.5823 \n",
      "val Loss: 0.9698 Acc: 0.5790 \n",
      "Epoch 5/29\n",
      "----------\n",
      "train Loss: 0.9560 Acc: 0.5823 \n",
      "val Loss: 0.9698 Acc: 0.5790 \n",
      "Epoch 6/29\n",
      "----------\n",
      "train Loss: 0.9559 Acc: 0.5823 \n",
      "val Loss: 0.9697 Acc: 0.5790 \n",
      "Epoch 7/29\n",
      "----------\n",
      "train Loss: 0.9558 Acc: 0.5823 \n",
      "val Loss: 0.9697 Acc: 0.5790 \n",
      "Epoch 8/29\n",
      "----------\n",
      "train Loss: 0.9557 Acc: 0.5823 \n",
      "val Loss: 0.9697 Acc: 0.5790 \n",
      "Epoch 9/29\n",
      "----------\n",
      "train Loss: 0.9556 Acc: 0.5823 \n",
      "val Loss: 0.9697 Acc: 0.5785 \n",
      "Epoch 10/29\n",
      "----------\n",
      "train Loss: 0.9555 Acc: 0.5820 \n",
      "val Loss: 0.9696 Acc: 0.5785 \n",
      "Epoch 11/29\n",
      "----------\n",
      "train Loss: 0.9554 Acc: 0.5820 \n",
      "val Loss: 0.9696 Acc: 0.5790 \n",
      "Epoch 12/29\n",
      "----------\n",
      "train Loss: 0.9554 Acc: 0.5823 \n",
      "val Loss: 0.9696 Acc: 0.5790 \n",
      "Epoch 13/29\n",
      "----------\n",
      "train Loss: 0.9553 Acc: 0.5820 \n",
      "val Loss: 0.9695 Acc: 0.5795 \n",
      "Epoch 14/29\n",
      "----------\n",
      "train Loss: 0.9552 Acc: 0.5830 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val Loss: 0.9695 Acc: 0.5790 \n",
      "Epoch 15/29\n",
      "----------\n",
      "train Loss: 0.9551 Acc: 0.5830 \n",
      "val Loss: 0.9695 Acc: 0.5790 \n",
      "Epoch 16/29\n",
      "----------\n",
      "train Loss: 0.9550 Acc: 0.5833 \n",
      "val Loss: 0.9695 Acc: 0.5785 \n",
      "Epoch 17/29\n",
      "----------\n",
      "train Loss: 0.9549 Acc: 0.5833 \n",
      "val Loss: 0.9694 Acc: 0.5775 \n",
      "Epoch 18/29\n",
      "----------\n",
      "train Loss: 0.9548 Acc: 0.5837 \n",
      "val Loss: 0.9694 Acc: 0.5775 \n",
      "Epoch 19/29\n",
      "----------\n",
      "train Loss: 0.9547 Acc: 0.5837 \n",
      "val Loss: 0.9694 Acc: 0.5780 \n",
      "Epoch 20/29\n",
      "----------\n",
      "train Loss: 0.9546 Acc: 0.5840 \n",
      "val Loss: 0.9694 Acc: 0.5780 \n",
      "Epoch 21/29\n",
      "----------\n",
      "train Loss: 0.9546 Acc: 0.5840 \n",
      "val Loss: 0.9693 Acc: 0.5780 \n",
      "Epoch 22/29\n",
      "----------\n",
      "train Loss: 0.9545 Acc: 0.5843 \n",
      "val Loss: 0.9693 Acc: 0.5775 \n",
      "Epoch 23/29\n",
      "----------\n",
      "train Loss: 0.9544 Acc: 0.5837 \n",
      "val Loss: 0.9693 Acc: 0.5785 \n",
      "Epoch 24/29\n",
      "----------\n",
      "train Loss: 0.9543 Acc: 0.5833 \n",
      "val Loss: 0.9693 Acc: 0.5785 \n",
      "Epoch 25/29\n",
      "----------\n",
      "train Loss: 0.9542 Acc: 0.5837 \n",
      "val Loss: 0.9692 Acc: 0.5785 \n",
      "Epoch 26/29\n",
      "----------\n",
      "train Loss: 0.9541 Acc: 0.5837 \n",
      "val Loss: 0.9692 Acc: 0.5785 \n",
      "Epoch 27/29\n",
      "----------\n",
      "train Loss: 0.9541 Acc: 0.5847 \n",
      "val Loss: 0.9692 Acc: 0.5785 \n",
      "Epoch 28/29\n",
      "----------\n",
      "train Loss: 0.9540 Acc: 0.5850 \n",
      "val Loss: 0.9692 Acc: 0.5785 \n",
      "Epoch 29/29\n",
      "----------\n",
      "train Loss: 0.9539 Acc: 0.5857 \n",
      "val Loss: 0.9692 Acc: 0.5785 \n",
      "Training complete in 0m 19s\n",
      "Best val Acc: 0.579500\n",
      "Epoch generations ( 111 /200) :accuracy(fittest mask) tensor(0.5827, dtype=torch.float64) keep_prob 1.0\n",
      "Epoch generations ( 112 /200) :accuracy(fittest mask) tensor(0.5827, dtype=torch.float64) keep_prob 1.0\n",
      "Epoch generations ( 113 /200) :accuracy(fittest mask) tensor(0.5827, dtype=torch.float64) keep_prob 1.0\n",
      "Epoch generations ( 114 /200) :accuracy(fittest mask) tensor(0.5827, dtype=torch.float64) keep_prob 1.0\n",
      "Epoch generations ( 115 /200) :accuracy(fittest mask) tensor(0.5827, dtype=torch.float64) keep_prob 1.0\n",
      "Epoch generations ( 116 /200) :accuracy(fittest mask) tensor(0.5827, dtype=torch.float64) keep_prob 1.0\n",
      "Epoch generations ( 117 /200) :accuracy(fittest mask) tensor(0.5827, dtype=torch.float64) keep_prob 1.0\n",
      "Epoch generations ( 118 /200) :accuracy(fittest mask) tensor(0.5827, dtype=torch.float64) keep_prob 1.0\n",
      "Epoch generations ( 119 /200) :accuracy(fittest mask) tensor(0.5827, dtype=torch.float64) keep_prob 1.0\n",
      "Epoch generations ( 120 /200) :accuracy(fittest mask) tensor(0.5827, dtype=torch.float64) keep_prob 1.0\n",
      "Epoch 0/29\n",
      "----------\n",
      "train Loss: 0.9564 Acc: 0.5820 \n",
      "val Loss: 0.9699 Acc: 0.5795 \n",
      "Epoch 1/29\n",
      "----------\n",
      "train Loss: 0.9563 Acc: 0.5820 \n",
      "val Loss: 0.9699 Acc: 0.5790 \n",
      "Epoch 2/29\n",
      "----------\n",
      "train Loss: 0.9562 Acc: 0.5823 \n",
      "val Loss: 0.9698 Acc: 0.5785 \n",
      "Epoch 3/29\n",
      "----------\n",
      "train Loss: 0.9561 Acc: 0.5823 \n",
      "val Loss: 0.9698 Acc: 0.5790 \n",
      "Epoch 4/29\n",
      "----------\n",
      "train Loss: 0.9560 Acc: 0.5823 \n",
      "val Loss: 0.9698 Acc: 0.5790 \n",
      "Epoch 5/29\n",
      "----------\n",
      "train Loss: 0.9559 Acc: 0.5823 \n",
      "val Loss: 0.9697 Acc: 0.5790 \n",
      "Epoch 6/29\n",
      "----------\n",
      "train Loss: 0.9558 Acc: 0.5823 \n",
      "val Loss: 0.9697 Acc: 0.5790 \n",
      "Epoch 7/29\n",
      "----------\n",
      "train Loss: 0.9557 Acc: 0.5823 \n",
      "val Loss: 0.9697 Acc: 0.5790 \n",
      "Epoch 8/29\n",
      "----------\n",
      "train Loss: 0.9556 Acc: 0.5823 \n",
      "val Loss: 0.9697 Acc: 0.5785 \n",
      "Epoch 9/29\n",
      "----------\n",
      "train Loss: 0.9555 Acc: 0.5820 \n",
      "val Loss: 0.9696 Acc: 0.5785 \n",
      "Epoch 10/29\n",
      "----------\n",
      "train Loss: 0.9554 Acc: 0.5820 \n",
      "val Loss: 0.9696 Acc: 0.5790 \n",
      "Epoch 11/29\n",
      "----------\n",
      "train Loss: 0.9554 Acc: 0.5823 \n",
      "val Loss: 0.9696 Acc: 0.5790 \n",
      "Epoch 12/29\n",
      "----------\n",
      "train Loss: 0.9553 Acc: 0.5820 \n",
      "val Loss: 0.9695 Acc: 0.5795 \n",
      "Epoch 13/29\n",
      "----------\n",
      "train Loss: 0.9552 Acc: 0.5823 \n",
      "val Loss: 0.9695 Acc: 0.5790 \n",
      "Epoch 14/29\n",
      "----------\n",
      "train Loss: 0.9551 Acc: 0.5830 \n",
      "val Loss: 0.9695 Acc: 0.5790 \n",
      "Epoch 15/29\n",
      "----------\n",
      "train Loss: 0.9550 Acc: 0.5833 \n",
      "val Loss: 0.9695 Acc: 0.5785 \n",
      "Epoch 16/29\n",
      "----------\n",
      "train Loss: 0.9549 Acc: 0.5833 \n",
      "val Loss: 0.9694 Acc: 0.5775 \n",
      "Epoch 17/29\n",
      "----------\n",
      "train Loss: 0.9548 Acc: 0.5837 \n",
      "val Loss: 0.9694 Acc: 0.5775 \n",
      "Epoch 18/29\n",
      "----------\n",
      "train Loss: 0.9547 Acc: 0.5837 \n",
      "val Loss: 0.9694 Acc: 0.5780 \n",
      "Epoch 19/29\n",
      "----------\n",
      "train Loss: 0.9546 Acc: 0.5840 \n",
      "val Loss: 0.9694 Acc: 0.5780 \n",
      "Epoch 20/29\n",
      "----------\n",
      "train Loss: 0.9546 Acc: 0.5840 \n",
      "val Loss: 0.9693 Acc: 0.5780 \n",
      "Epoch 21/29\n",
      "----------\n",
      "train Loss: 0.9545 Acc: 0.5843 \n",
      "val Loss: 0.9693 Acc: 0.5775 \n",
      "Epoch 22/29\n",
      "----------\n",
      "train Loss: 0.9544 Acc: 0.5840 \n",
      "val Loss: 0.9693 Acc: 0.5785 \n",
      "Epoch 23/29\n",
      "----------\n",
      "train Loss: 0.9543 Acc: 0.5837 \n",
      "val Loss: 0.9693 Acc: 0.5785 \n",
      "Epoch 24/29\n",
      "----------\n",
      "train Loss: 0.9542 Acc: 0.5837 \n",
      "val Loss: 0.9692 Acc: 0.5785 \n",
      "Epoch 25/29\n",
      "----------\n",
      "train Loss: 0.9541 Acc: 0.5837 \n",
      "val Loss: 0.9692 Acc: 0.5785 \n",
      "Epoch 26/29\n",
      "----------\n",
      "train Loss: 0.9541 Acc: 0.5847 \n",
      "val Loss: 0.9692 Acc: 0.5785 \n",
      "Epoch 27/29\n",
      "----------\n",
      "train Loss: 0.9540 Acc: 0.5853 \n",
      "val Loss: 0.9692 Acc: 0.5785 \n",
      "Epoch 28/29\n",
      "----------\n",
      "train Loss: 0.9539 Acc: 0.5857 \n",
      "val Loss: 0.9692 Acc: 0.5785 \n",
      "Epoch 29/29\n",
      "----------\n",
      "train Loss: 0.9538 Acc: 0.5857 \n",
      "val Loss: 0.9691 Acc: 0.5785 \n",
      "Training complete in 0m 21s\n",
      "Best val Acc: 0.579500\n",
      "Epoch generations ( 121 /200) :accuracy(fittest mask) tensor(0.5827, dtype=torch.float64) keep_prob 1.0\n",
      "Epoch generations ( 122 /200) :accuracy(fittest mask) tensor(0.5827, dtype=torch.float64) keep_prob 1.0\n",
      "Epoch generations ( 123 /200) :accuracy(fittest mask) tensor(0.5827, dtype=torch.float64) keep_prob 1.0\n",
      "Epoch generations ( 124 /200) :accuracy(fittest mask) tensor(0.5827, dtype=torch.float64) keep_prob 1.0\n",
      "Epoch generations ( 125 /200) :accuracy(fittest mask) tensor(0.5827, dtype=torch.float64) keep_prob 1.0\n",
      "Epoch generations ( 126 /200) :accuracy(fittest mask) tensor(0.5827, dtype=torch.float64) keep_prob 1.0\n",
      "Epoch generations ( 127 /200) :accuracy(fittest mask) tensor(0.5827, dtype=torch.float64) keep_prob 1.0\n",
      "Epoch generations ( 128 /200) :accuracy(fittest mask) tensor(0.5827, dtype=torch.float64) keep_prob 1.0\n",
      "Epoch generations ( 129 /200) :accuracy(fittest mask) tensor(0.5827, dtype=torch.float64) keep_prob 1.0\n",
      "Epoch generations ( 130 /200) :accuracy(fittest mask) tensor(0.5827, dtype=torch.float64) keep_prob 1.0\n",
      "Epoch 0/29\n",
      "----------\n",
      "train Loss: 0.9563 Acc: 0.5820 \n",
      "val Loss: 0.9699 Acc: 0.5790 \n",
      "Epoch 1/29\n",
      "----------\n",
      "train Loss: 0.9562 Acc: 0.5823 \n",
      "val Loss: 0.9698 Acc: 0.5785 \n",
      "Epoch 2/29\n",
      "----------\n",
      "train Loss: 0.9561 Acc: 0.5823 \n",
      "val Loss: 0.9698 Acc: 0.5790 \n",
      "Epoch 3/29\n",
      "----------\n",
      "train Loss: 0.9560 Acc: 0.5823 \n",
      "val Loss: 0.9698 Acc: 0.5790 \n",
      "Epoch 4/29\n",
      "----------\n",
      "train Loss: 0.9559 Acc: 0.5823 \n",
      "val Loss: 0.9697 Acc: 0.5790 \n",
      "Epoch 5/29\n",
      "----------\n",
      "train Loss: 0.9558 Acc: 0.5823 \n",
      "val Loss: 0.9697 Acc: 0.5790 \n",
      "Epoch 6/29\n",
      "----------\n",
      "train Loss: 0.9557 Acc: 0.5823 \n",
      "val Loss: 0.9697 Acc: 0.5790 \n",
      "Epoch 7/29\n",
      "----------\n",
      "train Loss: 0.9556 Acc: 0.5823 \n",
      "val Loss: 0.9697 Acc: 0.5785 \n",
      "Epoch 8/29\n",
      "----------\n",
      "train Loss: 0.9555 Acc: 0.5820 \n",
      "val Loss: 0.9696 Acc: 0.5785 \n",
      "Epoch 9/29\n",
      "----------\n",
      "train Loss: 0.9554 Acc: 0.5820 \n",
      "val Loss: 0.9696 Acc: 0.5790 \n",
      "Epoch 10/29\n",
      "----------\n",
      "train Loss: 0.9554 Acc: 0.5823 \n",
      "val Loss: 0.9696 Acc: 0.5790 \n",
      "Epoch 11/29\n",
      "----------\n",
      "train Loss: 0.9553 Acc: 0.5820 \n",
      "val Loss: 0.9695 Acc: 0.5795 \n",
      "Epoch 12/29\n",
      "----------\n",
      "train Loss: 0.9552 Acc: 0.5827 \n",
      "val Loss: 0.9695 Acc: 0.5790 \n",
      "Epoch 13/29\n",
      "----------\n",
      "train Loss: 0.9551 Acc: 0.5830 \n",
      "val Loss: 0.9695 Acc: 0.5790 \n",
      "Epoch 14/29\n",
      "----------\n",
      "train Loss: 0.9550 Acc: 0.5833 \n",
      "val Loss: 0.9695 Acc: 0.5785 \n",
      "Epoch 15/29\n",
      "----------\n",
      "train Loss: 0.9549 Acc: 0.5833 \n",
      "val Loss: 0.9694 Acc: 0.5775 \n",
      "Epoch 16/29\n",
      "----------\n",
      "train Loss: 0.9548 Acc: 0.5837 \n",
      "val Loss: 0.9694 Acc: 0.5775 \n",
      "Epoch 17/29\n",
      "----------\n",
      "train Loss: 0.9547 Acc: 0.5837 \n",
      "val Loss: 0.9694 Acc: 0.5780 \n",
      "Epoch 18/29\n",
      "----------\n",
      "train Loss: 0.9546 Acc: 0.5840 \n",
      "val Loss: 0.9694 Acc: 0.5780 \n",
      "Epoch 19/29\n",
      "----------\n",
      "train Loss: 0.9546 Acc: 0.5840 \n",
      "val Loss: 0.9693 Acc: 0.5780 \n",
      "Epoch 20/29\n",
      "----------\n",
      "train Loss: 0.9545 Acc: 0.5843 \n",
      "val Loss: 0.9693 Acc: 0.5775 \n",
      "Epoch 21/29\n",
      "----------\n",
      "train Loss: 0.9544 Acc: 0.5833 \n",
      "val Loss: 0.9693 Acc: 0.5785 \n",
      "Epoch 22/29\n",
      "----------\n",
      "train Loss: 0.9543 Acc: 0.5833 \n",
      "val Loss: 0.9693 Acc: 0.5785 \n",
      "Epoch 23/29\n",
      "----------\n",
      "train Loss: 0.9542 Acc: 0.5837 \n",
      "val Loss: 0.9692 Acc: 0.5785 \n",
      "Epoch 24/29\n",
      "----------\n",
      "train Loss: 0.9541 Acc: 0.5837 \n",
      "val Loss: 0.9692 Acc: 0.5785 \n",
      "Epoch 25/29\n",
      "----------\n",
      "train Loss: 0.9541 Acc: 0.5847 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val Loss: 0.9692 Acc: 0.5785 \n",
      "Epoch 26/29\n",
      "----------\n",
      "train Loss: 0.9540 Acc: 0.5853 \n",
      "val Loss: 0.9692 Acc: 0.5785 \n",
      "Epoch 27/29\n",
      "----------\n",
      "train Loss: 0.9539 Acc: 0.5857 \n",
      "val Loss: 0.9692 Acc: 0.5785 \n",
      "Epoch 28/29\n",
      "----------\n",
      "train Loss: 0.9538 Acc: 0.5857 \n",
      "val Loss: 0.9691 Acc: 0.5785 \n",
      "Epoch 29/29\n",
      "----------\n",
      "train Loss: 0.9537 Acc: 0.5853 \n",
      "val Loss: 0.9691 Acc: 0.5775 \n",
      "Training complete in 0m 19s\n",
      "Best val Acc: 0.579500\n",
      "Epoch generations ( 131 /200) :accuracy(fittest mask) tensor(0.5827, dtype=torch.float64) keep_prob 1.0\n",
      "Epoch generations ( 132 /200) :accuracy(fittest mask) tensor(0.5827, dtype=torch.float64) keep_prob 1.0\n",
      "Epoch generations ( 133 /200) :accuracy(fittest mask) tensor(0.5827, dtype=torch.float64) keep_prob 1.0\n",
      "Epoch generations ( 134 /200) :accuracy(fittest mask) tensor(0.5827, dtype=torch.float64) keep_prob 1.0\n",
      "Epoch generations ( 135 /200) :accuracy(fittest mask) tensor(0.5827, dtype=torch.float64) keep_prob 1.0\n",
      "Epoch generations ( 136 /200) :accuracy(fittest mask) tensor(0.5827, dtype=torch.float64) keep_prob 1.0\n",
      "Epoch generations ( 137 /200) :accuracy(fittest mask) tensor(0.5827, dtype=torch.float64) keep_prob 1.0\n",
      "Epoch generations ( 138 /200) :accuracy(fittest mask) tensor(0.5827, dtype=torch.float64) keep_prob 1.0\n",
      "Epoch generations ( 139 /200) :accuracy(fittest mask) tensor(0.5827, dtype=torch.float64) keep_prob 1.0\n",
      "Epoch generations ( 140 /200) :accuracy(fittest mask) tensor(0.5827, dtype=torch.float64) keep_prob 1.0\n",
      "Epoch 0/29\n",
      "----------\n",
      "train Loss: 0.9552 Acc: 0.5823 \n",
      "val Loss: 0.9695 Acc: 0.5790 \n",
      "Epoch 1/29\n",
      "----------\n",
      "train Loss: 0.9551 Acc: 0.5830 \n",
      "val Loss: 0.9695 Acc: 0.5790 \n",
      "Epoch 2/29\n",
      "----------\n",
      "train Loss: 0.9550 Acc: 0.5833 \n",
      "val Loss: 0.9695 Acc: 0.5785 \n",
      "Epoch 3/29\n",
      "----------\n",
      "train Loss: 0.9549 Acc: 0.5833 \n",
      "val Loss: 0.9694 Acc: 0.5775 \n",
      "Epoch 4/29\n",
      "----------\n",
      "train Loss: 0.9548 Acc: 0.5837 \n",
      "val Loss: 0.9694 Acc: 0.5775 \n",
      "Epoch 5/29\n",
      "----------\n",
      "train Loss: 0.9547 Acc: 0.5837 \n",
      "val Loss: 0.9694 Acc: 0.5775 \n",
      "Epoch 6/29\n",
      "----------\n",
      "train Loss: 0.9546 Acc: 0.5840 \n",
      "val Loss: 0.9694 Acc: 0.5780 \n",
      "Epoch 7/29\n",
      "----------\n",
      "train Loss: 0.9546 Acc: 0.5840 \n",
      "val Loss: 0.9693 Acc: 0.5780 \n",
      "Epoch 8/29\n",
      "----------\n",
      "train Loss: 0.9545 Acc: 0.5843 \n",
      "val Loss: 0.9693 Acc: 0.5775 \n",
      "Epoch 9/29\n",
      "----------\n",
      "train Loss: 0.9544 Acc: 0.5833 \n",
      "val Loss: 0.9693 Acc: 0.5780 \n",
      "Epoch 10/29\n",
      "----------\n",
      "train Loss: 0.9543 Acc: 0.5833 \n",
      "val Loss: 0.9693 Acc: 0.5785 \n",
      "Epoch 11/29\n",
      "----------\n",
      "train Loss: 0.9542 Acc: 0.5837 \n",
      "val Loss: 0.9692 Acc: 0.5785 \n",
      "Epoch 12/29\n",
      "----------\n",
      "train Loss: 0.9541 Acc: 0.5837 \n",
      "val Loss: 0.9692 Acc: 0.5785 \n",
      "Epoch 13/29\n",
      "----------\n",
      "train Loss: 0.9541 Acc: 0.5847 \n",
      "val Loss: 0.9692 Acc: 0.5785 \n",
      "Epoch 14/29\n",
      "----------\n",
      "train Loss: 0.9540 Acc: 0.5850 \n",
      "val Loss: 0.9692 Acc: 0.5785 \n",
      "Epoch 15/29\n",
      "----------\n",
      "train Loss: 0.9539 Acc: 0.5857 \n",
      "val Loss: 0.9692 Acc: 0.5785 \n",
      "Epoch 16/29\n",
      "----------\n",
      "train Loss: 0.9538 Acc: 0.5857 \n",
      "val Loss: 0.9691 Acc: 0.5785 \n",
      "Epoch 17/29\n",
      "----------\n",
      "train Loss: 0.9537 Acc: 0.5853 \n",
      "val Loss: 0.9691 Acc: 0.5775 \n",
      "Epoch 18/29\n",
      "----------\n",
      "train Loss: 0.9537 Acc: 0.5857 \n",
      "val Loss: 0.9691 Acc: 0.5765 \n",
      "Epoch 19/29\n",
      "----------\n",
      "train Loss: 0.9536 Acc: 0.5860 \n",
      "val Loss: 0.9691 Acc: 0.5760 \n",
      "Epoch 20/29\n",
      "----------\n",
      "train Loss: 0.9535 Acc: 0.5867 \n",
      "val Loss: 0.9691 Acc: 0.5760 \n",
      "Epoch 21/29\n",
      "----------\n",
      "train Loss: 0.9534 Acc: 0.5867 \n",
      "val Loss: 0.9691 Acc: 0.5760 \n",
      "Epoch 22/29\n",
      "----------\n",
      "train Loss: 0.9534 Acc: 0.5863 \n",
      "val Loss: 0.9690 Acc: 0.5755 \n",
      "Epoch 23/29\n",
      "----------\n",
      "train Loss: 0.9533 Acc: 0.5870 \n",
      "val Loss: 0.9690 Acc: 0.5750 \n",
      "Epoch 24/29\n",
      "----------\n",
      "train Loss: 0.9532 Acc: 0.5867 \n",
      "val Loss: 0.9690 Acc: 0.5745 \n",
      "Epoch 25/29\n",
      "----------\n",
      "train Loss: 0.9531 Acc: 0.5870 \n",
      "val Loss: 0.9690 Acc: 0.5745 \n",
      "Epoch 26/29\n",
      "----------\n",
      "train Loss: 0.9531 Acc: 0.5870 \n",
      "val Loss: 0.9690 Acc: 0.5740 \n",
      "Epoch 27/29\n",
      "----------\n",
      "train Loss: 0.9530 Acc: 0.5870 \n",
      "val Loss: 0.9690 Acc: 0.5730 \n",
      "Epoch 28/29\n",
      "----------\n",
      "train Loss: 0.9529 Acc: 0.5870 \n",
      "val Loss: 0.9690 Acc: 0.5725 \n",
      "Epoch 29/29\n",
      "----------\n",
      "train Loss: 0.9528 Acc: 0.5877 \n",
      "val Loss: 0.9690 Acc: 0.5730 \n",
      "Training complete in 0m 21s\n",
      "Best val Acc: 0.579000\n",
      "Epoch generations ( 141 /200) :accuracy(fittest mask) tensor(0.5837, dtype=torch.float64) keep_prob 1.0\n",
      "Epoch generations ( 142 /200) :accuracy(fittest mask) tensor(0.5837, dtype=torch.float64) keep_prob 1.0\n",
      "Epoch generations ( 143 /200) :accuracy(fittest mask) tensor(0.5837, dtype=torch.float64) keep_prob 1.0\n",
      "Epoch generations ( 144 /200) :accuracy(fittest mask) tensor(0.5837, dtype=torch.float64) keep_prob 1.0\n",
      "Epoch generations ( 145 /200) :accuracy(fittest mask) tensor(0.5837, dtype=torch.float64) keep_prob 1.0\n",
      "Epoch generations ( 146 /200) :accuracy(fittest mask) tensor(0.5837, dtype=torch.float64) keep_prob 1.0\n",
      "Epoch generations ( 147 /200) :accuracy(fittest mask) tensor(0.5837, dtype=torch.float64) keep_prob 1.0\n",
      "Epoch generations ( 148 /200) :accuracy(fittest mask) tensor(0.5837, dtype=torch.float64) keep_prob 1.0\n",
      "Epoch generations ( 149 /200) :accuracy(fittest mask) tensor(0.5837, dtype=torch.float64) keep_prob 1.0\n",
      "Epoch generations ( 150 /200) :accuracy(fittest mask) tensor(0.5837, dtype=torch.float64) keep_prob 1.0\n",
      "Epoch 0/29\n",
      "----------\n",
      "train Loss: 0.9551 Acc: 0.5830 \n",
      "val Loss: 0.9695 Acc: 0.5790 \n",
      "Epoch 1/29\n",
      "----------\n",
      "train Loss: 0.9550 Acc: 0.5833 \n",
      "val Loss: 0.9695 Acc: 0.5785 \n",
      "Epoch 2/29\n",
      "----------\n",
      "train Loss: 0.9549 Acc: 0.5833 \n",
      "val Loss: 0.9694 Acc: 0.5775 \n",
      "Epoch 3/29\n",
      "----------\n",
      "train Loss: 0.9548 Acc: 0.5837 \n",
      "val Loss: 0.9694 Acc: 0.5775 \n",
      "Epoch 4/29\n",
      "----------\n",
      "train Loss: 0.9547 Acc: 0.5837 \n",
      "val Loss: 0.9694 Acc: 0.5775 \n",
      "Epoch 5/29\n",
      "----------\n",
      "train Loss: 0.9546 Acc: 0.5840 \n",
      "val Loss: 0.9694 Acc: 0.5780 \n",
      "Epoch 6/29\n",
      "----------\n",
      "train Loss: 0.9546 Acc: 0.5840 \n",
      "val Loss: 0.9693 Acc: 0.5780 \n",
      "Epoch 7/29\n",
      "----------\n",
      "train Loss: 0.9545 Acc: 0.5843 \n",
      "val Loss: 0.9693 Acc: 0.5775 \n",
      "Epoch 8/29\n",
      "----------\n",
      "train Loss: 0.9544 Acc: 0.5837 \n",
      "val Loss: 0.9693 Acc: 0.5785 \n",
      "Epoch 9/29\n",
      "----------\n",
      "train Loss: 0.9543 Acc: 0.5833 \n",
      "val Loss: 0.9693 Acc: 0.5785 \n",
      "Epoch 10/29\n",
      "----------\n",
      "train Loss: 0.9542 Acc: 0.5833 \n",
      "val Loss: 0.9692 Acc: 0.5785 \n",
      "Epoch 11/29\n",
      "----------\n",
      "train Loss: 0.9541 Acc: 0.5837 \n",
      "val Loss: 0.9692 Acc: 0.5785 \n",
      "Epoch 12/29\n",
      "----------\n",
      "train Loss: 0.9541 Acc: 0.5847 \n",
      "val Loss: 0.9692 Acc: 0.5785 \n",
      "Epoch 13/29\n",
      "----------\n",
      "train Loss: 0.9540 Acc: 0.5850 \n",
      "val Loss: 0.9692 Acc: 0.5785 \n",
      "Epoch 14/29\n",
      "----------\n",
      "train Loss: 0.9539 Acc: 0.5857 \n",
      "val Loss: 0.9692 Acc: 0.5785 \n",
      "Epoch 15/29\n",
      "----------\n",
      "train Loss: 0.9538 Acc: 0.5857 \n",
      "val Loss: 0.9691 Acc: 0.5785 \n",
      "Epoch 16/29\n",
      "----------\n",
      "train Loss: 0.9537 Acc: 0.5853 \n",
      "val Loss: 0.9691 Acc: 0.5775 \n",
      "Epoch 17/29\n",
      "----------\n",
      "train Loss: 0.9537 Acc: 0.5857 \n",
      "val Loss: 0.9691 Acc: 0.5765 \n",
      "Epoch 18/29\n",
      "----------\n",
      "train Loss: 0.9536 Acc: 0.5860 \n",
      "val Loss: 0.9691 Acc: 0.5760 \n",
      "Epoch 19/29\n",
      "----------\n",
      "train Loss: 0.9535 Acc: 0.5867 \n",
      "val Loss: 0.9691 Acc: 0.5760 \n",
      "Epoch 20/29\n",
      "----------\n",
      "train Loss: 0.9534 Acc: 0.5867 \n",
      "val Loss: 0.9691 Acc: 0.5760 \n",
      "Epoch 21/29\n",
      "----------\n",
      "train Loss: 0.9534 Acc: 0.5867 \n",
      "val Loss: 0.9690 Acc: 0.5755 \n",
      "Epoch 22/29\n",
      "----------\n",
      "train Loss: 0.9533 Acc: 0.5867 \n",
      "val Loss: 0.9690 Acc: 0.5750 \n",
      "Epoch 23/29\n",
      "----------\n",
      "train Loss: 0.9532 Acc: 0.5867 \n",
      "val Loss: 0.9690 Acc: 0.5745 \n",
      "Epoch 24/29\n",
      "----------\n",
      "train Loss: 0.9531 Acc: 0.5870 \n",
      "val Loss: 0.9690 Acc: 0.5745 \n",
      "Epoch 25/29\n",
      "----------\n",
      "train Loss: 0.9531 Acc: 0.5870 \n",
      "val Loss: 0.9690 Acc: 0.5740 \n",
      "Epoch 26/29\n",
      "----------\n",
      "train Loss: 0.9530 Acc: 0.5870 \n",
      "val Loss: 0.9690 Acc: 0.5730 \n",
      "Epoch 27/29\n",
      "----------\n",
      "train Loss: 0.9529 Acc: 0.5873 \n",
      "val Loss: 0.9690 Acc: 0.5730 \n",
      "Epoch 28/29\n",
      "----------\n",
      "train Loss: 0.9528 Acc: 0.5877 \n",
      "val Loss: 0.9690 Acc: 0.5735 \n",
      "Epoch 29/29\n",
      "----------\n",
      "train Loss: 0.9528 Acc: 0.5877 \n",
      "val Loss: 0.9690 Acc: 0.5735 \n",
      "Training complete in 0m 20s\n",
      "Best val Acc: 0.579000\n",
      "Epoch generations ( 151 /200) :accuracy(fittest mask) tensor(0.5833, dtype=torch.float64) keep_prob 1.0\n",
      "Epoch generations ( 152 /200) :accuracy(fittest mask) tensor(0.5833, dtype=torch.float64) keep_prob 1.0\n",
      "Epoch generations ( 153 /200) :accuracy(fittest mask) tensor(0.5833, dtype=torch.float64) keep_prob 1.0\n",
      "Epoch generations ( 154 /200) :accuracy(fittest mask) tensor(0.5833, dtype=torch.float64) keep_prob 1.0\n",
      "Epoch generations ( 155 /200) :accuracy(fittest mask) tensor(0.5833, dtype=torch.float64) keep_prob 1.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch generations ( 156 /200) :accuracy(fittest mask) tensor(0.5833, dtype=torch.float64) keep_prob 1.0\n",
      "Epoch generations ( 157 /200) :accuracy(fittest mask) tensor(0.5833, dtype=torch.float64) keep_prob 1.0\n",
      "Epoch generations ( 158 /200) :accuracy(fittest mask) tensor(0.5833, dtype=torch.float64) keep_prob 1.0\n",
      "Epoch generations ( 159 /200) :accuracy(fittest mask) tensor(0.5833, dtype=torch.float64) keep_prob 1.0\n",
      "Epoch generations ( 160 /200) :accuracy(fittest mask) tensor(0.5833, dtype=torch.float64) keep_prob 1.0\n",
      "Epoch 0/29\n",
      "----------\n",
      "train Loss: 0.9550 Acc: 0.5833 \n",
      "val Loss: 0.9695 Acc: 0.5785 \n",
      "Epoch 1/29\n",
      "----------\n",
      "train Loss: 0.9549 Acc: 0.5833 \n",
      "val Loss: 0.9694 Acc: 0.5775 \n",
      "Epoch 2/29\n",
      "----------\n",
      "train Loss: 0.9548 Acc: 0.5837 \n",
      "val Loss: 0.9694 Acc: 0.5775 \n",
      "Epoch 3/29\n",
      "----------\n",
      "train Loss: 0.9547 Acc: 0.5837 \n",
      "val Loss: 0.9694 Acc: 0.5775 \n",
      "Epoch 4/29\n",
      "----------\n",
      "train Loss: 0.9546 Acc: 0.5840 \n",
      "val Loss: 0.9694 Acc: 0.5780 \n",
      "Epoch 5/29\n",
      "----------\n",
      "train Loss: 0.9546 Acc: 0.5840 \n",
      "val Loss: 0.9693 Acc: 0.5780 \n",
      "Epoch 6/29\n",
      "----------\n",
      "train Loss: 0.9545 Acc: 0.5843 \n",
      "val Loss: 0.9693 Acc: 0.5775 \n",
      "Epoch 7/29\n",
      "----------\n",
      "train Loss: 0.9544 Acc: 0.5837 \n",
      "val Loss: 0.9693 Acc: 0.5785 \n",
      "Epoch 8/29\n",
      "----------\n",
      "train Loss: 0.9543 Acc: 0.5833 \n",
      "val Loss: 0.9693 Acc: 0.5785 \n",
      "Epoch 9/29\n",
      "----------\n",
      "train Loss: 0.9542 Acc: 0.5833 \n",
      "val Loss: 0.9692 Acc: 0.5785 \n",
      "Epoch 10/29\n",
      "----------\n",
      "train Loss: 0.9541 Acc: 0.5837 \n",
      "val Loss: 0.9692 Acc: 0.5785 \n",
      "Epoch 11/29\n",
      "----------\n",
      "train Loss: 0.9541 Acc: 0.5847 \n",
      "val Loss: 0.9692 Acc: 0.5785 \n",
      "Epoch 12/29\n",
      "----------\n",
      "train Loss: 0.9540 Acc: 0.5847 \n",
      "val Loss: 0.9692 Acc: 0.5785 \n",
      "Epoch 13/29\n",
      "----------\n",
      "train Loss: 0.9539 Acc: 0.5857 \n",
      "val Loss: 0.9692 Acc: 0.5785 \n",
      "Epoch 14/29\n",
      "----------\n",
      "train Loss: 0.9538 Acc: 0.5857 \n",
      "val Loss: 0.9691 Acc: 0.5785 \n",
      "Epoch 15/29\n",
      "----------\n",
      "train Loss: 0.9537 Acc: 0.5857 \n",
      "val Loss: 0.9691 Acc: 0.5775 \n",
      "Epoch 16/29\n",
      "----------\n",
      "train Loss: 0.9537 Acc: 0.5853 \n",
      "val Loss: 0.9691 Acc: 0.5765 \n",
      "Epoch 17/29\n",
      "----------\n",
      "train Loss: 0.9536 Acc: 0.5857 \n",
      "val Loss: 0.9691 Acc: 0.5760 \n",
      "Epoch 18/29\n",
      "----------\n",
      "train Loss: 0.9535 Acc: 0.5867 \n",
      "val Loss: 0.9691 Acc: 0.5760 \n",
      "Epoch 19/29\n",
      "----------\n",
      "train Loss: 0.9534 Acc: 0.5867 \n",
      "val Loss: 0.9691 Acc: 0.5760 \n",
      "Epoch 20/29\n",
      "----------\n",
      "train Loss: 0.9534 Acc: 0.5863 \n",
      "val Loss: 0.9690 Acc: 0.5755 \n",
      "Epoch 21/29\n",
      "----------\n",
      "train Loss: 0.9533 Acc: 0.5867 \n",
      "val Loss: 0.9690 Acc: 0.5750 \n",
      "Epoch 22/29\n",
      "----------\n",
      "train Loss: 0.9532 Acc: 0.5867 \n",
      "val Loss: 0.9690 Acc: 0.5745 \n",
      "Epoch 23/29\n",
      "----------\n",
      "train Loss: 0.9531 Acc: 0.5870 \n",
      "val Loss: 0.9690 Acc: 0.5745 \n",
      "Epoch 24/29\n",
      "----------\n",
      "train Loss: 0.9531 Acc: 0.5870 \n",
      "val Loss: 0.9690 Acc: 0.5740 \n",
      "Epoch 25/29\n",
      "----------\n",
      "train Loss: 0.9530 Acc: 0.5870 \n",
      "val Loss: 0.9690 Acc: 0.5730 \n",
      "Epoch 26/29\n",
      "----------\n",
      "train Loss: 0.9529 Acc: 0.5873 \n",
      "val Loss: 0.9690 Acc: 0.5730 \n",
      "Epoch 27/29\n",
      "----------\n",
      "train Loss: 0.9528 Acc: 0.5877 \n",
      "val Loss: 0.9690 Acc: 0.5730 \n",
      "Epoch 28/29\n",
      "----------\n",
      "train Loss: 0.9528 Acc: 0.5877 \n",
      "val Loss: 0.9690 Acc: 0.5730 \n",
      "Epoch 29/29\n",
      "----------\n",
      "train Loss: 0.9527 Acc: 0.5870 \n",
      "val Loss: 0.9690 Acc: 0.5730 \n",
      "Training complete in 0m 20s\n",
      "Best val Acc: 0.578500\n",
      "Epoch generations ( 161 /200) :accuracy(fittest mask) tensor(0.5840, dtype=torch.float64) keep_prob 1.0\n",
      "Epoch generations ( 162 /200) :accuracy(fittest mask) tensor(0.5840, dtype=torch.float64) keep_prob 1.0\n",
      "Epoch generations ( 163 /200) :accuracy(fittest mask) tensor(0.5840, dtype=torch.float64) keep_prob 1.0\n",
      "Epoch generations ( 164 /200) :accuracy(fittest mask) tensor(0.5840, dtype=torch.float64) keep_prob 1.0\n",
      "Epoch generations ( 165 /200) :accuracy(fittest mask) tensor(0.5840, dtype=torch.float64) keep_prob 1.0\n",
      "Epoch generations ( 166 /200) :accuracy(fittest mask) tensor(0.5840, dtype=torch.float64) keep_prob 1.0\n",
      "Epoch generations ( 167 /200) :accuracy(fittest mask) tensor(0.5840, dtype=torch.float64) keep_prob 1.0\n",
      "Epoch generations ( 168 /200) :accuracy(fittest mask) tensor(0.5840, dtype=torch.float64) keep_prob 1.0\n",
      "Epoch generations ( 169 /200) :accuracy(fittest mask) tensor(0.5840, dtype=torch.float64) keep_prob 1.0\n",
      "Epoch generations ( 170 /200) :accuracy(fittest mask) tensor(0.5840, dtype=torch.float64) keep_prob 1.0\n",
      "Epoch 0/29\n",
      "----------\n",
      "train Loss: 0.9549 Acc: 0.5833 \n",
      "val Loss: 0.9694 Acc: 0.5775 \n",
      "Epoch 1/29\n",
      "----------\n",
      "train Loss: 0.9548 Acc: 0.5837 \n",
      "val Loss: 0.9694 Acc: 0.5775 \n",
      "Epoch 2/29\n",
      "----------\n",
      "train Loss: 0.9547 Acc: 0.5837 \n",
      "val Loss: 0.9694 Acc: 0.5775 \n",
      "Epoch 3/29\n",
      "----------\n",
      "train Loss: 0.9546 Acc: 0.5840 \n",
      "val Loss: 0.9694 Acc: 0.5780 \n",
      "Epoch 4/29\n",
      "----------\n",
      "train Loss: 0.9546 Acc: 0.5840 \n",
      "val Loss: 0.9693 Acc: 0.5780 \n",
      "Epoch 5/29\n",
      "----------\n",
      "train Loss: 0.9545 Acc: 0.5843 \n",
      "val Loss: 0.9693 Acc: 0.5775 \n",
      "Epoch 6/29\n",
      "----------\n",
      "train Loss: 0.9544 Acc: 0.5837 \n",
      "val Loss: 0.9693 Acc: 0.5780 \n",
      "Epoch 7/29\n",
      "----------\n",
      "train Loss: 0.9543 Acc: 0.5833 \n",
      "val Loss: 0.9693 Acc: 0.5785 \n",
      "Epoch 8/29\n",
      "----------\n",
      "train Loss: 0.9542 Acc: 0.5833 \n",
      "val Loss: 0.9692 Acc: 0.5785 \n",
      "Epoch 9/29\n",
      "----------\n",
      "train Loss: 0.9541 Acc: 0.5837 \n",
      "val Loss: 0.9692 Acc: 0.5785 \n",
      "Epoch 10/29\n",
      "----------\n",
      "train Loss: 0.9541 Acc: 0.5843 \n",
      "val Loss: 0.9692 Acc: 0.5785 \n",
      "Epoch 11/29\n",
      "----------\n",
      "train Loss: 0.9540 Acc: 0.5847 \n",
      "val Loss: 0.9692 Acc: 0.5785 \n",
      "Epoch 12/29\n",
      "----------\n",
      "train Loss: 0.9539 Acc: 0.5857 \n",
      "val Loss: 0.9692 Acc: 0.5785 \n",
      "Epoch 13/29\n",
      "----------\n",
      "train Loss: 0.9538 Acc: 0.5857 \n",
      "val Loss: 0.9691 Acc: 0.5785 \n",
      "Epoch 14/29\n",
      "----------\n",
      "train Loss: 0.9538 Acc: 0.5857 \n",
      "val Loss: 0.9691 Acc: 0.5775 \n",
      "Epoch 15/29\n",
      "----------\n",
      "train Loss: 0.9537 Acc: 0.5857 \n",
      "val Loss: 0.9691 Acc: 0.5765 \n",
      "Epoch 16/29\n",
      "----------\n",
      "train Loss: 0.9536 Acc: 0.5857 \n",
      "val Loss: 0.9691 Acc: 0.5760 \n",
      "Epoch 17/29\n",
      "----------\n",
      "train Loss: 0.9535 Acc: 0.5867 \n",
      "val Loss: 0.9691 Acc: 0.5760 \n",
      "Epoch 18/29\n",
      "----------\n",
      "train Loss: 0.9534 Acc: 0.5867 \n",
      "val Loss: 0.9691 Acc: 0.5760 \n",
      "Epoch 19/29\n",
      "----------\n",
      "train Loss: 0.9534 Acc: 0.5867 \n",
      "val Loss: 0.9690 Acc: 0.5755 \n",
      "Epoch 20/29\n",
      "----------\n",
      "train Loss: 0.9533 Acc: 0.5867 \n",
      "val Loss: 0.9690 Acc: 0.5750 \n",
      "Epoch 21/29\n",
      "----------\n",
      "train Loss: 0.9532 Acc: 0.5867 \n",
      "val Loss: 0.9690 Acc: 0.5745 \n",
      "Epoch 22/29\n",
      "----------\n",
      "train Loss: 0.9531 Acc: 0.5870 \n",
      "val Loss: 0.9690 Acc: 0.5745 \n",
      "Epoch 23/29\n",
      "----------\n",
      "train Loss: 0.9531 Acc: 0.5870 \n",
      "val Loss: 0.9690 Acc: 0.5740 \n",
      "Epoch 24/29\n",
      "----------\n",
      "train Loss: 0.9530 Acc: 0.5870 \n",
      "val Loss: 0.9690 Acc: 0.5730 \n",
      "Epoch 25/29\n",
      "----------\n",
      "train Loss: 0.9529 Acc: 0.5873 \n",
      "val Loss: 0.9690 Acc: 0.5730 \n",
      "Epoch 26/29\n",
      "----------\n",
      "train Loss: 0.9529 Acc: 0.5877 \n",
      "val Loss: 0.9690 Acc: 0.5735 \n",
      "Epoch 27/29\n",
      "----------\n",
      "train Loss: 0.9528 Acc: 0.5877 \n",
      "val Loss: 0.9690 Acc: 0.5735 \n",
      "Epoch 28/29\n",
      "----------\n",
      "train Loss: 0.9527 Acc: 0.5870 \n",
      "val Loss: 0.9690 Acc: 0.5735 \n",
      "Epoch 29/29\n",
      "----------\n",
      "train Loss: 0.9526 Acc: 0.5870 \n",
      "val Loss: 0.9689 Acc: 0.5735 \n",
      "Training complete in 0m 19s\n",
      "Best val Acc: 0.578500\n",
      "Epoch generations ( 171 /200) :accuracy(fittest mask) tensor(0.5847, dtype=torch.float64) keep_prob 1.0\n",
      "Epoch generations ( 172 /200) :accuracy(fittest mask) tensor(0.5847, dtype=torch.float64) keep_prob 1.0\n",
      "Epoch generations ( 173 /200) :accuracy(fittest mask) tensor(0.5847, dtype=torch.float64) keep_prob 1.0\n",
      "Epoch generations ( 174 /200) :accuracy(fittest mask) tensor(0.5847, dtype=torch.float64) keep_prob 1.0\n",
      "Epoch generations ( 175 /200) :accuracy(fittest mask) tensor(0.5847, dtype=torch.float64) keep_prob 1.0\n",
      "Epoch generations ( 176 /200) :accuracy(fittest mask) tensor(0.5847, dtype=torch.float64) keep_prob 1.0\n",
      "Epoch generations ( 177 /200) :accuracy(fittest mask) tensor(0.5847, dtype=torch.float64) keep_prob 1.0\n",
      "Epoch generations ( 178 /200) :accuracy(fittest mask) tensor(0.5847, dtype=torch.float64) keep_prob 1.0\n",
      "Epoch generations ( 179 /200) :accuracy(fittest mask) tensor(0.5847, dtype=torch.float64) keep_prob 1.0\n",
      "Epoch generations ( 180 /200) :accuracy(fittest mask) tensor(0.5847, dtype=torch.float64) keep_prob 1.0\n",
      "Epoch 0/29\n",
      "----------\n",
      "train Loss: 0.9542 Acc: 0.5833 \n",
      "val Loss: 0.9692 Acc: 0.5785 \n",
      "Epoch 1/29\n",
      "----------\n",
      "train Loss: 0.9541 Acc: 0.5833 \n",
      "val Loss: 0.9692 Acc: 0.5785 \n",
      "Epoch 2/29\n",
      "----------\n",
      "train Loss: 0.9541 Acc: 0.5843 \n",
      "val Loss: 0.9692 Acc: 0.5785 \n",
      "Epoch 3/29\n",
      "----------\n",
      "train Loss: 0.9540 Acc: 0.5850 \n",
      "val Loss: 0.9692 Acc: 0.5785 \n",
      "Epoch 4/29\n",
      "----------\n",
      "train Loss: 0.9539 Acc: 0.5857 \n",
      "val Loss: 0.9692 Acc: 0.5785 \n",
      "Epoch 5/29\n",
      "----------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train Loss: 0.9538 Acc: 0.5857 \n",
      "val Loss: 0.9691 Acc: 0.5785 \n",
      "Epoch 6/29\n",
      "----------\n",
      "train Loss: 0.9538 Acc: 0.5857 \n",
      "val Loss: 0.9691 Acc: 0.5775 \n",
      "Epoch 7/29\n",
      "----------\n",
      "train Loss: 0.9537 Acc: 0.5853 \n",
      "val Loss: 0.9691 Acc: 0.5765 \n",
      "Epoch 8/29\n",
      "----------\n",
      "train Loss: 0.9536 Acc: 0.5860 \n",
      "val Loss: 0.9691 Acc: 0.5760 \n",
      "Epoch 9/29\n",
      "----------\n",
      "train Loss: 0.9535 Acc: 0.5867 \n",
      "val Loss: 0.9691 Acc: 0.5760 \n",
      "Epoch 10/29\n",
      "----------\n",
      "train Loss: 0.9534 Acc: 0.5867 \n",
      "val Loss: 0.9691 Acc: 0.5760 \n",
      "Epoch 11/29\n",
      "----------\n",
      "train Loss: 0.9534 Acc: 0.5867 \n",
      "val Loss: 0.9690 Acc: 0.5755 \n",
      "Epoch 12/29\n",
      "----------\n",
      "train Loss: 0.9533 Acc: 0.5870 \n",
      "val Loss: 0.9690 Acc: 0.5750 \n",
      "Epoch 13/29\n",
      "----------\n",
      "train Loss: 0.9532 Acc: 0.5867 \n",
      "val Loss: 0.9690 Acc: 0.5745 \n",
      "Epoch 14/29\n",
      "----------\n",
      "train Loss: 0.9531 Acc: 0.5870 \n",
      "val Loss: 0.9690 Acc: 0.5745 \n",
      "Epoch 15/29\n",
      "----------\n",
      "train Loss: 0.9531 Acc: 0.5870 \n",
      "val Loss: 0.9690 Acc: 0.5740 \n",
      "Epoch 16/29\n",
      "----------\n",
      "train Loss: 0.9530 Acc: 0.5870 \n",
      "val Loss: 0.9690 Acc: 0.5730 \n",
      "Epoch 17/29\n",
      "----------\n",
      "train Loss: 0.9529 Acc: 0.5870 \n",
      "val Loss: 0.9690 Acc: 0.5730 \n",
      "Epoch 18/29\n",
      "----------\n",
      "train Loss: 0.9528 Acc: 0.5877 \n",
      "val Loss: 0.9690 Acc: 0.5735 \n",
      "Epoch 19/29\n",
      "----------\n",
      "train Loss: 0.9528 Acc: 0.5877 \n",
      "val Loss: 0.9690 Acc: 0.5735 \n",
      "Epoch 20/29\n",
      "----------\n",
      "train Loss: 0.9527 Acc: 0.5870 \n",
      "val Loss: 0.9690 Acc: 0.5735 \n",
      "Epoch 21/29\n",
      "----------\n",
      "train Loss: 0.9526 Acc: 0.5867 \n",
      "val Loss: 0.9689 Acc: 0.5735 \n",
      "Epoch 22/29\n",
      "----------\n",
      "train Loss: 0.9526 Acc: 0.5870 \n",
      "val Loss: 0.9689 Acc: 0.5735 \n",
      "Epoch 23/29\n",
      "----------\n",
      "train Loss: 0.9525 Acc: 0.5870 \n",
      "val Loss: 0.9689 Acc: 0.5735 \n",
      "Epoch 24/29\n",
      "----------\n",
      "train Loss: 0.9524 Acc: 0.5867 \n",
      "val Loss: 0.9689 Acc: 0.5730 \n",
      "Epoch 25/29\n",
      "----------\n",
      "train Loss: 0.9524 Acc: 0.5870 \n",
      "val Loss: 0.9689 Acc: 0.5730 \n",
      "Epoch 26/29\n",
      "----------\n",
      "train Loss: 0.9523 Acc: 0.5870 \n",
      "val Loss: 0.9689 Acc: 0.5730 \n",
      "Epoch 27/29\n",
      "----------\n",
      "train Loss: 0.9522 Acc: 0.5867 \n",
      "val Loss: 0.9689 Acc: 0.5730 \n",
      "Epoch 28/29\n",
      "----------\n",
      "train Loss: 0.9521 Acc: 0.5867 \n",
      "val Loss: 0.9689 Acc: 0.5730 \n",
      "Epoch 29/29\n",
      "----------\n",
      "train Loss: 0.9521 Acc: 0.5873 \n",
      "val Loss: 0.9689 Acc: 0.5730 \n",
      "Training complete in 0m 19s\n",
      "Best val Acc: 0.578500\n",
      "Epoch generations ( 181 /200) :accuracy(fittest mask) tensor(0.5843, dtype=torch.float64) keep_prob 1.0\n",
      "Epoch generations ( 182 /200) :accuracy(fittest mask) tensor(0.5843, dtype=torch.float64) keep_prob 1.0\n",
      "Epoch generations ( 183 /200) :accuracy(fittest mask) tensor(0.5843, dtype=torch.float64) keep_prob 1.0\n",
      "Epoch generations ( 184 /200) :accuracy(fittest mask) tensor(0.5843, dtype=torch.float64) keep_prob 1.0\n",
      "Epoch generations ( 185 /200) :accuracy(fittest mask) tensor(0.5843, dtype=torch.float64) keep_prob 1.0\n",
      "Epoch generations ( 186 /200) :accuracy(fittest mask) tensor(0.5843, dtype=torch.float64) keep_prob 1.0\n",
      "Epoch generations ( 187 /200) :accuracy(fittest mask) tensor(0.5843, dtype=torch.float64) keep_prob 1.0\n",
      "Epoch generations ( 188 /200) :accuracy(fittest mask) tensor(0.5843, dtype=torch.float64) keep_prob 1.0\n",
      "Epoch generations ( 189 /200) :accuracy(fittest mask) tensor(0.5843, dtype=torch.float64) keep_prob 1.0\n",
      "Epoch generations ( 190 /200) :accuracy(fittest mask) tensor(0.5843, dtype=torch.float64) keep_prob 1.0\n",
      "Epoch 0/29\n",
      "----------\n",
      "train Loss: 0.9541 Acc: 0.5833 \n",
      "val Loss: 0.9692 Acc: 0.5785 \n",
      "Epoch 1/29\n",
      "----------\n",
      "train Loss: 0.9541 Acc: 0.5843 \n",
      "val Loss: 0.9692 Acc: 0.5785 \n",
      "Epoch 2/29\n",
      "----------\n",
      "train Loss: 0.9540 Acc: 0.5847 \n",
      "val Loss: 0.9692 Acc: 0.5785 \n",
      "Epoch 3/29\n",
      "----------\n",
      "train Loss: 0.9539 Acc: 0.5853 \n",
      "val Loss: 0.9692 Acc: 0.5785 \n",
      "Epoch 4/29\n",
      "----------\n",
      "train Loss: 0.9538 Acc: 0.5857 \n",
      "val Loss: 0.9691 Acc: 0.5785 \n",
      "Epoch 5/29\n",
      "----------\n",
      "train Loss: 0.9538 Acc: 0.5857 \n",
      "val Loss: 0.9691 Acc: 0.5775 \n",
      "Epoch 6/29\n",
      "----------\n",
      "train Loss: 0.9537 Acc: 0.5857 \n",
      "val Loss: 0.9691 Acc: 0.5765 \n",
      "Epoch 7/29\n",
      "----------\n",
      "train Loss: 0.9536 Acc: 0.5857 \n",
      "val Loss: 0.9691 Acc: 0.5760 \n",
      "Epoch 8/29\n",
      "----------\n",
      "train Loss: 0.9535 Acc: 0.5867 \n",
      "val Loss: 0.9691 Acc: 0.5760 \n",
      "Epoch 9/29\n",
      "----------\n",
      "train Loss: 0.9534 Acc: 0.5867 \n",
      "val Loss: 0.9691 Acc: 0.5760 \n",
      "Epoch 10/29\n",
      "----------\n",
      "train Loss: 0.9534 Acc: 0.5867 \n",
      "val Loss: 0.9690 Acc: 0.5755 \n",
      "Epoch 11/29\n",
      "----------\n",
      "train Loss: 0.9533 Acc: 0.5867 \n",
      "val Loss: 0.9690 Acc: 0.5750 \n",
      "Epoch 12/29\n",
      "----------\n",
      "train Loss: 0.9532 Acc: 0.5867 \n",
      "val Loss: 0.9690 Acc: 0.5745 \n",
      "Epoch 13/29\n",
      "----------\n",
      "train Loss: 0.9531 Acc: 0.5870 \n",
      "val Loss: 0.9690 Acc: 0.5745 \n",
      "Epoch 14/29\n",
      "----------\n",
      "train Loss: 0.9531 Acc: 0.5870 \n",
      "val Loss: 0.9690 Acc: 0.5740 \n",
      "Epoch 15/29\n",
      "----------\n",
      "train Loss: 0.9530 Acc: 0.5870 \n",
      "val Loss: 0.9690 Acc: 0.5730 \n",
      "Epoch 16/29\n",
      "----------\n",
      "train Loss: 0.9529 Acc: 0.5870 \n",
      "val Loss: 0.9690 Acc: 0.5730 \n",
      "Epoch 17/29\n",
      "----------\n",
      "train Loss: 0.9528 Acc: 0.5877 \n",
      "val Loss: 0.9690 Acc: 0.5735 \n",
      "Epoch 18/29\n",
      "----------\n",
      "train Loss: 0.9528 Acc: 0.5877 \n",
      "val Loss: 0.9690 Acc: 0.5735 \n",
      "Epoch 19/29\n",
      "----------\n",
      "train Loss: 0.9527 Acc: 0.5870 \n",
      "val Loss: 0.9690 Acc: 0.5735 \n",
      "Epoch 20/29\n",
      "----------\n",
      "train Loss: 0.9526 Acc: 0.5867 \n",
      "val Loss: 0.9689 Acc: 0.5735 \n",
      "Epoch 21/29\n",
      "----------\n",
      "train Loss: 0.9526 Acc: 0.5870 \n",
      "val Loss: 0.9689 Acc: 0.5730 \n",
      "Epoch 22/29\n",
      "----------\n",
      "train Loss: 0.9525 Acc: 0.5870 \n",
      "val Loss: 0.9689 Acc: 0.5730 \n",
      "Epoch 23/29\n",
      "----------\n",
      "train Loss: 0.9524 Acc: 0.5867 \n",
      "val Loss: 0.9689 Acc: 0.5730 \n",
      "Epoch 24/29\n",
      "----------\n",
      "train Loss: 0.9524 Acc: 0.5870 \n",
      "val Loss: 0.9689 Acc: 0.5730 \n",
      "Epoch 25/29\n",
      "----------\n",
      "train Loss: 0.9523 Acc: 0.5870 \n",
      "val Loss: 0.9689 Acc: 0.5730 \n",
      "Epoch 26/29\n",
      "----------\n",
      "train Loss: 0.9522 Acc: 0.5867 \n",
      "val Loss: 0.9689 Acc: 0.5730 \n",
      "Epoch 27/29\n",
      "----------\n",
      "train Loss: 0.9521 Acc: 0.5867 \n",
      "val Loss: 0.9689 Acc: 0.5735 \n",
      "Epoch 28/29\n",
      "----------\n",
      "train Loss: 0.9521 Acc: 0.5870 \n",
      "val Loss: 0.9689 Acc: 0.5735 \n",
      "Epoch 29/29\n",
      "----------\n",
      "train Loss: 0.9520 Acc: 0.5873 \n",
      "val Loss: 0.9689 Acc: 0.5735 \n",
      "Training complete in 0m 19s\n",
      "Best val Acc: 0.578500\n",
      "Epoch generations ( 191 /200) :accuracy(fittest mask) tensor(0.5843, dtype=torch.float64) keep_prob 1.0\n",
      "Epoch generations ( 192 /200) :accuracy(fittest mask) tensor(0.5843, dtype=torch.float64) keep_prob 1.0\n",
      "Epoch generations ( 193 /200) :accuracy(fittest mask) tensor(0.5843, dtype=torch.float64) keep_prob 1.0\n",
      "Epoch generations ( 194 /200) :accuracy(fittest mask) tensor(0.5843, dtype=torch.float64) keep_prob 1.0\n",
      "Epoch generations ( 195 /200) :accuracy(fittest mask) tensor(0.5843, dtype=torch.float64) keep_prob 1.0\n",
      "Epoch generations ( 196 /200) :accuracy(fittest mask) tensor(0.5843, dtype=torch.float64) keep_prob 1.0\n",
      "Epoch generations ( 197 /200) :accuracy(fittest mask) tensor(0.5843, dtype=torch.float64) keep_prob 1.0\n",
      "Epoch generations ( 198 /200) :accuracy(fittest mask) tensor(0.5843, dtype=torch.float64) keep_prob 1.0\n",
      "Epoch generations ( 199 /200) :accuracy(fittest mask) tensor(0.5843, dtype=torch.float64) keep_prob 1.0\n",
      "Epoch generations ( 200 /200) :accuracy(fittest mask) tensor(0.5843, dtype=torch.float64) keep_prob 1.0\n",
      "Epoch 0/29\n",
      "----------\n",
      "train Loss: 0.9541 Acc: 0.5843 \n",
      "val Loss: 0.9692 Acc: 0.5785 \n",
      "Epoch 1/29\n",
      "----------\n",
      "train Loss: 0.9540 Acc: 0.5847 \n",
      "val Loss: 0.9692 Acc: 0.5785 \n",
      "Epoch 2/29\n",
      "----------\n",
      "train Loss: 0.9539 Acc: 0.5857 \n",
      "val Loss: 0.9692 Acc: 0.5785 \n",
      "Epoch 3/29\n",
      "----------\n",
      "train Loss: 0.9538 Acc: 0.5857 \n",
      "val Loss: 0.9691 Acc: 0.5785 \n",
      "Epoch 4/29\n",
      "----------\n",
      "train Loss: 0.9538 Acc: 0.5857 \n",
      "val Loss: 0.9691 Acc: 0.5775 \n",
      "Epoch 5/29\n",
      "----------\n",
      "train Loss: 0.9537 Acc: 0.5857 \n",
      "val Loss: 0.9691 Acc: 0.5765 \n",
      "Epoch 6/29\n",
      "----------\n",
      "train Loss: 0.9536 Acc: 0.5857 \n",
      "val Loss: 0.9691 Acc: 0.5760 \n",
      "Epoch 7/29\n",
      "----------\n",
      "train Loss: 0.9535 Acc: 0.5867 \n",
      "val Loss: 0.9691 Acc: 0.5760 \n",
      "Epoch 8/29\n",
      "----------\n",
      "train Loss: 0.9534 Acc: 0.5867 \n",
      "val Loss: 0.9691 Acc: 0.5755 \n",
      "Epoch 9/29\n",
      "----------\n",
      "train Loss: 0.9534 Acc: 0.5867 \n",
      "val Loss: 0.9690 Acc: 0.5755 \n",
      "Epoch 10/29\n",
      "----------\n",
      "train Loss: 0.9533 Acc: 0.5867 \n",
      "val Loss: 0.9690 Acc: 0.5750 \n",
      "Epoch 11/29\n",
      "----------\n",
      "train Loss: 0.9532 Acc: 0.5870 \n",
      "val Loss: 0.9690 Acc: 0.5750 \n",
      "Epoch 12/29\n",
      "----------\n",
      "train Loss: 0.9531 Acc: 0.5870 \n",
      "val Loss: 0.9690 Acc: 0.5745 \n",
      "Epoch 13/29\n",
      "----------\n",
      "train Loss: 0.9531 Acc: 0.5870 \n",
      "val Loss: 0.9690 Acc: 0.5740 \n",
      "Epoch 14/29\n",
      "----------\n",
      "train Loss: 0.9530 Acc: 0.5870 \n",
      "val Loss: 0.9690 Acc: 0.5730 \n",
      "Epoch 15/29\n",
      "----------\n",
      "train Loss: 0.9529 Acc: 0.5873 \n",
      "val Loss: 0.9690 Acc: 0.5730 \n",
      "Epoch 16/29\n",
      "----------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train Loss: 0.9528 Acc: 0.5877 \n",
      "val Loss: 0.9690 Acc: 0.5735 \n",
      "Epoch 17/29\n",
      "----------\n",
      "train Loss: 0.9528 Acc: 0.5873 \n",
      "val Loss: 0.9690 Acc: 0.5735 \n",
      "Epoch 18/29\n",
      "----------\n",
      "train Loss: 0.9527 Acc: 0.5870 \n",
      "val Loss: 0.9690 Acc: 0.5735 \n",
      "Epoch 19/29\n",
      "----------\n",
      "train Loss: 0.9526 Acc: 0.5870 \n",
      "val Loss: 0.9689 Acc: 0.5735 \n",
      "Epoch 20/29\n",
      "----------\n",
      "train Loss: 0.9526 Acc: 0.5870 \n",
      "val Loss: 0.9689 Acc: 0.5735 \n",
      "Epoch 21/29\n",
      "----------\n",
      "train Loss: 0.9525 Acc: 0.5870 \n",
      "val Loss: 0.9689 Acc: 0.5735 \n",
      "Epoch 22/29\n",
      "----------\n",
      "train Loss: 0.9524 Acc: 0.5870 \n",
      "val Loss: 0.9689 Acc: 0.5730 \n",
      "Epoch 23/29\n",
      "----------\n",
      "train Loss: 0.9524 Acc: 0.5870 \n",
      "val Loss: 0.9689 Acc: 0.5730 \n",
      "Epoch 24/29\n",
      "----------\n",
      "train Loss: 0.9523 Acc: 0.5873 \n",
      "val Loss: 0.9689 Acc: 0.5730 \n",
      "Epoch 25/29\n",
      "----------\n",
      "train Loss: 0.9522 Acc: 0.5867 \n",
      "val Loss: 0.9689 Acc: 0.5730 \n",
      "Epoch 26/29\n",
      "----------\n",
      "train Loss: 0.9521 Acc: 0.5867 \n",
      "val Loss: 0.9689 Acc: 0.5730 \n",
      "Epoch 27/29\n",
      "----------\n",
      "train Loss: 0.9521 Acc: 0.5873 \n",
      "val Loss: 0.9689 Acc: 0.5730 \n",
      "Epoch 28/29\n",
      "----------\n",
      "train Loss: 0.9520 Acc: 0.5873 \n",
      "val Loss: 0.9689 Acc: 0.5735 \n",
      "Epoch 29/29\n",
      "----------\n",
      "train Loss: 0.9519 Acc: 0.5873 \n",
      "val Loss: 0.9689 Acc: 0.5735 \n",
      "Training complete in 0m 19s\n",
      "Best val Acc: 0.578500\n"
     ]
    }
   ],
   "source": [
    "#control block controls the epochs and the generations of mask\n",
    "#step 1 an object of the population class randomly generating the first population \n",
    "#step2 :calculate fitness of each entitiy of the population \n",
    "#step3: creates a mating pool of the population based on the worst two performing parent \n",
    "#step 4 :fittest mask of the generating along with keep_prob found \n",
    "#step 5: if 0th ,10th ,20th, the epochs starts training on the worst performing mask /other wise new generation is created \n",
    "\n",
    "epochgens=0\n",
    "population =Population(mutation_rate,max_population,maskLength)\n",
    "total_acc=[]\n",
    "total_loss=[]\n",
    "while (epochgens<=200):\n",
    "    print ('Epoch generations (',epochgens,'/200)',end=' :')\n",
    "    population .calcFitness(model)\n",
    "    population.naturalSelection()\n",
    "    fittestmask,p=population .fittest()\n",
    "    accuracy=fittestmask.fitness(model)\n",
    "    print (\"accuracy(fittest mask)\",accuracy,\"keep_prob\",p,end='\\n')\n",
    "    if (epochgens%10==0):\n",
    "        model,losses,accuracies=train_model(model,criterion,optimizer,fittestmask.gene,p,30)\n",
    "        total_acc=total_acc+accuracies\n",
    "        total_loss=total_loss+losses\n",
    "    population.generate()\n",
    "    epochgens+=1\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt \n",
    "plt.plot(range(630),total_acc,label='accuracy')\n",
    "plt.plot(range(630),total_loss,label='loss')\n",
    "plt.legend()\n",
    "plt.xlabel('number of epochs')\n",
    "plt.title('proposed')\n",
    "plt.savefig('wave 20/ga_drop.jpg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
