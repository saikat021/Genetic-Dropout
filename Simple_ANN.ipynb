{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "id": "pdWtHV8WcuiV",
    "outputId": "d00f3832-17bc-4679-b5d7-1ddb119ed3c7"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 256
    },
    "colab_type": "code",
    "id": "mmQC8wswc4f1",
    "outputId": "cdf2a6dc-61aa-4f7f-dda0-b9fc1e8caf21"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>att_1</th>\n",
       "      <th>att_2</th>\n",
       "      <th>att_3</th>\n",
       "      <th>att_4</th>\n",
       "      <th>att_5</th>\n",
       "      <th>att_6</th>\n",
       "      <th>att_7</th>\n",
       "      <th>att_8</th>\n",
       "      <th>att_9</th>\n",
       "      <th>att_10</th>\n",
       "      <th>...</th>\n",
       "      <th>att_136</th>\n",
       "      <th>att_137</th>\n",
       "      <th>att_138</th>\n",
       "      <th>att_139</th>\n",
       "      <th>att_140</th>\n",
       "      <th>att_141</th>\n",
       "      <th>att_142</th>\n",
       "      <th>att_143</th>\n",
       "      <th>att_144</th>\n",
       "      <th>class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>279</td>\n",
       "      <td>60</td>\n",
       "      <td>39</td>\n",
       "      <td>36</td>\n",
       "      <td>24</td>\n",
       "      <td>21</td>\n",
       "      <td>20</td>\n",
       "      <td>19</td>\n",
       "      <td>20</td>\n",
       "      <td>22</td>\n",
       "      <td>...</td>\n",
       "      <td>3993</td>\n",
       "      <td>2419</td>\n",
       "      <td>3320</td>\n",
       "      <td>2660</td>\n",
       "      <td>2850</td>\n",
       "      <td>2591</td>\n",
       "      <td>2594</td>\n",
       "      <td>2444</td>\n",
       "      <td>2546</td>\n",
       "      <td>A</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>272</td>\n",
       "      <td>62</td>\n",
       "      <td>43</td>\n",
       "      <td>27</td>\n",
       "      <td>24</td>\n",
       "      <td>20</td>\n",
       "      <td>20</td>\n",
       "      <td>20</td>\n",
       "      <td>25</td>\n",
       "      <td>18</td>\n",
       "      <td>...</td>\n",
       "      <td>2687</td>\n",
       "      <td>2887</td>\n",
       "      <td>3494</td>\n",
       "      <td>2496</td>\n",
       "      <td>2591</td>\n",
       "      <td>2745</td>\n",
       "      <td>2384</td>\n",
       "      <td>2351</td>\n",
       "      <td>2411</td>\n",
       "      <td>A</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>177</td>\n",
       "      <td>82</td>\n",
       "      <td>46</td>\n",
       "      <td>28</td>\n",
       "      <td>23</td>\n",
       "      <td>22</td>\n",
       "      <td>19</td>\n",
       "      <td>16</td>\n",
       "      <td>17</td>\n",
       "      <td>16</td>\n",
       "      <td>...</td>\n",
       "      <td>3071</td>\n",
       "      <td>3717</td>\n",
       "      <td>3073</td>\n",
       "      <td>2835</td>\n",
       "      <td>3094</td>\n",
       "      <td>2499</td>\n",
       "      <td>2480</td>\n",
       "      <td>2483</td>\n",
       "      <td>2548</td>\n",
       "      <td>A</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>205</td>\n",
       "      <td>84</td>\n",
       "      <td>44</td>\n",
       "      <td>26</td>\n",
       "      <td>25</td>\n",
       "      <td>23</td>\n",
       "      <td>20</td>\n",
       "      <td>20</td>\n",
       "      <td>20</td>\n",
       "      <td>28</td>\n",
       "      <td>...</td>\n",
       "      <td>3818</td>\n",
       "      <td>2340</td>\n",
       "      <td>2372</td>\n",
       "      <td>2372</td>\n",
       "      <td>2370</td>\n",
       "      <td>2537</td>\n",
       "      <td>2526</td>\n",
       "      <td>2521</td>\n",
       "      <td>2418</td>\n",
       "      <td>A</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>166</td>\n",
       "      <td>91</td>\n",
       "      <td>38</td>\n",
       "      <td>26</td>\n",
       "      <td>25</td>\n",
       "      <td>21</td>\n",
       "      <td>19</td>\n",
       "      <td>19</td>\n",
       "      <td>19</td>\n",
       "      <td>17</td>\n",
       "      <td>...</td>\n",
       "      <td>2388</td>\n",
       "      <td>2561</td>\n",
       "      <td>2558</td>\n",
       "      <td>2543</td>\n",
       "      <td>3033</td>\n",
       "      <td>2928</td>\n",
       "      <td>2825</td>\n",
       "      <td>2530</td>\n",
       "      <td>2424</td>\n",
       "      <td>A</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 145 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   att_1  att_2  att_3  att_4  att_5  att_6  att_7  att_8  att_9  att_10  ...  \\\n",
       "0    279     60     39     36     24     21     20     19     20      22  ...   \n",
       "1    272     62     43     27     24     20     20     20     25      18  ...   \n",
       "2    177     82     46     28     23     22     19     16     17      16  ...   \n",
       "3    205     84     44     26     25     23     20     20     20      28  ...   \n",
       "4    166     91     38     26     25     21     19     19     19      17  ...   \n",
       "\n",
       "   att_136  att_137  att_138  att_139  att_140  att_141  att_142  att_143  \\\n",
       "0     3993     2419     3320     2660     2850     2591     2594     2444   \n",
       "1     2687     2887     3494     2496     2591     2745     2384     2351   \n",
       "2     3071     3717     3073     2835     3094     2499     2480     2483   \n",
       "3     3818     2340     2372     2372     2370     2537     2526     2521   \n",
       "4     2388     2561     2558     2543     3033     2928     2825     2530   \n",
       "\n",
       "   att_144  class  \n",
       "0     2546      A  \n",
       "1     2411      A  \n",
       "2     2548      A  \n",
       "3     2418      A  \n",
       "4     2424      A  \n",
       "\n",
       "[5 rows x 145 columns]"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "path ='Riktim_Testing/Line-level-Features/DHT_Algorithm_144_Features.csv'\n",
    "df = pd.read_csv(path) #header=None\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "id": "JvdIw7L-c6jj",
    "outputId": "5877d884-9f75-4988-eed5-3ccbebe32172"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "wi_1eAi-c80l"
   },
   "outputs": [],
   "source": [
    "# Import modules\n",
    "import numpy as np\n",
    "# import matplotlib.pyplot as plt\n",
    "# from sklearn.datasets import load_iris,load_digits,load_breast_cancer,load_wine\n",
    "# from sklearn import preprocessing\n",
    "\n",
    "# Some more magic so that the notebook will reload external python modules;\n",
    "# see http://stackoverflow.com/questions/1907993/autoreload-of-modules-in-ipython\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "XWti03uamkMV"
   },
   "outputs": [],
   "source": [
    "# # Load the iris dataset\n",
    "# data = load_digits()\n",
    "\n",
    "# # Store the features as X and the labels as y\n",
    "# X = data.data\n",
    "# y = data.target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 127
    },
    "colab_type": "code",
    "id": "CTevYo6Tc-jM",
    "outputId": "4d19021d-0704-42f7-e6dd-2594f5283364"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['att_1', 'att_2', 'att_3', 'att_4', 'att_5', 'att_6', 'att_7', 'att_8',\n",
       "       'att_9', 'att_10',\n",
       "       ...\n",
       "       'att_136', 'att_137', 'att_138', 'att_139', 'att_140', 'att_141',\n",
       "       'att_142', 'att_143', 'att_144', 'class'],\n",
       "      dtype='object', length=145)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 127
    },
    "colab_type": "code",
    "id": "RyCqrQcEdBMu",
    "outputId": "2994132f-52d1-4930-956d-b3d8bf45c9b9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3600, 145)\n",
      "<class 'numpy.ndarray'>\n",
      "(3600, 144)\n",
      "\n",
      "['A' 'B' 'C' 'D' 'E' 'F' 'G' 'H' 'I' 'J' 'K' 'L']\n",
      "[0 0 0 0 0 0 0 0 0 0]\n"
     ]
    }
   ],
   "source": [
    "print(df.shape)\n",
    "#X=df.loc[:,df.columns != 'Classifier'].values\n",
    "X=df.loc[:,df.columns != df.columns[-1]].values\n",
    "print(type(X))\n",
    "print(X.shape)\n",
    "#y_label=df['Classifier'].values \n",
    "y_label =df.iloc[:,-1].values\n",
    "#print(type(y_label))\n",
    "#print(X[0])\n",
    "print()\n",
    "from sklearn import preprocessing\n",
    "le = preprocessing.LabelEncoder()\n",
    "le.fit(y_label)\n",
    "print(le.classes_)\n",
    "y=le.transform(y_label)\n",
    "print(y[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 54
    },
    "colab_type": "code",
    "id": "pI7UW-CrdH6W",
    "outputId": "f658c4c5-a824-475f-f474-234fcdb6f94d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11} {0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11}\n",
      "(3600, 144) (3600,) (2520, 144) (1080, 144) (2520,) (1080,)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler #for normalization\n",
    "sc = StandardScaler()\n",
    "X = sc.fit_transform(X)\n",
    "# from sklearn.preprocessing import MinMaxScaler\n",
    "# scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "# X = scaler.fit_transform(X)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42, shuffle=True)\n",
    "# print(y_train)\n",
    "# print(y_test)\n",
    "#print(len(X_train), len(y_train), len(X_test), len(y_test))\n",
    "# # X=X_train\n",
    "# # y=y_train\n",
    "print(set(y_train), set(y_test))\n",
    "print(X.shape, y.shape,X_train.shape,X_test.shape,y_train.shape,y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "K3sGsQIWdKTq"
   },
   "outputs": [],
   "source": [
    "n_inputs = X_train.shape[1]\n",
    "n_hidden = X_train.shape[1]\n",
    "n_classes = len(le.classes_)\n",
    "#N = X_train.shape[0] # Number of samples\n",
    "\n",
    "#print(N)\n",
    "#hyperparameters\n",
    "lr = 0.001\n",
    "num_epoch = 310"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "VdBHK_mIdU7x"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "#torch.manual_seed(1234)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "PWfQFapWjDPM"
   },
   "outputs": [],
   "source": [
    "#build model\n",
    "class Net(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        self.fc1 = nn.Linear(n_inputs, n_hidden)\n",
    "        self.drop = nn.Dropout (p=0.4)\n",
    "        self.fc2 = nn.Linear(n_hidden,n_classes )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x=self.drop(x)\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "        \n",
    "net = Net()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "JS0_RD2EjHQt"
   },
   "outputs": [],
   "source": [
    "#choose optimizer and loss function\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "#optimizer = torch.optim.SGD(net.parameters(), lr=0.001)\n",
    "optimizer = torch.optim.Adam(net.parameters(), lr=lr)\n",
    "\n",
    "#swap test and train\n",
    "# X_train,X_test=X_test,X_train\n",
    "# y_train,y_test =y_test,y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "azXqn4uCAupH"
   },
   "outputs": [],
   "source": [
    "# #print(type(X_train[0]),type(X_test[0]),type(y_train[0]),type(y_test[0]))\n",
    "# from sklearn.model_selection import KFold # import KFold\n",
    "# kf = KFold(n_splits=5,shuffle=True, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "USZ9caBSjIDA"
   },
   "outputs": [],
   "source": [
    "# #train\n",
    "# train_scores=[]\n",
    "# test_scores=[]\n",
    "# acc=0\n",
    "# for k, (train, test) in enumerate(kf.split(X, y)):\n",
    "#   X_train, X_test = X[train], X[test]\n",
    "#   y_train, y_test = y[train], y[test]\n",
    "#  # X_train = torch.tensor(X_train, dtype=torch.float)\n",
    "#  # y_train = torch.tensor(y_train, dtype=torch.long)\n",
    "#  # X_test = torch.tensor(X_test, dtype=torch.float)\n",
    "#  # y_test = torch.tensor(y_test, dtype=torch.long)\n",
    "#   net.train()\n",
    "#   #print(X_train.shape, y_train.shape)\n",
    "#   #train.sort()\n",
    "#   #test.sort()\n",
    "#   #print(train== test)\n",
    "#   #print(train[:10])\n",
    "#   #print(test[:10])\n",
    "#   print(set(list(y_test)))\n",
    "#   print(set(list(y_train)))\n",
    "#   continue\n",
    "#   for epoch in range(num_epoch):\n",
    "#       #feedforward - backprop\n",
    "#       optimizer.zero_grad()\n",
    "#       out = net(X_train)\n",
    "#       loss = criterion(out, y_train)\n",
    "#       loss.backward()\n",
    "#       optimizer.step()\n",
    "#       acc = 100 * torch.sum(y_train==torch.max(out.data, 1)[1]).double() / len(y_train)\n",
    "#       print ('Epoch [%d/%d] Loss: %.4f   Acc: %.4f' \n",
    "#                     %(epoch+1, num_epoch, loss.item(), acc.item()))\n",
    "#   train_scores.append(acc)\n",
    "#   print(X_test.shape, y_test.shape)\n",
    "#   net.eval()\n",
    "#   out = net(X_test)\n",
    "#   _, predicted = torch.max(out.data, 1) \n",
    "#   test_scores.append((100 * torch.sum(y_test==predicted).double() / len(y_test)));\n",
    "#   del net\n",
    "#   net = Net()\n",
    "#   criterion = nn.CrossEntropyLoss()\n",
    "#   #optimizer = torch.optim.SGD(net.parameters(), lr=0.001)\n",
    "#   optimizer = torch.optim.Adam(net.parameters(), lr=lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "70hRvef3fnEK"
   },
   "outputs": [],
   "source": [
    "# out = net(X_train)\n",
    "# _, predicted = torch.max(out.data, 1) \n",
    "# print((100 * torch.sum(y_train==predicted).double() / len(y_train)));"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "GJ_nuJcRcXvy"
   },
   "outputs": [],
   "source": [
    "# print(train_scores)\n",
    "# print(test_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "i7ZhHlDOiadV",
    "outputId": "5a948084-df8e-4ba0-b8e2-e6aa33fb9ce2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/310] Loss: 2.5466   Acc: 7.2619\n",
      "Epoch [2/310] Loss: 2.4555   Acc: 12.0238\n",
      "Epoch [3/310] Loss: 2.3796   Acc: 18.0159\n",
      "Epoch [4/310] Loss: 2.3168   Acc: 21.7063\n",
      "Epoch [5/310] Loss: 2.2500   Acc: 25.7937\n",
      "Epoch [6/310] Loss: 2.1928   Acc: 31.5476\n",
      "Epoch [7/310] Loss: 2.1315   Acc: 33.6111\n",
      "Epoch [8/310] Loss: 2.0842   Acc: 35.9127\n",
      "Epoch [9/310] Loss: 2.0288   Acc: 37.7381\n",
      "Epoch [10/310] Loss: 1.9867   Acc: 39.0079\n",
      "Epoch [11/310] Loss: 1.9458   Acc: 41.5079\n",
      "Epoch [12/310] Loss: 1.9005   Acc: 44.2460\n",
      "Epoch [13/310] Loss: 1.8506   Acc: 44.4444\n",
      "Epoch [14/310] Loss: 1.8166   Acc: 46.7857\n",
      "Epoch [15/310] Loss: 1.7981   Acc: 46.3889\n",
      "Epoch [16/310] Loss: 1.7472   Acc: 49.2460\n",
      "Epoch [17/310] Loss: 1.7197   Acc: 49.8016\n",
      "Epoch [18/310] Loss: 1.6837   Acc: 51.0317\n",
      "Epoch [19/310] Loss: 1.6456   Acc: 52.3413\n",
      "Epoch [20/310] Loss: 1.6241   Acc: 53.2937\n",
      "Epoch [21/310] Loss: 1.5911   Acc: 54.6032\n",
      "Epoch [22/310] Loss: 1.5535   Acc: 57.3016\n",
      "Epoch [23/310] Loss: 1.5240   Acc: 57.8175\n",
      "Epoch [24/310] Loss: 1.4953   Acc: 58.3730\n",
      "Epoch [25/310] Loss: 1.4644   Acc: 59.3651\n",
      "Epoch [26/310] Loss: 1.4377   Acc: 60.6349\n",
      "Epoch [27/310] Loss: 1.4013   Acc: 61.1111\n",
      "Epoch [28/310] Loss: 1.3916   Acc: 60.5952\n",
      "Epoch [29/310] Loss: 1.3475   Acc: 62.3810\n",
      "Epoch [30/310] Loss: 1.3343   Acc: 63.7698\n",
      "Epoch [31/310] Loss: 1.3111   Acc: 63.6905\n",
      "Epoch [32/310] Loss: 1.2818   Acc: 65.0397\n",
      "Epoch [33/310] Loss: 1.2548   Acc: 65.2381\n",
      "Epoch [34/310] Loss: 1.2304   Acc: 66.0714\n",
      "Epoch [35/310] Loss: 1.2099   Acc: 66.7857\n",
      "Epoch [36/310] Loss: 1.1875   Acc: 66.8254\n",
      "Epoch [37/310] Loss: 1.1624   Acc: 67.8175\n",
      "Epoch [38/310] Loss: 1.1432   Acc: 68.3333\n",
      "Epoch [39/310] Loss: 1.1229   Acc: 68.8889\n",
      "Epoch [40/310] Loss: 1.1042   Acc: 68.2937\n",
      "Epoch [41/310] Loss: 1.0796   Acc: 70.8333\n",
      "Epoch [42/310] Loss: 1.0553   Acc: 70.9127\n",
      "Epoch [43/310] Loss: 1.0408   Acc: 71.6270\n",
      "Epoch [44/310] Loss: 1.0270   Acc: 71.3492\n",
      "Epoch [45/310] Loss: 0.9972   Acc: 73.0556\n",
      "Epoch [46/310] Loss: 0.9801   Acc: 73.5317\n",
      "Epoch [47/310] Loss: 0.9689   Acc: 73.0952\n",
      "Epoch [48/310] Loss: 0.9406   Acc: 74.3254\n",
      "Epoch [49/310] Loss: 0.9366   Acc: 74.7619\n",
      "Epoch [50/310] Loss: 0.9241   Acc: 74.3651\n",
      "Epoch [51/310] Loss: 0.8925   Acc: 76.3889\n",
      "Epoch [52/310] Loss: 0.8667   Acc: 77.4206\n",
      "Epoch [53/310] Loss: 0.8719   Acc: 76.9048\n",
      "Epoch [54/310] Loss: 0.8481   Acc: 76.7063\n",
      "Epoch [55/310] Loss: 0.8363   Acc: 77.6984\n",
      "Epoch [56/310] Loss: 0.8225   Acc: 77.6984\n",
      "Epoch [57/310] Loss: 0.7958   Acc: 78.4524\n",
      "Epoch [58/310] Loss: 0.7974   Acc: 77.4603\n",
      "Epoch [59/310] Loss: 0.7862   Acc: 78.6508\n",
      "Epoch [60/310] Loss: 0.7550   Acc: 80.2778\n",
      "Epoch [61/310] Loss: 0.7567   Acc: 80.3968\n",
      "Epoch [62/310] Loss: 0.7383   Acc: 80.4762\n",
      "Epoch [63/310] Loss: 0.7261   Acc: 80.7937\n",
      "Epoch [64/310] Loss: 0.7230   Acc: 80.8730\n",
      "Epoch [65/310] Loss: 0.7017   Acc: 82.0238\n",
      "Epoch [66/310] Loss: 0.7010   Acc: 81.1905\n",
      "Epoch [67/310] Loss: 0.6722   Acc: 82.0238\n",
      "Epoch [68/310] Loss: 0.6756   Acc: 81.6667\n",
      "Epoch [69/310] Loss: 0.6707   Acc: 81.6667\n",
      "Epoch [70/310] Loss: 0.6510   Acc: 82.9762\n",
      "Epoch [71/310] Loss: 0.6433   Acc: 83.6508\n",
      "Epoch [72/310] Loss: 0.6317   Acc: 83.7302\n",
      "Epoch [73/310] Loss: 0.6279   Acc: 83.6905\n",
      "Epoch [74/310] Loss: 0.6125   Acc: 82.8571\n",
      "Epoch [75/310] Loss: 0.6078   Acc: 83.3730\n",
      "Epoch [76/310] Loss: 0.5892   Acc: 84.6825\n",
      "Epoch [77/310] Loss: 0.5867   Acc: 84.8016\n",
      "Epoch [78/310] Loss: 0.5822   Acc: 84.7619\n",
      "Epoch [79/310] Loss: 0.5739   Acc: 84.8016\n",
      "Epoch [80/310] Loss: 0.5542   Acc: 85.3175\n",
      "Epoch [81/310] Loss: 0.5548   Acc: 85.6349\n",
      "Epoch [82/310] Loss: 0.5509   Acc: 85.9921\n",
      "Epoch [83/310] Loss: 0.5373   Acc: 86.0317\n",
      "Epoch [84/310] Loss: 0.5380   Acc: 85.4365\n",
      "Epoch [85/310] Loss: 0.5196   Acc: 87.1429\n",
      "Epoch [86/310] Loss: 0.5205   Acc: 86.1111\n",
      "Epoch [87/310] Loss: 0.5092   Acc: 86.6667\n",
      "Epoch [88/310] Loss: 0.4944   Acc: 87.0238\n",
      "Epoch [89/310] Loss: 0.4948   Acc: 86.8651\n",
      "Epoch [90/310] Loss: 0.4886   Acc: 87.3413\n",
      "Epoch [91/310] Loss: 0.4795   Acc: 88.0556\n",
      "Epoch [92/310] Loss: 0.4790   Acc: 87.3413\n",
      "Epoch [93/310] Loss: 0.4817   Acc: 86.7460\n",
      "Epoch [94/310] Loss: 0.4546   Acc: 88.6111\n",
      "Epoch [95/310] Loss: 0.4569   Acc: 88.1349\n",
      "Epoch [96/310] Loss: 0.4452   Acc: 88.4524\n",
      "Epoch [97/310] Loss: 0.4439   Acc: 88.0159\n",
      "Epoch [98/310] Loss: 0.4388   Acc: 88.3333\n",
      "Epoch [99/310] Loss: 0.4396   Acc: 88.6111\n",
      "Epoch [100/310] Loss: 0.4369   Acc: 88.6905\n",
      "Epoch [101/310] Loss: 0.4184   Acc: 89.2460\n",
      "Epoch [102/310] Loss: 0.4267   Acc: 88.7302\n",
      "Epoch [103/310] Loss: 0.4113   Acc: 89.3651\n",
      "Epoch [104/310] Loss: 0.4040   Acc: 89.8016\n",
      "Epoch [105/310] Loss: 0.3981   Acc: 90.5952\n",
      "Epoch [106/310] Loss: 0.3994   Acc: 89.6032\n",
      "Epoch [107/310] Loss: 0.3908   Acc: 90.1587\n",
      "Epoch [108/310] Loss: 0.3918   Acc: 89.7222\n",
      "Epoch [109/310] Loss: 0.3880   Acc: 90.3968\n",
      "Epoch [110/310] Loss: 0.3771   Acc: 89.9206\n",
      "Epoch [111/310] Loss: 0.3693   Acc: 91.2302\n",
      "Epoch [112/310] Loss: 0.3717   Acc: 90.5556\n",
      "Epoch [113/310] Loss: 0.3683   Acc: 90.6746\n",
      "Epoch [114/310] Loss: 0.3659   Acc: 90.1984\n",
      "Epoch [115/310] Loss: 0.3612   Acc: 90.4762\n",
      "Epoch [116/310] Loss: 0.3542   Acc: 90.7937\n",
      "Epoch [117/310] Loss: 0.3583   Acc: 91.0317\n",
      "Epoch [118/310] Loss: 0.3424   Acc: 90.8730\n",
      "Epoch [119/310] Loss: 0.3447   Acc: 91.3889\n",
      "Epoch [120/310] Loss: 0.3396   Acc: 91.5476\n",
      "Epoch [121/310] Loss: 0.3322   Acc: 91.5079\n",
      "Epoch [122/310] Loss: 0.3234   Acc: 92.1825\n",
      "Epoch [123/310] Loss: 0.3220   Acc: 91.5873\n",
      "Epoch [124/310] Loss: 0.3234   Acc: 92.2619\n",
      "Epoch [125/310] Loss: 0.3259   Acc: 91.7063\n",
      "Epoch [126/310] Loss: 0.3132   Acc: 92.0238\n",
      "Epoch [127/310] Loss: 0.3199   Acc: 91.8254\n",
      "Epoch [128/310] Loss: 0.3173   Acc: 92.0238\n",
      "Epoch [129/310] Loss: 0.3107   Acc: 92.6984\n",
      "Epoch [130/310] Loss: 0.3030   Acc: 91.5873\n",
      "Epoch [131/310] Loss: 0.2899   Acc: 92.8175\n",
      "Epoch [132/310] Loss: 0.2902   Acc: 93.3333\n",
      "Epoch [133/310] Loss: 0.2984   Acc: 92.4603\n",
      "Epoch [134/310] Loss: 0.2917   Acc: 92.2619\n",
      "Epoch [135/310] Loss: 0.2883   Acc: 92.6587\n",
      "Epoch [136/310] Loss: 0.2852   Acc: 93.1349\n",
      "Epoch [137/310] Loss: 0.2826   Acc: 93.2540\n",
      "Epoch [138/310] Loss: 0.2771   Acc: 93.0952\n",
      "Epoch [139/310] Loss: 0.2830   Acc: 92.9365\n",
      "Epoch [140/310] Loss: 0.2792   Acc: 93.2937\n",
      "Epoch [141/310] Loss: 0.2635   Acc: 94.0873\n",
      "Epoch [142/310] Loss: 0.2766   Acc: 92.9365\n",
      "Epoch [143/310] Loss: 0.2599   Acc: 93.9683\n",
      "Epoch [144/310] Loss: 0.2648   Acc: 93.4524\n",
      "Epoch [145/310] Loss: 0.2551   Acc: 93.3730\n",
      "Epoch [146/310] Loss: 0.2553   Acc: 94.3254\n",
      "Epoch [147/310] Loss: 0.2587   Acc: 93.6905\n",
      "Epoch [148/310] Loss: 0.2590   Acc: 93.6508\n",
      "Epoch [149/310] Loss: 0.2501   Acc: 93.6508\n",
      "Epoch [150/310] Loss: 0.2497   Acc: 94.4841\n",
      "Epoch [151/310] Loss: 0.2520   Acc: 93.8492\n",
      "Epoch [152/310] Loss: 0.2478   Acc: 93.7302\n",
      "Epoch [153/310] Loss: 0.2466   Acc: 93.8492\n",
      "Epoch [154/310] Loss: 0.2309   Acc: 94.8016\n",
      "Epoch [155/310] Loss: 0.2398   Acc: 94.4444\n",
      "Epoch [156/310] Loss: 0.2340   Acc: 94.6825\n",
      "Epoch [157/310] Loss: 0.2353   Acc: 94.4841\n",
      "Epoch [158/310] Loss: 0.2234   Acc: 95.3968\n",
      "Epoch [159/310] Loss: 0.2274   Acc: 94.6429\n",
      "Epoch [160/310] Loss: 0.2278   Acc: 94.5238\n",
      "Epoch [161/310] Loss: 0.2235   Acc: 94.2063\n",
      "Epoch [162/310] Loss: 0.2225   Acc: 94.9603\n",
      "Epoch [163/310] Loss: 0.2217   Acc: 94.7619\n",
      "Epoch [164/310] Loss: 0.2238   Acc: 95.2381\n",
      "Epoch [165/310] Loss: 0.2088   Acc: 95.0397\n",
      "Epoch [166/310] Loss: 0.2113   Acc: 95.0397\n",
      "Epoch [167/310] Loss: 0.2132   Acc: 95.1984\n",
      "Epoch [168/310] Loss: 0.2085   Acc: 95.3571\n",
      "Epoch [169/310] Loss: 0.2031   Acc: 95.9127\n",
      "Epoch [170/310] Loss: 0.2065   Acc: 95.1984\n",
      "Epoch [171/310] Loss: 0.1990   Acc: 95.7937\n",
      "Epoch [172/310] Loss: 0.1991   Acc: 96.0714\n",
      "Epoch [173/310] Loss: 0.1986   Acc: 95.5952\n",
      "Epoch [174/310] Loss: 0.2066   Acc: 95.4365\n",
      "Epoch [175/310] Loss: 0.1980   Acc: 95.5952\n",
      "Epoch [176/310] Loss: 0.2105   Acc: 94.5635\n",
      "Epoch [177/310] Loss: 0.1879   Acc: 95.7540\n",
      "Epoch [178/310] Loss: 0.1938   Acc: 95.7937\n",
      "Epoch [179/310] Loss: 0.1952   Acc: 95.1587\n",
      "Epoch [180/310] Loss: 0.1957   Acc: 95.5952\n",
      "Epoch [181/310] Loss: 0.1860   Acc: 96.2302\n",
      "Epoch [182/310] Loss: 0.1843   Acc: 95.8730\n",
      "Epoch [183/310] Loss: 0.1862   Acc: 95.6349\n",
      "Epoch [184/310] Loss: 0.1790   Acc: 95.9921\n",
      "Epoch [185/310] Loss: 0.1802   Acc: 96.3492\n",
      "Epoch [186/310] Loss: 0.1727   Acc: 96.5079\n",
      "Epoch [187/310] Loss: 0.1773   Acc: 96.1905\n",
      "Epoch [188/310] Loss: 0.1819   Acc: 95.9921\n",
      "Epoch [189/310] Loss: 0.1739   Acc: 96.5873\n",
      "Epoch [190/310] Loss: 0.1670   Acc: 96.8254\n",
      "Epoch [191/310] Loss: 0.1761   Acc: 96.3889\n",
      "Epoch [192/310] Loss: 0.1707   Acc: 96.4683\n",
      "Epoch [193/310] Loss: 0.1718   Acc: 96.4286\n",
      "Epoch [194/310] Loss: 0.1673   Acc: 96.3889\n",
      "Epoch [195/310] Loss: 0.1647   Acc: 96.4683\n",
      "Epoch [196/310] Loss: 0.1625   Acc: 96.8254\n",
      "Epoch [197/310] Loss: 0.1723   Acc: 95.6746\n",
      "Epoch [198/310] Loss: 0.1629   Acc: 96.3095\n",
      "Epoch [199/310] Loss: 0.1541   Acc: 97.0238\n",
      "Epoch [200/310] Loss: 0.1582   Acc: 97.0238\n",
      "Epoch [201/310] Loss: 0.1520   Acc: 96.9048\n",
      "Epoch [202/310] Loss: 0.1544   Acc: 96.7857\n",
      "Epoch [203/310] Loss: 0.1527   Acc: 96.7460\n",
      "Epoch [204/310] Loss: 0.1481   Acc: 96.7460\n",
      "Epoch [205/310] Loss: 0.1580   Acc: 96.5079\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [206/310] Loss: 0.1474   Acc: 96.7857\n",
      "Epoch [207/310] Loss: 0.1551   Acc: 96.3492\n",
      "Epoch [208/310] Loss: 0.1456   Acc: 97.0238\n",
      "Epoch [209/310] Loss: 0.1479   Acc: 97.1032\n",
      "Epoch [210/310] Loss: 0.1432   Acc: 97.3413\n",
      "Epoch [211/310] Loss: 0.1443   Acc: 97.2222\n",
      "Epoch [212/310] Loss: 0.1349   Acc: 97.7381\n",
      "Epoch [213/310] Loss: 0.1474   Acc: 96.9841\n",
      "Epoch [214/310] Loss: 0.1413   Acc: 97.5000\n",
      "Epoch [215/310] Loss: 0.1376   Acc: 97.3810\n",
      "Epoch [216/310] Loss: 0.1331   Acc: 97.5397\n",
      "Epoch [217/310] Loss: 0.1408   Acc: 97.2619\n",
      "Epoch [218/310] Loss: 0.1372   Acc: 97.4603\n",
      "Epoch [219/310] Loss: 0.1337   Acc: 97.2222\n",
      "Epoch [220/310] Loss: 0.1403   Acc: 97.2619\n",
      "Epoch [221/310] Loss: 0.1362   Acc: 97.1825\n",
      "Epoch [222/310] Loss: 0.1293   Acc: 97.5397\n",
      "Epoch [223/310] Loss: 0.1356   Acc: 97.0238\n",
      "Epoch [224/310] Loss: 0.1351   Acc: 97.1429\n",
      "Epoch [225/310] Loss: 0.1318   Acc: 97.3810\n",
      "Epoch [226/310] Loss: 0.1305   Acc: 97.5397\n",
      "Epoch [227/310] Loss: 0.1230   Acc: 97.6190\n",
      "Epoch [228/310] Loss: 0.1233   Acc: 97.6190\n",
      "Epoch [229/310] Loss: 0.1304   Acc: 97.7778\n",
      "Epoch [230/310] Loss: 0.1263   Acc: 97.5000\n",
      "Epoch [231/310] Loss: 0.1300   Acc: 97.3413\n",
      "Epoch [232/310] Loss: 0.1297   Acc: 97.0238\n",
      "Epoch [233/310] Loss: 0.1216   Acc: 97.7778\n",
      "Epoch [234/310] Loss: 0.1226   Acc: 97.7381\n",
      "Epoch [235/310] Loss: 0.1201   Acc: 97.6984\n",
      "Epoch [236/310] Loss: 0.1282   Acc: 97.4206\n",
      "Epoch [237/310] Loss: 0.1233   Acc: 97.4206\n",
      "Epoch [238/310] Loss: 0.1207   Acc: 97.6587\n",
      "Epoch [239/310] Loss: 0.1194   Acc: 97.6190\n",
      "Epoch [240/310] Loss: 0.1135   Acc: 98.0952\n",
      "Epoch [241/310] Loss: 0.1199   Acc: 97.9365\n",
      "Epoch [242/310] Loss: 0.1197   Acc: 97.7381\n",
      "Epoch [243/310] Loss: 0.1216   Acc: 97.6190\n",
      "Epoch [244/310] Loss: 0.1117   Acc: 97.9365\n",
      "Epoch [245/310] Loss: 0.1040   Acc: 98.5714\n",
      "Epoch [246/310] Loss: 0.1142   Acc: 97.5794\n",
      "Epoch [247/310] Loss: 0.1125   Acc: 97.6984\n",
      "Epoch [248/310] Loss: 0.1162   Acc: 97.4603\n",
      "Epoch [249/310] Loss: 0.1073   Acc: 98.0952\n",
      "Epoch [250/310] Loss: 0.1106   Acc: 98.0952\n",
      "Epoch [251/310] Loss: 0.1127   Acc: 97.9762\n",
      "Epoch [252/310] Loss: 0.1060   Acc: 98.0159\n",
      "Epoch [253/310] Loss: 0.1011   Acc: 98.2540\n",
      "Epoch [254/310] Loss: 0.1057   Acc: 97.9365\n",
      "Epoch [255/310] Loss: 0.1126   Acc: 97.9762\n",
      "Epoch [256/310] Loss: 0.1067   Acc: 98.0556\n",
      "Epoch [257/310] Loss: 0.1081   Acc: 98.0159\n",
      "Epoch [258/310] Loss: 0.1034   Acc: 98.4127\n",
      "Epoch [259/310] Loss: 0.1075   Acc: 98.0159\n",
      "Epoch [260/310] Loss: 0.1059   Acc: 97.8968\n",
      "Epoch [261/310] Loss: 0.1054   Acc: 97.8571\n",
      "Epoch [262/310] Loss: 0.1000   Acc: 97.7778\n",
      "Epoch [263/310] Loss: 0.1002   Acc: 98.3730\n",
      "Epoch [264/310] Loss: 0.0993   Acc: 98.2143\n",
      "Epoch [265/310] Loss: 0.1026   Acc: 97.8175\n",
      "Epoch [266/310] Loss: 0.1027   Acc: 97.9762\n",
      "Epoch [267/310] Loss: 0.1013   Acc: 98.2937\n",
      "Epoch [268/310] Loss: 0.0956   Acc: 98.2143\n",
      "Epoch [269/310] Loss: 0.1006   Acc: 98.1349\n",
      "Epoch [270/310] Loss: 0.1015   Acc: 98.1349\n",
      "Epoch [271/310] Loss: 0.1003   Acc: 98.0952\n",
      "Epoch [272/310] Loss: 0.0928   Acc: 98.5317\n",
      "Epoch [273/310] Loss: 0.0924   Acc: 98.6905\n",
      "Epoch [274/310] Loss: 0.0949   Acc: 98.1349\n",
      "Epoch [275/310] Loss: 0.0999   Acc: 98.4524\n",
      "Epoch [276/310] Loss: 0.0994   Acc: 98.1746\n",
      "Epoch [277/310] Loss: 0.0915   Acc: 98.5714\n",
      "Epoch [278/310] Loss: 0.0888   Acc: 98.4921\n",
      "Epoch [279/310] Loss: 0.0881   Acc: 98.2540\n",
      "Epoch [280/310] Loss: 0.0871   Acc: 98.6905\n",
      "Epoch [281/310] Loss: 0.0873   Acc: 98.6111\n",
      "Epoch [282/310] Loss: 0.0909   Acc: 98.2937\n",
      "Epoch [283/310] Loss: 0.0900   Acc: 98.3730\n",
      "Epoch [284/310] Loss: 0.0907   Acc: 98.5317\n",
      "Epoch [285/310] Loss: 0.0853   Acc: 98.8095\n",
      "Epoch [286/310] Loss: 0.0907   Acc: 98.4127\n",
      "Epoch [287/310] Loss: 0.0947   Acc: 98.0556\n",
      "Epoch [288/310] Loss: 0.0954   Acc: 98.3730\n",
      "Epoch [289/310] Loss: 0.0887   Acc: 97.9762\n",
      "Epoch [290/310] Loss: 0.0899   Acc: 98.2937\n",
      "Epoch [291/310] Loss: 0.0823   Acc: 98.7698\n",
      "Epoch [292/310] Loss: 0.0852   Acc: 98.8095\n",
      "Epoch [293/310] Loss: 0.0881   Acc: 98.3333\n",
      "Epoch [294/310] Loss: 0.0882   Acc: 98.2937\n",
      "Epoch [295/310] Loss: 0.0787   Acc: 98.8889\n",
      "Epoch [296/310] Loss: 0.0828   Acc: 98.3730\n",
      "Epoch [297/310] Loss: 0.0838   Acc: 98.2937\n",
      "Epoch [298/310] Loss: 0.0893   Acc: 98.2937\n",
      "Epoch [299/310] Loss: 0.0793   Acc: 98.9683\n",
      "Epoch [300/310] Loss: 0.0847   Acc: 98.5317\n",
      "Epoch [301/310] Loss: 0.0760   Acc: 98.8889\n",
      "Epoch [302/310] Loss: 0.0769   Acc: 98.7698\n",
      "Epoch [303/310] Loss: 0.0800   Acc: 98.6508\n",
      "Epoch [304/310] Loss: 0.0770   Acc: 98.7698\n",
      "Epoch [305/310] Loss: 0.0782   Acc: 98.6508\n",
      "Epoch [306/310] Loss: 0.0841   Acc: 98.6111\n",
      "Epoch [307/310] Loss: 0.0828   Acc: 98.4524\n",
      "Epoch [308/310] Loss: 0.0745   Acc: 98.8492\n",
      "Epoch [309/310] Loss: 0.0773   Acc: 98.5317\n",
      "Epoch [310/310] Loss: 0.0743   Acc: 99.0079\n"
     ]
    }
   ],
   "source": [
    "\n",
    "X_train = torch.tensor(X_train, dtype=torch.float)\n",
    "y_train = torch.tensor(y_train, dtype=torch.long)\n",
    "X_test = torch.tensor(X_test, dtype=torch.float)\n",
    "y_test = torch.tensor(y_test, dtype=torch.long)\n",
    "net.train()\n",
    "#print(X_train.shape, y_train.shape)\n",
    "for epoch in range(num_epoch):\n",
    "    #feedforward - backprop\n",
    "    optimizer.zero_grad()\n",
    "    out = net(X_train)\n",
    "    loss = criterion(out, y_train)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    acc = 100 * torch.sum(y_train==torch.max(out.data, 1)[1]).double() / len(y_train)\n",
    "    print ('Epoch [%d/%d] Loss: %.4f   Acc: %.4f' \n",
    "                  %(epoch+1, num_epoch, loss.item(), acc.item()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 54
    },
    "colab_type": "code",
    "id": "jUKi1hQPjKpo",
    "outputId": "6a83f14a-927e-4052-b563-b81814b28983"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([ 4,  5, 11, 11,  0, 11,  4,  0,  8,  5])\n",
      "Accuracy of the network 98.7963 %\n"
     ]
    }
   ],
   "source": [
    "#X_test = torch.tensor(X_test, dtype=torch.float)\n",
    "#y_test = torch.tensor(y_test, dtype=torch.long)\n",
    "net.eval()\n",
    "out = net(X_test)\n",
    "_, predicted = torch.max(out.data, 1)\n",
    "print(predicted[0:10])\n",
    "#get accuration\n",
    "print('Accuracy of the network %.4f %%' % (100 * torch.sum(y_test==predicted).double() / len(y_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 147
    },
    "colab_type": "code",
    "id": "XRjj_lVnkjFe",
    "outputId": "499233ed-75a5-4a48-9138-77d2cf6e96e6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([11,  0, 10,  6, 11,  1,  5,  3,  2,  5])\n",
      "Accuracy of the network 99.8016 %\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\ana\\envs\\packt_torch\\lib\\site-packages\\ipykernel_launcher.py:1: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n",
      "D:\\ana\\envs\\packt_torch\\lib\\site-packages\\ipykernel_launcher.py:2: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  \n"
     ]
    }
   ],
   "source": [
    "X_train = torch.tensor(X_train, dtype=torch.float)\n",
    "y_train= torch.tensor(y_train, dtype=torch.long)\n",
    "net.eval()\n",
    "out = net(X_train)\n",
    "_, predicted = torch.max(out.data, 1)\n",
    "print(predicted[0:10])\n",
    "#get accuration\n",
    "print('Accuracy of the network %.4f %%' % (100 * torch.sum(y_train==predicted).double() / len(y_train)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "sCU-z7q21iya"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "name": "Simple_ANN.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
